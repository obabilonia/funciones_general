{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RF2_embedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obabilonia/test1/blob/master/RF2_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aw7i7RN0RWDH",
        "colab_type": "code",
        "outputId": "14943556-9094-417d-d20d-dca2ca60d415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"preparando ambiente\")\n",
        "#!pip install pyvirtualdisplay\n",
        "!pip install unidecode\n",
        "!pip install lxml\n",
        "#!wget https://github.com/denadai2/google_street_view_deep_neural/archive/master.zip\n",
        "#!unzip master.zip\n",
        "#!mv google_street_view_deep_neural-master/* .\n",
        "#!wget https://ndownloader.figshare.com/files/11086517\n",
        "#!mv 11086517 generated_files/pytorch_state.npy\n",
        "#!pip install torchvision\n",
        "#!pip install Pillow==4.0.0\n",
        "#!pip install image\n",
        "#!mkdir images\n",
        "!wget http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.txt.bz2\n",
        "!bzip2 -d SBW-vectors-300-min5.txt.bz2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparando ambiente\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.23\n",
            "Collecting lxml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/14/f4343239f955442da9da1919a99f7311bc5627522741bada61b2349c8def/lxml-4.2.5-cp27-cp27mu-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 3.0MB/s \n",
            "\u001b[?25hInstalling collected packages: lxml\n",
            "Successfully installed lxml-4.2.5\n",
            "--2018-12-07 19:04:03--  http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.txt.bz2\n",
            "Resolving cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)... 200.16.17.55\n",
            "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.txt.bz2 [following]\n",
            "--2018-12-07 19:04:04--  https://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.txt.bz2\n",
            "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 818175453 (780M) [application/x-bzip2]\n",
            "Saving to: ‘SBW-vectors-300-min5.txt.bz2’\n",
            "\n",
            "SBW-vectors-300-min 100%[===================>] 780.27M  13.0MB/s    in 72s     \n",
            "\n",
            "2018-12-07 19:05:16 (10.8 MB/s) - ‘SBW-vectors-300-min5.txt.bz2’ saved [818175453/818175453]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v0CNT51cRWDU",
        "colab_type": "code",
        "outputId": "ed67bc69-e4e0-4d61-9e43-841a89be1dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -O Oscar3.py https://raw.githubusercontent.com/obabilonia/test1/master/Oscar.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-07 19:11:29--  https://raw.githubusercontent.com/obabilonia/test1/master/Oscar.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5175 (5.1K) [text/plain]\n",
            "Saving to: ‘Oscar3.py’\n",
            "\n",
            "\rOscar3.py             0%[                    ]       0  --.-KB/s               \rOscar3.py           100%[===================>]   5.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-12-07 19:11:29 (60.7 MB/s) - ‘Oscar3.py’ saved [5175/5175]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_lXxLPO_RWDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import Oscar3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dz63ssF7RWDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import lxml\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "#from selenium import webdriver\n",
        "#from selenium.webdriver.support.ui import Select\n",
        "import time\n",
        "from PIL import Image\n",
        "import subprocess\n",
        "#import commands\n",
        "import pickle\n",
        "#from selenium.webdriver.chrome.options import Options\n",
        "#from pyvirtualdisplay import Display\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from unidecode import unidecode\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FEmm2D-ERWDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#import torch\n",
        "#import torch.nn as nn\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "#from torch.utils.data import Dataset, DataLoader\n",
        "#from torch.autograd import Variable\n",
        "#from torchvision import transforms, utils\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from matplotlib.pyplot import imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w0ahufOVRWDh",
        "colab_type": "code",
        "outputId": "ad344acb-9336-4588-d4c2-653652f00448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Get Descripcion\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get Descripcion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6ZQVai8QRWEr",
        "colab_type": "code",
        "outputId": "4aadd584-a3ea-4877-e244-919b4f44223a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "!curl \"https://raw.githubusercontent.com/andreafabrizi/Dropbox-Uploader/master/dropbox_uploader.sh\" -o dropbox_uploader.sh\n",
        "\n",
        "\n",
        "!chmod +x dropbox_uploader.sh\n",
        "!./dropbox_uploader.sh\n",
        "#iJMCDCbjLNIAAAAAAAANHFZu-tNNmw6FXMABCQuh-2f03xDCxv33TFd9yO0XQlqE\n",
        "#%notebook ./filename.ipynb\n",
        "#!./dropbox_uploader.sh upload filename.ipynb /"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 49655  100 49655    0     0   292k      0 --:--:-- --:--:-- --:--:--  290k\n",
            "\n",
            " This is the first time you run this script, please follow the instructions:\n",
            "\n",
            " 1) Open the following URL in your Browser, and log in using your account: https://www.dropbox.com/developers/apps\n",
            " 2) Click on \"Create App\", then select \"Dropbox API app\"\n",
            " 3) Now go on with the configuration, choosing the app permissions and access restrictions to your DropBox folder\n",
            " 4) Enter the \"App Name\" that you prefer (e.g. MyUploader4311995832639)\n",
            "\n",
            " Now, click on the \"Create App\" button.\n",
            "\n",
            " When your new App is successfully created, please click on the Generate button\n",
            " under the 'Generated access token' section, then copy and paste the new access token here:\n",
            "\n",
            " # Access token: iJMCDCbjLNIAAAAAAAANHFZu-tNNmw6FXMABCQuh-2f03xDCxv33TFd9yO0XQlqE\n",
            "\n",
            " > The access token is iJMCDCbjLNIAAAAAAAANHFZu-tNNmw6FXMABCQuh-2f03xDCxv33TFd9yO0XQlqE. Looks ok? [y/N]: y\n",
            "   The configuration has been saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5OmqlhvkRWEu",
        "colab_type": "code",
        "outputId": "a765ebe2-a15b-4e62-fb06-63d43f4e4054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!./dropbox_uploader.sh download df.csv ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " > Downloading \"/df.csv\" to \"/content/df.csv\"... DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ywrrTkXiRWEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"df.csv\", delimiter=\";\", index_col=\"Unnamed: 0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCu5jXz6RWEz",
        "colab_type": "code",
        "outputId": "66f3985c-bc04-4b3f-a58e-f2ad148c24e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>banos</th>\n",
              "      <th>desc_main</th>\n",
              "      <th>desc_sec</th>\n",
              "      <th>desc_texto</th>\n",
              "      <th>desc_title</th>\n",
              "      <th>desc_ubica</th>\n",
              "      <th>desc_vend</th>\n",
              "      <th>descripcion</th>\n",
              "      <th>habitaciones</th>\n",
              "      <th>imagens</th>\n",
              "      <th>lat</th>\n",
              "      <th>link</th>\n",
              "      <th>lng</th>\n",
              "      <th>mapa</th>\n",
              "      <th>percepcion_0</th>\n",
              "      <th>percepcion_180</th>\n",
              "      <th>percepcion_270</th>\n",
              "      <th>percepcion_90</th>\n",
              "      <th>precio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>936</th>\n",
              "      <td>77</td>\n",
              "      <td>2</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 77 Op...</td>\n",
              "      <td>0</td>\n",
              "      <td>APTO PISO SEPTIMO: Consta de 77 m2, 3 habitac...</td>\n",
              "      <td>Apto 77 M2 3 Hab Cerca De Estacion San Javier</td>\n",
              "      <td>Ubicacion Medellin 3053181060, San Javier, Me...</td>\n",
              "      <td>Asiestucasa</td>\n",
              "      <td>{'title': 'Apto 77 M2 3 Hab Cerca De Estacion ...</td>\n",
              "      <td>3</td>\n",
              "      <td>['https://http2.mlstatic.com/none-D_NQ_NP_8829...</td>\n",
              "      <td>6.255651</td>\n",
              "      <td>https://apartamento.mercadolibre.com.co/MCO-48...</td>\n",
              "      <td>-75.618627</td>\n",
              "      <td>['6.2556509', '-75.6186265']</td>\n",
              "      <td>4.852417</td>\n",
              "      <td>4.220643</td>\n",
              "      <td>3.951810</td>\n",
              "      <td>4.010629</td>\n",
              "      <td>200000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 55 Op...</td>\n",
              "      <td>Otros Condicion del item: Usado</td>\n",
              "      <td>Informacion detallada. Hermoso Apartamento ub...</td>\n",
              "      <td>Apartamento En Belen Los Alpes</td>\n",
              "      <td>Ubicacion Belen Los Alpes, Medellin, Antioquia</td>\n",
              "      <td>Luz Elena Meneses Moreno</td>\n",
              "      <td>{'title': 'Apartamento En Belen Los Alpes', 'v...</td>\n",
              "      <td>2</td>\n",
              "      <td>['https://http2.mlstatic.com/none-D_NQ_NP_8414...</td>\n",
              "      <td>6.228799</td>\n",
              "      <td>https://apartamento.mercadolibre.com.co/MCO-48...</td>\n",
              "      <td>-75.606185</td>\n",
              "      <td>['6.2287988', '-75.6061855']</td>\n",
              "      <td>4.482049</td>\n",
              "      <td>4.327452</td>\n",
              "      <td>5.193517</td>\n",
              "      <td>5.267939</td>\n",
              "      <td>200000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>67</td>\n",
              "      <td>2</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 67 Op...</td>\n",
              "      <td>0</td>\n",
              "      <td>Se vende apartamento en Rodeo Alto, 3 habitac...</td>\n",
              "      <td>Se Vende Apartamento En Rodeo Alto</td>\n",
              "      <td>Ubicacion Medellin, Antioquia</td>\n",
              "      <td>Prado Gestion Inmobiliaria S.a.s</td>\n",
              "      <td>{'title': 'Se Vende Apartamento En Rodeo Alto'...</td>\n",
              "      <td>3</td>\n",
              "      <td>['https://http2.mlstatic.com/none-D_NQ_NP_9144...</td>\n",
              "      <td>6.244203</td>\n",
              "      <td>https://apartamento.mercadolibre.com.co/MCO-48...</td>\n",
              "      <td>-75.581212</td>\n",
              "      <td>['6.244203', '-75.5812119']</td>\n",
              "      <td>4.042085</td>\n",
              "      <td>4.859144</td>\n",
              "      <td>4.474911</td>\n",
              "      <td>4.472118</td>\n",
              "      <td>200000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>939</th>\n",
              "      <td>67</td>\n",
              "      <td>2</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 67 Op...</td>\n",
              "      <td>Caracteristicas adicionales Balcon Cocina Int...</td>\n",
              "      <td>SE VENDE APARTAMETO EN LOS COLORES en Centro ...</td>\n",
              "      <td>Se Vende Apartameto En Los Colores</td>\n",
              "      <td>Ubicacion Centro Occidente, Medellin, Antioquia</td>\n",
              "      <td>Prado Gestion Inmobiliaria S.a.s</td>\n",
              "      <td>{'title': 'Se Vende Apartameto En Los Colores'...</td>\n",
              "      <td>3</td>\n",
              "      <td>['https://http2.mlstatic.com/none-D_NQ_NP_7736...</td>\n",
              "      <td>6.253453</td>\n",
              "      <td>https://apartamento.mercadolibre.com.co/MCO-48...</td>\n",
              "      <td>-75.624792</td>\n",
              "      <td>['6.2534528', '-75.6247916']</td>\n",
              "      <td>5.527111</td>\n",
              "      <td>4.960363</td>\n",
              "      <td>4.864713</td>\n",
              "      <td>5.027576</td>\n",
              "      <td>200000000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>67</td>\n",
              "      <td>2</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 67 Op...</td>\n",
              "      <td>Caracteristicas adicionales Balcon Calefaccio...</td>\n",
              "      <td>HERMOSO APARTAMENTO EN BELEN LA MOTA CERCA A ...</td>\n",
              "      <td>Hermoso Apartamento En Belen La Mota</td>\n",
              "      <td>Ubicacion La Mota, Medellin, Antioquia</td>\n",
              "      <td>Su Propiedad</td>\n",
              "      <td>{'title': 'Hermoso Apartamento En Belen La Mot...</td>\n",
              "      <td>2</td>\n",
              "      <td>['https://http2.mlstatic.com/none-D_NQ_NP_9028...</td>\n",
              "      <td>6.208338</td>\n",
              "      <td>https://apartamento.mercadolibre.com.co/MCO-48...</td>\n",
              "      <td>-75.599514</td>\n",
              "      <td>['6.2083376', '-75.5995139']</td>\n",
              "      <td>4.486433</td>\n",
              "      <td>5.168179</td>\n",
              "      <td>6.034755</td>\n",
              "      <td>4.307685</td>\n",
              "      <td>200000000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    area banos                                          desc_main  \\\n",
              "936   77     2   Inmueble: Apartamento Metros de const.: 77 Op...   \n",
              "937   55     2   Inmueble: Apartamento Metros de const.: 55 Op...   \n",
              "938   67     2   Inmueble: Apartamento Metros de const.: 67 Op...   \n",
              "939   67     2   Inmueble: Apartamento Metros de const.: 67 Op...   \n",
              "940   67     2   Inmueble: Apartamento Metros de const.: 67 Op...   \n",
              "\n",
              "                                              desc_sec  \\\n",
              "936                                                  0   \n",
              "937                   Otros Condicion del item: Usado    \n",
              "938                                                  0   \n",
              "939   Caracteristicas adicionales Balcon Cocina Int...   \n",
              "940   Caracteristicas adicionales Balcon Calefaccio...   \n",
              "\n",
              "                                            desc_texto  \\\n",
              "936   APTO PISO SEPTIMO: Consta de 77 m2, 3 habitac...   \n",
              "937   Informacion detallada. Hermoso Apartamento ub...   \n",
              "938   Se vende apartamento en Rodeo Alto, 3 habitac...   \n",
              "939   SE VENDE APARTAMETO EN LOS COLORES en Centro ...   \n",
              "940   HERMOSO APARTAMENTO EN BELEN LA MOTA CERCA A ...   \n",
              "\n",
              "                                        desc_title  \\\n",
              "936  Apto 77 M2 3 Hab Cerca De Estacion San Javier   \n",
              "937                 Apartamento En Belen Los Alpes   \n",
              "938             Se Vende Apartamento En Rodeo Alto   \n",
              "939             Se Vende Apartameto En Los Colores   \n",
              "940           Hermoso Apartamento En Belen La Mota   \n",
              "\n",
              "                                            desc_ubica  \\\n",
              "936   Ubicacion Medellin 3053181060, San Javier, Me...   \n",
              "937    Ubicacion Belen Los Alpes, Medellin, Antioquia    \n",
              "938                     Ubicacion Medellin, Antioquia    \n",
              "939   Ubicacion Centro Occidente, Medellin, Antioquia    \n",
              "940            Ubicacion La Mota, Medellin, Antioquia    \n",
              "\n",
              "                              desc_vend  \\\n",
              "936                        Asiestucasa    \n",
              "937           Luz Elena Meneses Moreno    \n",
              "938   Prado Gestion Inmobiliaria S.a.s    \n",
              "939   Prado Gestion Inmobiliaria S.a.s    \n",
              "940                       Su Propiedad    \n",
              "\n",
              "                                           descripcion habitaciones  \\\n",
              "936  {'title': 'Apto 77 M2 3 Hab Cerca De Estacion ...            3   \n",
              "937  {'title': 'Apartamento En Belen Los Alpes', 'v...            2   \n",
              "938  {'title': 'Se Vende Apartamento En Rodeo Alto'...            3   \n",
              "939  {'title': 'Se Vende Apartameto En Los Colores'...            3   \n",
              "940  {'title': 'Hermoso Apartamento En Belen La Mot...            2   \n",
              "\n",
              "                                               imagens       lat  \\\n",
              "936  ['https://http2.mlstatic.com/none-D_NQ_NP_8829...  6.255651   \n",
              "937  ['https://http2.mlstatic.com/none-D_NQ_NP_8414...  6.228799   \n",
              "938  ['https://http2.mlstatic.com/none-D_NQ_NP_9144...  6.244203   \n",
              "939  ['https://http2.mlstatic.com/none-D_NQ_NP_7736...  6.253453   \n",
              "940  ['https://http2.mlstatic.com/none-D_NQ_NP_9028...  6.208338   \n",
              "\n",
              "                                                  link        lng  \\\n",
              "936  https://apartamento.mercadolibre.com.co/MCO-48... -75.618627   \n",
              "937  https://apartamento.mercadolibre.com.co/MCO-48... -75.606185   \n",
              "938  https://apartamento.mercadolibre.com.co/MCO-48... -75.581212   \n",
              "939  https://apartamento.mercadolibre.com.co/MCO-48... -75.624792   \n",
              "940  https://apartamento.mercadolibre.com.co/MCO-48... -75.599514   \n",
              "\n",
              "                             mapa  percepcion_0  percepcion_180  \\\n",
              "936  ['6.2556509', '-75.6186265']      4.852417        4.220643   \n",
              "937  ['6.2287988', '-75.6061855']      4.482049        4.327452   \n",
              "938   ['6.244203', '-75.5812119']      4.042085        4.859144   \n",
              "939  ['6.2534528', '-75.6247916']      5.527111        4.960363   \n",
              "940  ['6.2083376', '-75.5995139']      4.486433        5.168179   \n",
              "\n",
              "     percepcion_270  percepcion_90       precio  \n",
              "936        3.951810       4.010629  200000000.0  \n",
              "937        5.193517       5.267939  200000000.0  \n",
              "938        4.474911       4.472118  200000000.0  \n",
              "939        4.864713       5.027576  200000000.0  \n",
              "940        6.034755       4.307685  200000000.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "mhZ5c6V1RWE2",
        "colab_type": "code",
        "outputId": "f46760d7-66d6-45f3-d031-4db8e99b85be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Get Distances - Interes Points\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get Distances - Interes Points\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U5MaTND0RWE5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df[\"lat\"] = df[\"lat\"].astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oAwa1bjHRWE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[\"lat\"] = df[\"mapa\"].map(lambda x: eval(eval(x)[0]))\n",
        "df[\"lng\"] = df[\"mapa\"].map(lambda x: eval(eval(x)[1]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-DhwnIrRWE_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_Estrato(x):\n",
        "  try:\n",
        "    y= x.split(\"Estrato:\")[1].split(\" \")\n",
        "    return int(y[1])\n",
        "  except:\n",
        "    return np.nan\n",
        "    \n",
        "df[\"Estrato\"] = df[\"desc_main\"].map(lambda x: get_Estrato(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnb5dTmMRWFC",
        "colab_type": "code",
        "outputId": "92541062-3b18-4c83-de27-61c39803f933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "cell_type": "code",
      "source": [
        "Oscar3.O_check_base(df[[\"Estrato\"]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>columns</th>\n",
              "      <th>types</th>\n",
              "      <th>missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Estrato</td>\n",
              "      <td>float64</td>\n",
              "      <td>263.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   columns    types  missing\n",
              "0  Estrato  float64    263.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "fiHTAZ7-RWFF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[\"Estrato\"] = df[[\"Estrato\"]].fillna(df[\"Estrato\"].mode()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GXhkTpvWRWFI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df[df[\"area\"] != \"3</span>\\n\\t\\t\\t</li>\\n\"]\n",
        "df = df[df[\"area\"] != '1</span>\\n\\t\\t\\t</li>\\n']\n",
        "#df[df[\"banos\"] == 'M\\xc3\\xa1s de 10']\n",
        "#Oscar3.O_print_full(df[df[\"banos\"] == 'M\\xc3\\xa1s de 10'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jvEutKJvRWFK",
        "colab_type": "code",
        "outputId": "ebc2838c-d640-4825-be13-bad7e80340dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "Oscar3.O_print_full(df[df[\"habitaciones\"] == 'M\\xc3\\xa1s de 10'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    area banos                                                                                                                                                                               desc_main                                                                                                                                                                                                                                                                                                                    desc_sec                                                                                                                                                                                                                                                                                                                                                                                                       desc_texto                       desc_title                                         desc_ubica            desc_vend  \\\n",
            "5    1    0      Inmueble: Apartamento Metros de const.: 1 Operacion: Venta Habitaciones: Mas de 10 Banos: No tiene Anos de antiguedad: 1 Metros de terreno: 1                                           Otros Condicion del item: Usado                                                                                                                                                                                                                                                                                            0                                                                                                                                                                                                                                                                                                                                                                                                                Apartamentos En Venta             Ubicacion Medellin, Antioquia                     0                     \n",
            "604  63   2      Inmueble: Apartamento Metros de const.: 63 Operacion: Venta Valor administracion ($): 148000 Habitaciones: Mas de 10 Banos: 2 Anos de antiguedad: 0 Estrato: 4 Metros de terreno: 63    Caracteristicas adicionales Balcon Calefaccion o Chimenea Cocina Integral Garaje Piscina Vista panoramica Cercanias a menos de dos cuadras Centro comercial Colegio Parque Universidad Instalaciones y comodidades Circuito Cerrado de TV Conjunto Cerrado Ascensores Parque infantil Salon de Fiesta Vigilancia privada    Apartamento en venta en rodeo alto con area de 63m2, 3 alcobas , 2 banos, sala comedor, cocina integral cerrada, balcon con vista panoramica, calentador de paso, parqueadero cubierto. Unidad cerrada con vigilancia las 24 horas, pscina, placas deportivas, amplias zonas verdes, sendero ecologico, parqueadero de visitantes. Valor de venta: 165.000.000. NEGOCIABLES ! Urve inmobiliaria Colombia SAS.   Apartamento En Venta Rodeo Alto   Ubicacion Belen Rodeo Alto, Medellin, Antioquia    Patricia Valencia    \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      descripcion habitaciones                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         imagens                  lat                                                                                       link                  lng                          mapa  \\\n",
            "5    {'title': 'Apartamentos En Venta', 'vendedor': 0, 'texto': 0, 'ubicacion': ' Ubicacion Medellin, Antioquia ', 'main_descripcion': ' Inmueble: Apartamento Metros de const.: 1 Operacion: Venta Habitaciones: Mas de 10 Banos: No tiene Anos de antiguedad: 1 Metros de terreno: 1 ', 'sec_descripcion': ' Otros Condicion del item: Usado '}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Más de 10    0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              6.24  https://apartamento.mercadolibre.com.co/MCO-494405318-apartamentos-en-venta-_JM                         -75.58  ['6.2447031', '-75.5807149']   \n",
            "604  {'title': 'Apartamento En Venta Rodeo Alto', 'vendedor': ' Patricia Valencia ', 'texto': ' Apartamento en venta en rodeo alto con area de 63m2, 3 alcobas , 2 banos, sala comedor, cocina integral cerrada, balcon con vista panoramica, calentador de paso, parqueadero cubierto. Unidad cerrada con vigilancia las 24 horas, pscina, placas deportivas, amplias zonas verdes, sendero ecologico, parqueadero de visitantes. Valor de venta: 165.000.000. NEGOCIABLES ! Urve inmobiliaria Colombia SAS. ', 'ubicacion': ' Ubicacion Belen Rodeo Alto, Medellin, Antioquia ', 'main_descripcion': ' Inmueble: Apartamento Metros de const.: 63 Operacion: Venta Valor administracion ($): 148000 Habitaciones: Mas de 10 Banos: 2 Anos de antiguedad: 0 Estrato: 4 Metros de terreno: 63 ', 'sec_descripcion': ' Caracteristicas adicionales Balcon Calefaccion o Chimenea Cocina Integral Garaje Piscina Vista panoramica Cercanias a menos de dos cuadras Centro comercial Colegio Parque Universidad Instalaciones y comodidades Circuito Cerrado de TV Conjunto Cerrado Ascensores Parque infantil Salon de Fiesta Vigilancia privada '}  Más de 10    ['https://http2.mlstatic.com/none-D_NQ_NP_715780-MCO27884834507_082018-F.jpg', 'https://http2.mlstatic.com/none-D_NQ_NP_756261-MCO27884834510_082018-F.jpg', 'https://http2.mlstatic.com/none-D_NQ_NP_823084-MCO27884834506_082018-F.jpg', 'https://http2.mlstatic.com/none-D_NQ_NP_974547-MCO27884834504_082018-F.jpg', 'https://http2.mlstatic.com/none-D_NQ_NP_831514-MCO27884834505_082018-F.jpg', 'https://http2.mlstatic.com/none-D_NQ_NP_853051-MCO27884834502_082018-F.jpg', 'https://http2.mlstatic.com/none-D_NQ_NP_655567-MCO27884834508_082018-F.jpg', 'https://http2.mlstatic.com/none-D_NQ_NP_982623-MCO27884834509_082018-F.jpg', 'https://http2.mlstatic.com/none-D_NQ_NP_615278-MCO27884834503_082018-F.jpg']                 6.20  https://apartamento.mercadolibre.com.co/MCO-479630765-apartamento-en-venta-rodeo-alto-_JM               -75.61  ['6.2036942', '-75.6050723']   \n",
            "\n",
            "            percepcion_0       percepcion_180       percepcion_270        percepcion_90               precio              Estrato  \n",
            "5                   5.18                 5.37                 5.30                 4.40         1,000,000.00                 3.00  \n",
            "604                 5.06                 5.37                 5.43                 5.10       165,000,000.00                 4.00  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tN5ZNjnoRWFQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Oscar3.O_print_full(df.iloc[603])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4J0CH8YcRWFX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.drop(index=5, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a5-vjumJRWFb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.at[407,\"banos\"] = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fFFLZI-zRWFg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.at[604,\"habitaciones\"] = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cIIB7Z8tRWFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df.iloc[603]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i6ABtLbqRWFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df.iloc[407]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "roSSrD2FRWFt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df[\"area\"] = df[\"area\"].astype(int)\n",
        "df[\"banos\"] = df[\"banos\"].astype(int)\n",
        "df[\"habitaciones\"] = df[\"habitaciones\"].astype(int)\n",
        "df[\"Estrato\"] = df[\"Estrato\"].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dkq9NMhDRWFw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df[[\"desc_main\"]].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-xiEMCBRWF0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df[\"lat\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tl7vt7-gRWF4",
        "colab_type": "code",
        "outputId": "031cf1ab-5db9-4687-8b11-d033fd4d3282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "cell_type": "code",
      "source": [
        "Oscar3.O_check_base(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>columns</th>\n",
              "      <th>types</th>\n",
              "      <th>missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>area</td>\n",
              "      <td>int64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>banos</td>\n",
              "      <td>int64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>desc_main</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>desc_sec</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>desc_texto</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>desc_title</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>desc_ubica</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>desc_vend</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>descripcion</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>habitaciones</td>\n",
              "      <td>int64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>imagens</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>lat</td>\n",
              "      <td>float64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>link</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>lng</td>\n",
              "      <td>float64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>mapa</td>\n",
              "      <td>object</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>percepcion_0</td>\n",
              "      <td>float64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>percepcion_180</td>\n",
              "      <td>float64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>percepcion_270</td>\n",
              "      <td>float64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>percepcion_90</td>\n",
              "      <td>float64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>precio</td>\n",
              "      <td>float64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Estrato</td>\n",
              "      <td>int64</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           columns    types  missing\n",
              "0             area    int64      0.0\n",
              "1            banos    int64      0.0\n",
              "2        desc_main   object      0.0\n",
              "3         desc_sec   object      0.0\n",
              "4       desc_texto   object      0.0\n",
              "5       desc_title   object      0.0\n",
              "6       desc_ubica   object      0.0\n",
              "7        desc_vend   object      0.0\n",
              "8      descripcion   object      0.0\n",
              "9     habitaciones    int64      0.0\n",
              "10         imagens   object      0.0\n",
              "11             lat  float64      0.0\n",
              "12            link   object      0.0\n",
              "13             lng  float64      0.0\n",
              "14            mapa   object      0.0\n",
              "15    percepcion_0  float64      0.0\n",
              "16  percepcion_180  float64      0.0\n",
              "17  percepcion_270  float64      0.0\n",
              "18   percepcion_90  float64      0.0\n",
              "19          precio  float64      0.0\n",
              "20         Estrato    int64      0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "WiYumkC6RWF9",
        "colab_type": "code",
        "outputId": "aadda657-0cd5-4450-f184-e1fd1b750821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['area', 'banos', 'desc_main', 'desc_sec', 'desc_texto', 'desc_title', 'desc_ubica', 'desc_vend', 'descripcion', 'habitaciones', 'imagens', 'lat', 'link', 'lng', 'mapa', 'percepcion_0', 'percepcion_180', 'percepcion_270', 'percepcion_90', 'precio', 'Estrato']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-T8wVwxZRWGA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_0 = df\n",
        "df[\"description\"] = df_0['desc_main'] + df_0['desc_sec'] + df_0['desc_texto'] + df_0['desc_title'] + df_0['desc_ubica'] + df_0['desc_vend']\n",
        "df = df[['area', 'banos', 'habitaciones', 'lat', 'lng', 'percepcion_0', 'percepcion_180', 'percepcion_270', 'percepcion_90', 'Estrato', 'description', 'precio']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fwnhuBFWRWGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df[df[\"precio\"] >0.8e8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YJmYs1xMRWGK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df = df.drop(columns=['desc_main', 'desc_sec', 'desc_texto', 'desc_title', 'desc_ubica', 'desc_vend', 'descripcion','imagens','mapa','link'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "72KwWODjRWGM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#lista"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDhXmZ93RWGQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ind  = df.columns.tolist().index(\"precio\")\n",
        "lista = df.columns.tolist()\n",
        "V_objetivo = lista.pop(ind)\n",
        "X = df[lista]\n",
        "y = df[[V_objetivo]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bB8N1KEERWGT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#X.head().columns.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5mwf-1uRWGX",
        "colab_type": "code",
        "outputId": "15559c66-2ed0-47ed-d2c8-22331919afb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>banos</th>\n",
              "      <th>habitaciones</th>\n",
              "      <th>lat</th>\n",
              "      <th>lng</th>\n",
              "      <th>percepcion_0</th>\n",
              "      <th>percepcion_180</th>\n",
              "      <th>percepcion_270</th>\n",
              "      <th>percepcion_90</th>\n",
              "      <th>Estrato</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6.278447</td>\n",
              "      <td>-75.598370</td>\n",
              "      <td>4.707252</td>\n",
              "      <td>4.382922</td>\n",
              "      <td>5.228550</td>\n",
              "      <td>4.345707</td>\n",
              "      <td>2</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 41 Op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6.282000</td>\n",
              "      <td>-75.595000</td>\n",
              "      <td>5.046718</td>\n",
              "      <td>5.681197</td>\n",
              "      <td>4.264784</td>\n",
              "      <td>5.246507</td>\n",
              "      <td>3</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 36 Op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6.186444</td>\n",
              "      <td>-75.658270</td>\n",
              "      <td>5.616747</td>\n",
              "      <td>5.264588</td>\n",
              "      <td>4.858189</td>\n",
              "      <td>5.142254</td>\n",
              "      <td>3</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 43 Op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6.200800</td>\n",
              "      <td>-75.595636</td>\n",
              "      <td>5.077016</td>\n",
              "      <td>5.385752</td>\n",
              "      <td>5.238197</td>\n",
              "      <td>5.441324</td>\n",
              "      <td>2</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 60 Op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6.260916</td>\n",
              "      <td>-75.598985</td>\n",
              "      <td>4.849460</td>\n",
              "      <td>5.030130</td>\n",
              "      <td>4.857601</td>\n",
              "      <td>5.665002</td>\n",
              "      <td>4</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 48 Op...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    area  banos  habitaciones       lat        lng  percepcion_0  \\\n",
              "53    41      1             2  6.278447 -75.598370      4.707252   \n",
              "54    36      1             3  6.282000 -75.595000      5.046718   \n",
              "55    43      1             2  6.186444 -75.658270      5.616747   \n",
              "56    60      1             2  6.200800 -75.595636      5.077016   \n",
              "57    48      1             2  6.260916 -75.598985      4.849460   \n",
              "\n",
              "    percepcion_180  percepcion_270  percepcion_90  Estrato  \\\n",
              "53        4.382922        5.228550       4.345707        2   \n",
              "54        5.681197        4.264784       5.246507        3   \n",
              "55        5.264588        4.858189       5.142254        3   \n",
              "56        5.385752        5.238197       5.441324        2   \n",
              "57        5.030130        4.857601       5.665002        4   \n",
              "\n",
              "                                          description  \n",
              "53   Inmueble: Apartamento Metros de const.: 41 Op...  \n",
              "54   Inmueble: Apartamento Metros de const.: 36 Op...  \n",
              "55   Inmueble: Apartamento Metros de const.: 43 Op...  \n",
              "56   Inmueble: Apartamento Metros de const.: 60 Op...  \n",
              "57   Inmueble: Apartamento Metros de const.: 48 Op...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "CgS3q0PhRWGb",
        "colab_type": "code",
        "outputId": "f7d8bcac-6416-466c-bce3-82f143fb5b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "cell_type": "code",
      "source": [
        "y.hist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f237f76a390>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEUCAYAAADUVaY3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f23a9243890>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LcRLw4zfRWGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PCMnJFHRWGh",
        "colab_type": "code",
        "outputId": "fd0db8e0-042a-458a-d688-b9f40b22dc32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.hist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f237c65ba90>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEUCAYAAADUVaY3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEX1JREFUeJzt3X+Q3HV9x/FnyFVIwqkHbktMVUTt\nW5GOVVqRQjAgCCIOakBbgYJgAcWO2h+WqVYBR6HYDKAwFqQMlFY6FoskIyIFrWK1FHVG7YzzrlGD\nP4JyNGc8DEaD6R/fb5zlmrvb/e7e7t6H52Mmk93vfn+8P7P3fd3nPt/vfnbJzp07kSSVYY9hFyBJ\n6h9DXZIKYqhLUkEMdUkqiKEuSQUx1CWpIIa61EBE3BURLxh2HdJMS7xPXZLKMTbsAqSFEhFrgA8A\n/wacADwO+EPgOGAV8DzgI8AVwF8DpwB7AR8H/jQzH4mIA4DrgScDU8A5mfmViNgEnJqZn4+Ik4F3\nU51Pm4E/zsxvDaaV0qM5/KLSHQj8V2YG8F7gQ/Xy44HjM/Ny4FTgNcALgWfU/95Yr3cNcFNmPrPe\n/sb2nUfEU4EPA6/MzGcDnwCuXtAWSXMw1FW6h4CP1o8/BvwOsBy4JzMfrJe/ArguM7dm5g7gWuDV\nEbEXcCRwU73ercAhM/Z/DPCZzNxYP78WODIi/CtYQ+EPnko3lZm7Lhz9uP7/icCWtnWeCPx5RJxd\nPx8DJoF9qDo+WwHq/Tw0Y/8tqmEZ6nW2RsQS4EnAD/vYDqkjhrpKt2/b44n6/y1UobvLZmB9Zl7Z\nvmFE7AnsrPfxYB3WzwDax8t/BBzats0E8EvgQaQhcPhFpVseEa+sH58EfAn42Yx1bgVOi4jlABFx\nTkScnpnbgTuAM+r1jgVua+v5Q3UR9oj6girAucAd9TCONHD21FW6TcDhEXEp1d0vJwMvn7HOx4Hn\nAl+JCKh64mfVr70B+KeIeBNVD/917Rtm5vcj4g3ArRHxa8B3gLORhsT71FWs+pbGa+s7V6THBIdf\nJKkghrokFcThF0kqiD11SSrIUO9+mZyc7vrPhImJ5UxNbVuIcgbOtowm2zK6SmpPL21ptcaXzPba\nouupj40tHXYJfWNbRpNtGV0ltWeh2rLoQl2SNDtDXZIKYqhLUkEMdUkqiKEuSQUx1CWpIIa6JBXE\nUJekghjqklQQvyRDI+3MSz49lONed/5RQzmu1Ct76pJUEHvqkoDh/VUE/mXUTx2FekQcRPXlvJdl\n5pUR8RTgRmApcD9wWmZuj4hTgLdSfZv6NZn59wtUtyRpN+YdfomIFcAHgbvaFl8EXJWZq4GNwJn1\neu8CjgbWAG+LiH36XrEkaVadjKlvB44HNrctWwOsrx9voAryQ4B7M3NrZj4M/AdwWP9KlSTNZ97h\nl8zcAeyIiPbFKzJze/34AWAlsB8w2bbOruWzmphY3mhO4VZrvOttRpVtGU22ZbC6qbGf7XnFn93a\nt311a8O6ExfkvenHhdLZvoFj1m/m2KXJt360WuNMTk53vd0osi2jq5S2LJb3pdMaF0t7OtW0LXP9\nMmh6S+NDEbGsfryKamhmM1VvnRnLJUkD0jTU7wTW1o/XArcD9wC/FxFPjIi9qcbT7+69RElSp+Yd\nfomIg4F1wP7ALyLiJOAU4PqIOAe4D7ghM38REecDnwJ2Ahdm5tYFq1yS9P90cqH0y1R3u8x0zG7W\nvRm4ufeyJElNOE2AJBXEUJekghjqklQQQ12SCmKoS1JBDHVJKoihLkkFMdQlqSCGuiQVxFCXpIIY\n6pJUEENdkgpiqEtSQQx1SSqIoS5JBTHUJakghrokFcRQl6SCGOqSVBBDXZIKYqhLUkEMdUkqiKEu\nSQUx1CWpIIa6JBXEUJekghjqklQQQ12SCmKoS1JBDHVJKoihLkkFGWuyUUTsDfwDMAHsCVwI/BD4\nELAT+FpmvrFfRUqSOtO0p34GkJl5JHAScAVwOfCWzDwMeEJEvKw/JUqSOtU01B8E9q0fTwBbgKdn\n5r31sg3A0T3WJknqUqPhl8z854g4IyI2UoX6K4Cr2lZ5AFg5334mJpYzNra06+O3WuNdbzOqbMto\nsi2D1U2Ni6E9nVqItjQdUz8V+G5mHhcRzwNuAba2rbKkk/1MTW3r+tit1jiTk9NdbzeKbMvoKqUt\ni+V96bTGxdKeTjVty1y/DJoOvxwGfAogM78KLAOe1Pb6KmBzw31LkhpqGuobgUMAIuJpwDTwjYg4\nvH791cDtvZcnSepGo+EX4Grguoj4bL2Pc6luabw6IvYA7snMO/tUo6TCnXnJp4ddQjGaXih9CHjN\nbl5a3Vs5kqRe+IlSSSqIoS5JBTHUJakghrokFcRQl6SCGOqSVBBDXZIKYqhLUkEMdUkqiKEuSQUx\n1CWpIIa6JBXEUJekghjqklSQpvOpS0Ub5vze151/1NCOrcXPnrokFcRQl6SCGOqSVBBDXZIKYqhL\nUkEMdUkqiKEuSQUx1CWpIIa6JBXEUJekghjqklQQ536RRsww553R4mdPXZIKYqhLUkEMdUkqiKEu\nSQVpfKE0Ik4B3g7sAN4FfA24EVgK3A+clpnb+1GkJKkzjXrqEbEv8G7gcOAE4ETgIuCqzFwNbATO\n7FeRkqTONB1+ORq4MzOnM/P+zDwbWAOsr1/fUK8jSRqgpsMv+wPLI2I9MAFcAKxoG255AFg5304m\nJpYzNra064O3WuNdbzOqbIv02LUQ50zTUF8C7Au8Cnga8Jl6Wfvr85qa2tb1gVutcSYnp7vebhTZ\nFumxrek5M9cvg6bDLz8CvpCZOzLzW8A0MB0Ry+rXVwGbG+5bktRQ01C/AzgqIvaoL5ruDdwJrK1f\nXwvc3of6JEldaBTqmfkD4GbgP4FPAn9CdTfM6RFxN7APcEO/ipQkdabxfeqZeTVw9YzFx/RWjiSp\nF4t2lsZhzmR33flHDe3YkjQXpwmQpIIY6pJUEENdkgpiqEtSQQx1SSqIoS5JBTHUJakghrokFcRQ\nl6SCGOqSVBBDXZIKsmjnfnkscr4bSfOxpy5JBTHUJakghrokFcRQl6SCGOqSVBBDXZIKYqhLUkEM\ndUkqiKEuSQXxE6XqyDA/zSqpc/bUJakghrokFcRQl6SCGOqSVBBDXZIKYqhLUkEMdUkqiKEuSQXp\n6cNHEbEM+G/gPcBdwI3AUuB+4LTM3N5zhZKkjvXaU38nsKV+fBFwVWauBjYCZ/a4b0lSlxqHekQ8\nGzgQ+ES9aA2wvn68ATi6p8okSV3rZfhlHfBm4PT6+Yq24ZYHgJXz7WBiYjljY0u7PnCrNd71Nv3U\nz+MPuy2Shmchzv9GoR4RfwR8MTO/ExG7W2VJJ/uZmtrW9bFbrXEmJ6e73q6f+nX8UWiLpOFpev7P\n9cugaU/95cABEXEC8JvAduChiFiWmQ8Dq4DNDfctSWqoUahn5mt3PY6IC4BNwO8Da4F/rP+/vffy\nJEnd6Od96u8GTo+Iu4F9gBv6uG9JUgd6/pKMzLyg7ekxve5PktScnyiVpIIY6pJUEL+jtAG/r1PS\nqLKnLkkFMdQlqSCGuiQVxFCXpIIY6pJUEENdkgpiqEtSQQx1SSqIoS5JBTHUJakghrokFcRQl6SC\nGOqSVBBDXZIKYqhLUkEMdUkqiKEuSQUx1CWpIIa6JBXEUJekghjqklQQQ12SCmKoS1JBDHVJKoih\nLkkFMdQlqSCGuiQVxFCXpIKMNd0wIi4FVtf7uBi4F7gRWArcD5yWmdv7UaQkqTONeuoRcSRwUGYe\nChwHXA5cBFyVmauBjcCZfatSktSRpsMvnwNOrh//GFgBrAHW18s2AEf3VJkkqWuNhl8y8xHgp/XT\ns4DbgGPbhlseAFb2Xp4kqRuNx9QBIuJEqlB/KfDNtpeWdLL9xMRyxsaWdn3cVmu8620kadQsRJb1\ncqH0WOAdwHGZuTUiHoqIZZn5MLAK2DzfPqamtnV93FZrnMnJ6a63k6RR0zTL5vpl0PRC6ROA9wMn\nZOaWevGdwNr68Vrg9ib7liQ117Sn/lrgScBHI2LXstOBayPiHOA+4Ibey5MkdaPphdJrgGt289Ix\nvZUjSeqFnyiVpIIY6pJUEENdkgpiqEtSQQx1SSqIoS5JBTHUJakghrokFcRQl6SCGOqSVBBDXZIK\nYqhLUkEMdUkqiKEuSQUx1CWpIIa6JBXEUJekghjqklQQQ12SCmKoS1JBDHVJKoihLkkFMdQlqSCG\nuiQVxFCXpIIY6pJUEENdkgpiqEtSQQx1SSqIoS5JBTHUJakghrokFWSs3zuMiMuAFwE7gbdk5r39\nPoYkaff62lOPiBcDz8rMQ4GzgA/0c/+SpLn1e/jlJcDHATLzG8BERDy+z8eQJM2i38Mv+wFfbns+\nWS/7ye5WbrXGlzQ5SKs1zoZ1JzbZVJJGRqs13vd9LvSF0kahLUlqpt+hvpmqZ77Lk4H7+3wMSdIs\n+h3qdwAnAUTEC4DNmTnd52NIkmaxZOfOnX3dYURcAhwB/BI4LzO/2tcDSJJm1fdQlyQNj58olaSC\nGOqSVJC+TxPQL3NNNxAR5wGnAo8AX8rMtw6nys5FxEHArcBlmXnljNeOBt5H1Z7bMvM9QyixY/O0\n5UjgYqq2JPCGzPzl4KvszFxtaVvnYuDQzFwzyNq6Nc/78hTgJuBxwFcy89whlNixedqyqM7/iLgU\nWE2Vtxdn5r+2vdb3c38ke+pzTTdQf0L1L4DVmXk4cGBEvGg4lXYmIlYAHwTummWVDwBrgcOAl0bE\ngYOqrVsdtOUa4KTMPAwYB44bVG3d6qAt1O/FEQMrqqEO2rIOWJeZLwQeiYinDqy4Ls3VlsV2/ted\nnIPqLDsOuHzGKn0/90cy1Jl7uoGf1//2jogxYDmwZShVdm47cDzVffyPEhEHAFsy83t1j/Y2qvaP\nqlnbUjs4M79fP54E9h1IVc3M1xaowvAdgymnJ3P9jO1B1VNcD5CZ52XmdwdbXlfmel8W2/n/OeDk\n+vGPgRURsRQW7twf1VDfjyoQdtk13QCZ+TPgQuDbwH3APZn5PwOvsAuZuSMzH57l5ZltfQBYufBV\nNTNPW8jMnwBExErgpVQ/qCNpvrZExBnAZ4FNg6qpqXna0gKmgcsi4vP1cNLImqsti+38z8xHMvOn\n9dOzqIZYHqmfL8i5P6qhPtOvphuoe+x/BfwW8HTgkIh43rAKWwCLfmqFiPh1YAPwpsz832HX00RE\n7AO8nqqnvtgtAVYBVwAvBp4fES8fbknNLNbzPyJOpAr1N8+xWl/O/VEN9bmmG3gO8O3MfDAzfw7c\nDRw84Pr6aWZbVzH3cMBIq0+6TwLvzMw7hl1PD46i6uHeDdwCvKC+eL8YPQjcl5nfqnuJdwHPHXJN\nTS268z8ijqUawntZZm5te2lBzv1RDfW5phvYBDwnIpbVz38X+ObAK+yTzNwEPD4i9q/HCE+gav9i\ntY7qjoXbh11ILzLz5sw8MDNfBLyK6o6Rtw27riYycwfw7Yh4Vr3oYKo7kxajTSyi8z8ingC8Hzgh\nMx819r9Q5/7IfqJ05nQDwPOBrZl5S0ScQ/Wn8Q7gC5n59uFVOr+IOJgq7PYHfgH8gOqi1Xfq9hwB\n/E29+scy82+HUmgH5moL8ClgCvhi2yYfycxrBlxmR+Z7X9rW2x+4fpRvaezgZ+yZwPVUHbmvA28c\n1VtNO2jLojn/I+Js4AKgfdz/08DXF+rcH9lQlyR1b1SHXyRJDRjqklQQQ12SCmKoS1JBRnZCL0kq\nWSeTydXrvRdYQ9UJvyUzL51rv/bUJWnAOplMrl7vIODIeoK8w4DXR8R+c21jT12SBm/XpGV/uWtB\nPUPjlVTTjU8DZwBbgb0iYk9gKdXndrbNtWN76pI0YLNMWvZB4JzMfAnVJ0vPy8zvAf9CNXnZfcDf\n7Zo0bzaGuiSNhhcCH46IfwdOA36jnp73VcABwDOBc+sJ82bl8IskjYZtVOPnv/qYf0S8lmp64W31\n868BB1FNNbBbhrokjYavUn070icj4g+o5lrfCLy1/qKTpcBvU80lPyvnfpGkAZtl0rJ3AJdQXQx9\nGHhdZm6JiAuBY+pNP5qZM78S71EMdUkqiBdKJakghrokFcRQl6SCGOqSVBBDXZIKYqhLUkEMdUkq\nyP8BTRPQOtcv3+oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f237c65b510>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XI6t2pmuRWGj",
        "colab_type": "code",
        "outputId": "99ccd2b0-eec1-4c8e-f460-93dfe4a210ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "cell_type": "code",
      "source": [
        "y_test.hist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f237c5d5550>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEUCAYAAADk2bcWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f237c5de950>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "OJOoHdrGRWGm",
        "colab_type": "code",
        "outputId": "dd8bbd52-2b19-4dcc-b9f5-1ad64eb3e424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.columns[0 : -1 ]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([u'area', u'banos', u'habitaciones', u'lat', u'lng', u'percepcion_0',\n",
              "       u'percepcion_180', u'percepcion_270', u'percepcion_90', u'Estrato'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "tZSiaCk1RWGp",
        "colab_type": "code",
        "outputId": "eb4a8d66-eb49-4244-c81c-97f92eccf5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>area</th>\n",
              "      <th>banos</th>\n",
              "      <th>habitaciones</th>\n",
              "      <th>lat</th>\n",
              "      <th>lng</th>\n",
              "      <th>percepcion_0</th>\n",
              "      <th>percepcion_180</th>\n",
              "      <th>percepcion_270</th>\n",
              "      <th>percepcion_90</th>\n",
              "      <th>Estrato</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-75.560286</td>\n",
              "      <td>6.252380</td>\n",
              "      <td>4.318297</td>\n",
              "      <td>5.006413</td>\n",
              "      <td>4.393741</td>\n",
              "      <td>4.716533</td>\n",
              "      <td>4</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 45 Op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-75.599427</td>\n",
              "      <td>6.279888</td>\n",
              "      <td>4.623398</td>\n",
              "      <td>5.794126</td>\n",
              "      <td>5.650282</td>\n",
              "      <td>5.263768</td>\n",
              "      <td>3</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 45 Op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-75.656639</td>\n",
              "      <td>6.186241</td>\n",
              "      <td>4.402307</td>\n",
              "      <td>5.063399</td>\n",
              "      <td>4.032859</td>\n",
              "      <td>4.199199</td>\n",
              "      <td>3</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 38 Op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>-75.556054</td>\n",
              "      <td>6.237129</td>\n",
              "      <td>4.398416</td>\n",
              "      <td>5.371661</td>\n",
              "      <td>5.485881</td>\n",
              "      <td>5.190678</td>\n",
              "      <td>4</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 57 Op...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-75.574635</td>\n",
              "      <td>6.227697</td>\n",
              "      <td>4.541640</td>\n",
              "      <td>4.799234</td>\n",
              "      <td>4.683369</td>\n",
              "      <td>4.625693</td>\n",
              "      <td>3</td>\n",
              "      <td>Inmueble: Apartamento Metros de const.: 28 Op...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     area  banos  habitaciones        lat       lng  percepcion_0  \\\n",
              "309    45      2             1 -75.560286  6.252380      4.318297   \n",
              "228    45      1             2 -75.599427  6.279888      4.623398   \n",
              "110    38      1             2 -75.656639  6.186241      4.402307   \n",
              "564    57      2             3 -75.556054  6.237129      4.398416   \n",
              "121    28      1             1 -75.574635  6.227697      4.541640   \n",
              "\n",
              "     percepcion_180  percepcion_270  percepcion_90  Estrato  \\\n",
              "309        5.006413        4.393741       4.716533        4   \n",
              "228        5.794126        5.650282       5.263768        3   \n",
              "110        5.063399        4.032859       4.199199        3   \n",
              "564        5.371661        5.485881       5.190678        4   \n",
              "121        4.799234        4.683369       4.625693        3   \n",
              "\n",
              "                                           description  \n",
              "309   Inmueble: Apartamento Metros de const.: 45 Op...  \n",
              "228   Inmueble: Apartamento Metros de const.: 45 Op...  \n",
              "110   Inmueble: Apartamento Metros de const.: 38 Op...  \n",
              "564   Inmueble: Apartamento Metros de const.: 57 Op...  \n",
              "121   Inmueble: Apartamento Metros de const.: 28 Op...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "oj8wMGC9RWGt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#df_0['desc_main'] + df_0['desc_sec'] + df_0['desc_texto'] + df_0['desc_title'] + df_0['desc_ubica'] + df_0['desc_vend']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Go9q3jALRWGv",
        "colab_type": "code",
        "outputId": "7d8afd6a-f4e1-42ea-c0ba-3bbb82e5c581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from keras.preprocessing.text import Tokenizer\n",
        "    from keras.preprocessing.sequence import pad_sequences\n",
        "    from keras.layers import Input, Embedding, LSTM, Dense, SimpleRNN, concatenate, Dropout\n",
        "    from keras.models import Model\n",
        "    from keras.optimizers import RMSprop\n",
        "    from keras.callbacks import EarlyStopping \n",
        "except ModuleNotFoundError:\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, SimpleRNN, concatenate\n",
        "    from tensorflow.keras.models import Model\n",
        "    from tensorflow.keras.optimizers import RMSprop\n",
        "    from tensorflow.keras.callbacks import EarlyStopping \n",
        "    \n",
        "    \n",
        "MAX_LEN = 100\n",
        "MAX_WORDS = 25000\n",
        "INDEX = 3\n",
        "\n",
        "tokenizer = Tokenizer(num_words = MAX_WORDS)\n",
        "tokenizer.fit_on_texts(X_train['description'])\n",
        "print('Found {} unique tokens.'.format(len(tokenizer.word_index)))\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(X_train['description'])\n",
        "X_train_main = pad_sequences(sequences, maxlen = MAX_LEN)\n",
        "print('\\nTrain Dataset:')\n",
        "print('\\tShape of the main input: {}'.format(X_train_main.shape))\n",
        "X_train_aux = X_train[X_train.columns[0 : -1 ]]\n",
        "print('\\tShape of the aux input: {}'.format(X_train_aux.shape))\n",
        "print('\\tShape of output: {}'.format(y_train.shape))\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(X_test['description'])\n",
        "X_test_main = pad_sequences(sequences, maxlen = MAX_LEN)\n",
        "#sequences = tokenizer.texts_to_sequences(X_test['description'])\n",
        "#np.vstack((X_test_main, pad_sequences(sequences, maxlen = MAX_LEN)))\n",
        "print('\\nValidation Dataset:')\n",
        "print('\\tShape of the main input: {}'.format(X_test_main.shape))\n",
        "X_test_aux = X_test[X_test.columns[0 : -1]]\n",
        "print('\\tShape of the aux input: {}'.format(X_test_aux.shape))\n",
        "print('\\tShape of output: {}'.format(y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 3068 unique tokens.\n",
            "\n",
            "Train Dataset:\n",
            "\tShape of the main input: (707, 100)\n",
            "\tShape of the aux input: (707, 10)\n",
            "\tShape of output: (707, 1)\n",
            "\n",
            "Validation Dataset:\n",
            "\tShape of the main input: (177, 100)\n",
            "\tShape of the aux input: (177, 10)\n",
            "\tShape of output: (177, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CjgpgEnhRWGx",
        "colab_type": "code",
        "outputId": "af0ba06e-89f3-42fc-e0bf-73790b4a5673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 300\n",
        "\n",
        "embeddings_index = {}\n",
        "#with open('../datasets/{}/cbow_s{}.txt'.format(EMBEDDING, EMBEDDING_DIM), mode = 'r', encoding = 'UTF-8') as f:\n",
        "with open('./SBW-vectors-300-min5.txt', mode = 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1 : ], dtype = 'float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        except:\n",
        "            print('[ERROR] {}: {}'.format(i, values[1 : ]))\n",
        "    f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_weights = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < MAX_WORDS and embedding_vector is not None:\n",
        "            embedding_weights[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000654 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZXQYvq7LZ35b",
        "colab_type": "code",
        "outputId": "a5f77d16-7631-4c95-c816-0d44d4b94e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train_main.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "6oOzwWBQRWGz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#X_train_main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFeO4FIrRWG2",
        "colab_type": "code",
        "outputId": "a1cf8eea-808b-4eca-ee25-553827bef022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "LSTM_SIZE = 5\n",
        "DENSE1_SIZE = 256\n",
        "DENSE2_SIZE = 256\n",
        "LEARNING_RATE = 0.01\n",
        "RHO = 0.05\n",
        "EPSILON = None\n",
        "DECAY = 0.0\n",
        "\n",
        "main_input = Input(shape = (X_train_main.shape[1], ), name = 'main_input')\n",
        "\n",
        "x = Embedding(MAX_WORDS, EMBEDDING_DIM, \n",
        "              input_length = MAX_LEN, \n",
        "              weights = [embedding_weights], \n",
        "              name = 'embedding',\n",
        "              trainable = False)(main_input)\n",
        "\n",
        "lstm_out = LSTM(LSTM_SIZE, name = 'lstm')(x)\n",
        "\n",
        "#aux_input = Input(shape = (X_train_aux.shape[1], ), name = 'aux_input')\n",
        "\n",
        "x = concatenate([lstm_out, aux_input])\n",
        "#x = Dense(DENSE1_SIZE, kernel_initializer = 'uniform', activation = 'relu', name = 'dense1')(x)\n",
        "#x = Dropout(.5)(x)\n",
        "#x = Dense(DENSE2_SIZE, kernel_initializer = 'uniform', activation='relu', name = 'dense2')(x)\n",
        "#x = Dropout(.3)(x)\n",
        "#x = Dense(DENSE2_SIZE, kernel_initializer = 'uniform', activation='relu', name = 'dense3')(x)\n",
        "#x = Dropout(.3)(x)\n",
        "\n",
        "#main_output = Dense(1, activation = 'linear', kernel_initializer = 'normal', name = 'main_output')(x)\n",
        "\n",
        "model = Model(inputs = [main_input, aux_input], outputs = x)\n",
        "optimizer = RMSprop(lr = LEARNING_RATE, rho = RHO, epsilon = EPSILON, decay = DECAY)\n",
        "model.compile(optimizer = optimizer, loss = 'mse', metrics = ['mse'])\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "main_input (InputLayer)         (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 300)     7500000     main_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 5)            6120        embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "aux_input (InputLayer)          (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 15)           0           lstm[0][0]                       \n",
            "                                                                 aux_input[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,506,120\n",
            "Trainable params: 6,120\n",
            "Non-trainable params: 7,500,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FWAvkBabJX6W",
        "colab_type": "code",
        "outputId": "fdad9d82-968e-468b-fb91-d9bf96edbe07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = MAX_WORDS)\n",
        "tokenizer.fit_on_texts(X['description'])\n",
        "print('Found {} unique tokens.'.format(len(tokenizer.word_index)))\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(X['description'])\n",
        "X_main = pad_sequences(sequences, maxlen = MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3364 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tJkCQEuFH9EF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "salida = model.predict([X_main, X[X.columns[0 : -1]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUb-8SQMJy7s",
        "colab_type": "code",
        "outputId": "541e7aec-86c7-44de-80de-e0d5cdf35e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(884, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "eYsGYouGLfOZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LDI7Vfq_Kg-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LSTM_OUT = pd.DataFrame(salida, index=X.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrZzngaHLih6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_0 =df\n",
        "df = pd.concat([df, LSTM_OUT], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YS9JN0R2Ma_i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df.drop('description', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H8NuObZsL3pn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ind  = df.columns.tolist().index(\"precio\")\n",
        "lista = df.columns.tolist()\n",
        "V_objetivo = lista.pop(ind)\n",
        "X = df[lista]\n",
        "y = df[[V_objetivo]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FfiONBuRRWG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2H5GqqYHRWG6",
        "colab_type": "code",
        "outputId": "9c25a81d-8d2f-4446-b948-a836e9e01ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4, 5, 6]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "random_grid"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': [True, False],\n",
              " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
              " 'max_features': ['auto', 'sqrt'],\n",
              " 'min_samples_leaf': [1, 2, 4, 5, 6],\n",
              " 'min_samples_split': [2, 5, 10],\n",
              " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "yzpqH37mMPne",
        "colab_type": "code",
        "outputId": "617a26c9-a3a1-4e00-b870-05a66ef8a70a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py:458: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "UuwH7-ogRWG9",
        "colab_type": "code",
        "outputId": "a9763540-884b-4bee-c140-965c75d09114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "#df_results = pd.read_csv(DF_RESULTS_FILENAME, index_col = 0)\n",
        "    \n",
        "PATIENCE = 10\n",
        "EPOCHS = 2000\n",
        "BATCH_SIZE = int(math.ceil(y_train.shape[0] / 4))\n",
        "\n",
        "\n",
        "history = model.fit([X_train_main, X_train_aux], \n",
        "                    y_train,\n",
        "                    verbose = 1,\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-b802fbc6393e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m history = model.fit([X_train_main, X_train_aux], \n\u001b[1;32m     10\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected concatenate_9 to have shape (15,) but got array with shape (1,)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jPlPEN9brXgd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5l3TPb_ORWHH",
        "colab_type": "code",
        "outputId": "18deb9df-628c-4f86-9f71-96134c228119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        }
      },
      "cell_type": "code",
      "source": [
        "train_mse = history.history['mean_squared_error']\n",
        "validation_mse = history.history['val_mean_squared_error']\n",
        "train_loss = history.history['loss']\n",
        "validation_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(train_mse))\n",
        "\n",
        "plt.plot(epochs, train_mse, 'b-', label = 'Training MSE')\n",
        "plt.plot(epochs, validation_mse, 'r-', label = 'Validation MSE')\n",
        "plt.title('Training and validation MSE')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, train_loss, 'b-', label = 'Training loss')\n",
        "plt.plot(epochs, validation_loss, 'r-', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEHCAYAAACzy817AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff9ceb74390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEHCAYAAACzy817AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xd4FNXbxvHvlmyy6QEDCCqKwkHs\nYkGK9N47ohTpCAIidlEUBHwVRRQVFH6ISO9IVYoURZqKKBwrIIISSno22fb+MYsESEIISTZLns91\ncWV36r3D7rOzZ2bOmLxeL0IIIQKX2d8BhBBCXB4p5EIIEeCkkAshRICTQi6EEAFOCrkQQgQ4KeRC\nCBHgrP4OIAqPUuoDoK7v6Y3AUSDN9/xerXXSJSzrAFBba/1vDtOMAw5prT/MY+R8p5T6EpiltZ6R\nD8vyAtcC9wIttda98ro+pVRfrfVHvscX3baXkHEG8JvWeszlLksUXVLIixGt9cAzj5VSB4FHtNZb\n87isyrmY5rm8LDvQaK2XAEvyOr9SqgzwNPCRb3kX3bZCZCaFXPxHKbUJ2Aa0A3oDvwOfANcDwcC7\nWuu3fNOe2Ru9CRgHbALaACFAT631V5n3Bn1fHON8y70WmK21ftK3rOeBYcAh4H/A01rr67PI1wd4\nEuN9ewzoprU+pJTqCTQHEoFagAvoqLX+SSlVAZgDXAVsJ4v3vFKqGfC61vq2TMO+B54FvstuG2Sa\ntifGl2KDnNanlGoFvAbYgGSgt9b6e+Br4BrfnvjtQDpwrdb6iFJqCDAAoxlUA3201nG+bXsIqA5U\nAn4BWmutU89/fZnWfzvwAVAScADPaK3XKqXCgU+Byr7XuB54zPf4guFaa2d26xD+IW3k4nxVgVu0\n1l8DLwJ/+vYQ6wPjlFLXZjHPXcB2rfXNwPu++bLyIPCAbx2PK6WuUUrdgrE3egdGEe6U1YxKqVLA\ne0BDrXVF4DdgZKZJmgHva60rARsxvhgAxgPrtdY3Au8ANbJY/JcYhfQG37puAK7xDc/tNjgjy/Up\npawYXwh9tdYKWAa86ZunF3BYa11Za52R6TVXA54C6vjWfxjjy/CMjkBnjGayWKBtdqGUUmZgLvCe\nb1l9gDlKqQigBxDv+/+rhPFFeEsOw0UR47dCrpS6VSn1u1Jq8EWmi1FKrVFKLTxv+Ail1PdKqZ1K\nqXsLNm2xskpr7fE9HgI8DqC1/gP4B7ghi3mStNbLfI/3ANdls+zZWmu31voo8C/GnvmDwCat9TGt\ntQOYntWMWuvjQKTW+ohv0BagQqZJftZa784iw4PAPN8ydgAHslh2BrACaOUb1BZYqrV2XcI2OCPL\n9fmWVUprvT2b/FlpDiz0vXaAj4FGmcav1Fqf8i37R7Lf7vgyl8Eo5mitd2Hs0d8LHAceUEo1Aixa\n64G+XwrZDRdFjF+aVpRSYcC7GD/VLuZDYCtwZ6b5bwG6APdg/BRtDezM/6TF0qlMj+/F2AO9DnAD\nV5P1l39CpsduwJLNsrOaLua8df6d1YxKKQvwqq95wgJEYDQnXCxDifPGnc4m20JgKMZedBtgtG94\nbrfBGTmtb4hSqgdGM0UIcLGOjmIxDkhnXlapTM9zu93PLCtea515nacxvlzmKqVKYLzmykqpWcBw\nrfWCbIanXyS3KGT+2iNPx/gp/N+bVClVRSm1QSm1Xim1VCkV7RvVB6OQZ9YCmK+1dmmt92itXy6c\n2MXOLIwCV8n3czyuANaRCIRnen51NtN1xthjftDXNJHb//PTQFSm57HZTLcWuFMpVRGjGWGDb/il\nboMs16eUqg48A7Ty5e+Ti+z/YrRnn1HSNywv/gVKKKVMWS1Paz1Fa30/UAWj6at7TsNF0eKXQu4r\nwGnnDX4X6K+1rg+sAwb5ps3qlLjrget8TS7rlVJ3FGjg4qsUsFtr7fXtSYZxbtHNDzuAukqpq5RS\nwRjtstllOai1PqGUKonRlp6bLN/gazv2FdObsprIt5e5Fvg/YJnW2p1pvZeyDbJbXymMporDSqlQ\n3+sM8xVWJxDua0fPbCXQzvd6Afr7huXFQeAIxhfimWxlgB1KqZFKqV4AWuu/gT8Bb3bD87h+UYCK\n0sHO+4CPfGdOdANK5zCtCeNnZFOMPbOPCzxd8TQSWKKU2otRvKZg/B/dmF8r8LUjf4JxdsgGjLbq\nrIrFHKCkUuo33+MXgWuVUhMusoqngZZKqd+BwcAXOUy7EKNZZX6mYZe6DbJb3xqMX6C/Y+yoTMRo\nGlkI7MVoXvrH14QD/LdtxgNbfGe0RAMvXOT1ZsnXpNIFGKyU2g9MwjizJwXjzJRuSintW0+Gb1h2\nw0URY/Jnf+RKqVHACa31e0qpf4Ey57XhnZmuDjBYa93B9/wV4IDWeo7veZzWOrufzKKIU0qZzvy/\nK6WaA2O01nf5OZYQAaMo7ZH/ADQBUEp1UUrVz2Ha1UBj37SVgb8KPp4oCEqpWOCEUqq8r5mhE0bz\nhBAil/yyR66UqgpMwGjrdmKcqfACxs9ID8Zl410xfnqux/hJWQ74CXhVa73Bt1d+5lSs4Vpr+fAH\nKKXUAGAERpPKAYwLZY7nPJcQ4gy/Nq0IIYS4fEWpaUUIIUQeFPoFQXFxSXn+CRATE8rp09l2JVHk\nSN6CFUh5AykrSN6ClNessbERpuzG5aqQK6X+D6MfDCswTmu9ONO4gxgHG8+cd/uw75zTfGe15nTh\nWtEjeQtWIOUNpKwgeQtSQWS9aCFXStUFbtVaP+C7MOE7YPF5kzXVWifnezohhBAXlZs28s0YvawB\nxGNcjRY4X39CCHGFu6SzVpRS/YBaWutumYYdxOgL5Xrf3+eyuqjnDJfL7Q2kn0FCCFFEXF4bOYBS\nqjXGTQEanTfqJYzLj08BS4H2GJcdZ+lyDkjExkYQF5fru5H5neQtWIGUN5CyguQtSHnNGhsbke24\n3B7sbIxxwU4TrXXmrjPRWs/MNN0q4DZyKORCCCHy10XbyJVSUcAbQAut9anzxyml1iqlbL5BtYF9\n+R9TCCFEdnKzR94Z4/6D85VSZ4ZtAH7UWi/x7YVvV0qlYZzRInvjQghRiC5ayLXWU4GpOYx/B+Ou\nKgXK/PcRGPkO4clpYLPhtQVDcDBeux1PTAm8MTFn/5Yug6dUaTDLhatCiCufX271lhfWH76HKVOw\n53J6r82Gp2w53Ndci/va63DfWBF35cq4KlXGc115KfJCFCHvvvs2Wu/n1KmTOBwOypYtR2RkFGPH\nvnHReVetWkHZsrHceWe1LMe/884EOnbsQtmy5fKUbfDgfgwf/jQVKmR5T5IiIWAKeUazFnDsGKcO\nHYP0DEzODEhPx5SSgjn+NKZTp4y/p09hPnYMy5HDmI8cwbZ18wXL8trtuFRlXHfejfPue3DdVRV3\nxUpS3IXwk8cffwIwivIff/zO4MHDcj1vs2YtczwTZOjQJ/MlY1EWMIUcgDJlcFvCLm0ehwPLkb+w\n/PoLVr0fiz6ARR/A+vNPBH3/HfYZ0wDwhEfgqnoPGbVq46z5IK7b7wRrYG0eIa40e/bsYu7cWaSm\npjJ48BN8991uNm1aj8fj4YEHatCrVz+mTZvCNdeUITa2HIsXz8dkMnPo0J/UqVOfXr36/bdHvXHj\nelJSkjl8+BB//32EIUOe5IEHajBr1gy+/HIdZcuWw+Vy0aXLw9x99z0XZElOTua110aRnJyEy+Vi\n2LCnUKoyEye+wYED+3G73bRt24FmzVpmOawgXfmVKiQE900Vcd9UkYymzc8Oz8jA+vM+rHt2E/Td\nbqx7dmH7aiO2rzYC4ImIxFm9Bhl1G5DRuCmectf46QUIUbhGjQpm5UrweC5xpykHLVu6GDUqPU/z\n/v77b8yZsxibzcZ33+3m/fc/xmw206lTazp37nrOtD///BOzZy/C4/HQsWNLevXqd87448f/5c03\nJ7F9+9csW7aIW265lcWLFzBnziJSUlLo0qUdXbo8nGWOBQvmcMstt/LIIz05cOBn3n33LcaOfYOv\nv97K/PnLcLlcrFq1gsTEhAuGFbQrv5Bnx2bDdefduO68Gwd9ATDFxWH7egtBWzYTtPUrgteuJnjt\nanj2SZy33UFG46ZkNGmG67Y7wJTtRVZCiHx0000VsdmMM5xDQkIYPLgfFouF+Ph4EhMTz5lWqcqE\nhIRku6zbb78TgFKlSpGcnMyRI39RocKNBAeHEBwcws0335LtvAcO/Ez37r0BqFy5CkeO/EVkZBTX\nXlueZ58dTt26DWjSpDk2m+2CYQWt+BbyLHhjY0lv3Y701u0AMP91GNsXawleu4qgrZsJ+vEHwt4c\nj7v89TjadSC9XSfcqrKfUwuRv0aNSmfyZBtxcSn+jgJAUFAQAP/8c4x58z5j+vTPCA0NpVu3ThdM\na7Hk3P1H5vFerxevF8yZjo3ltH9mMpnI3KWJx+MBYMKESWh9gC++WMOaNSt5++3JWQ4rSHJ0Lwee\na6/D0asvCfOWcPLAnyRMm4mjXUfMcXGEvf0mJWrdR0zdGtgnvY35n2P+jivEFS0+Pp6YmBhCQ0PR\n+gD//PMPTqfzspZ59dVX88cfv+NyuTh9+jQHDuzPdtrKlavw3Xe7ANi370duuOFGjh07yoIFc1Gq\nMoMHDyMhISHLYQVN9shzyRsRSUbLNmS0bENSSgrB61YTvGQhtvVfED7mZcLGvUpGo6Y4uvcko059\nuMiegRDi0lSsWAm7PZSBA3tx22130rp1OyZMeJ3bb78jz8ssUaIkDRs2oW/f7pQvfwNVqtyS7V59\np04PMXbsKwwZMgCPx8Pw4c9w1VWx7Nv3A+vXryMoKIjmzVtlOaygFfo9Oy/nDkFFsWMc0+lTBC9d\nTMinMwjatxcA9zXX4ni4O2HDBhN3qWfZ+FFR3L45CaS8gZQVilfeVatW0LBhEywWC927d+Gtt96l\nVKnS+ZzwrMvoNCvbhh9pWrlM3pgSOB7tQ/z6LZxet4m0bj0xnzpF2OuvQfnyRAwZiOUn6X5GiKLq\n5MmT9OvXgwEDetGoUZMCLeIFJaD2yL3eCI4fTyYoyIvNBjYbBAUVvRNITMlJBC+YR8S0D+GXXwDI\nqFWHtIGDyKjXsMheeFSc9sIKWyBlBclbkApijzxg2shXrbLSsydA+DnDzWYv0dFeYmLw/fVSsqSX\ncuU8XHPN2b/XXOPBntvr+y+TNzwCx6N9iBgxlIQ5C7F/OBnblk3YtmzCVeVWUoY/RUaL1kW2oAsh\nAkvAFPI77nDTrx8cP+4kIwOcThPp6ZCWZiIhAU6fNnH4sBmnM+svLZPJS/nyXipXdlOpkgelPNx8\ns4fKlT0FdwGn2UxGo6ZkNGqK5ce9hL4/ieAlC4nq0wOXqkzqE08ZpzrKgVEhxGUIqKaVi/0k8Xoh\nJQWOHzdx9KiZI0dMHDli/P3zTzNamzl16ty9YLvdy+23u7nrLg9Vq7q55x435crlzzbJKq/lj98I\nnTiB4AVzMbnduG68idQnnyG9XUe/76EH0s9TCKy8gZQVJG9BKoimlSuqkOfGiRMmtDaK+r59Zvbs\nsXDggBmP5+w2uuEGD7VquahVy02NGm6uuipvkXPKaz74J6Hvvk3I3M8wOZ24brmN5JGjcNZt4LdG\n/0D6MEBg5Q2krCB5C5KctZIPrrrKS40abnr1cvLWW+ls2pTK778ns3x5Ki+/7KBxYxcnTpiYOdNG\n3752qlQJp169UF5/3cYPP5jJr+89z/U3kDxhEqe+2YOj00NYft5HdJf2RLVvifW73fmzEiECRP/+\nj15wMc6HH77HnDmzspx+z55dvPji0wA8++zwC8YvWjSPadOmZLu+3377lcOHDwHw8svPkZ7uyGt0\nOnRoSWpq3u9FnB+KXSHPSlgYVKvmZtAgJ59+mobWyaxencILL6RTq5aLX34xM2FCMA0bhnHnnWE8\n/XQwGzZYuMyLygDwXFeepPemcHrDNtIbNMK2dTMxjesS2bs75oN/Xv4KhAgADRs2ZsOGL84ZtmnT\nBho0OP9e7xcaP/6tS17fV19t4K+/DgPwyivjCA7Ovn+WQBAwBzsLk9UKVat6qFo1g6FDITkZNm60\nsmaNlS+/tDJjho0ZM2yULOmhdWsX7do5ufdez2W1iLhvuZXE2QsJ2raFsNEvEbxiKbZ1q0l97HFS\nhzxpfNsIcYWqX78RAwf25rHHhgBw4MB+YmNjiY0txc6d3/Lxxx8SFBREREQEr746/px5mzevz44d\nO9i1aweTJk2gRImSlCx51X/d0r722iji4o6TlpZGr179KFPmapYtW8xXX20gJiaGl156jpkz55Gc\nnMS4ca/idDoxm808++xITCYTr702irJly/Hbb79SqZLi2WdHZvkajh//94L5S5UqzauvjuTkyRNk\nZGTQu3d/mjSpx0svPXfOsGrVql/W9pNCngvh4UY3nC1bunC5YMcOCytWWFm2zMr06TamT7dx3XUe\n2rVz0qWLkwoV8t7+4qxRi/jVGwhespCwV0YS9vabhMybQ8qoMcYZLkXtpHlxxQkb9SKsXEYJT/4d\nP0tv2YaUUWOyHR8TU4KyZcvx88/7qFLlVjZs+IKGDZsAkJSUxMsvj6Fs2XKMHv0S3377DaGhoRcs\nY8qU9xg5cjQVK1ZixIghlC1bjqSkRO67rxpNm7bg77+PMHLks0yfPov773+AOnXqU6XKrf/N//HH\nH9KiRWvq12/Exo1fMn36VHr37o/W+3nllbHExJSgbdtmJCUlERERccH6s5q/Y8eHSEiIZ/Lkj0hK\nSuKbb7bxyy+/XDDscknTyiWyWqF6dTfjxqWzd28Kc+em0qGDkxMnTEycGEy1auG0b29n2TIrGRl5\nXInJRHq7jpz6ejcpT4zAfCKOyH6PEtWmmVwlKq5YDRs2Yf16o3ll27bN1KlTH4Do6Ghef30Mgwf3\n47vvdpOYmHUnVMeOHaNixUoA3Hnn3QBERESyf/9PDBzYi9deG5XtvABa7+euu6oCcPfd9/DrrxqA\ncuWupWTJqzCbzVx1VSwpKcm5nr98+etJTU1h9OiR7NmzkwYNGlGhQoULhl0u2SO/DFYr1Kvnpl49\nNykpxkVLs2YFsWWLlS1brLzwAnTubKNHDyfXXZeHvZuwMFKfewlHl0cIf/kFgtesJKbhg6Q9NoSU\nJ5+h0K5wEsVKyqgxhE5+h1OFfBZI7dp1mTlzOg0bNubaa68jMjISgHHjRvPGGxO5/vobeOut17Od\nP3N3tGfOxvviizUkJiYyefLHJCYm0qdPtxwSnO2m1ul0YTIZyzu/E63sz/S7cP6QkBCmTJnBjz/u\nZfXqFWzbtoW3337zgmHPP/9yTpvmomSPPJ+EhUHHji6WLUtj69YU+vfPwOmEd98N5r77wujTJ4Rd\nu/K2uT03VCBx5hzi5y7CU7YcoZPeokTtagT57mYkxJUgNDSMG2+syMyZ//uvWQUgJSWZ0qXLkJSU\nxJ49u7Ptuvaqq2I5fPggXq+X73xnfsXHx3P11WUxm8189dWG/+Y1mUy43e5z5r/55irs2WN0U/v9\n97upXPnmS8qf1fxn+iS/4447GTHiOQ4e/JOffvrpgmGXSwp5AahUycPo0ekcPQrvvptGlSoeli8P\nolmzMJo1C2XFCivnvYdyxVmvIae+2k7qoKGY/zpMdMfWRAzqh+nEifx/EUL4QcOGTdi581tq1nzw\nv2Ht2nVk4MDe/N//vcbDD3dn1qwZnDx54Xu+X7/HePHFZ3jmmSf+6/iqTp16fP31FoYOHYjdbqdU\nqVL8738fcccddzFx4hvs2rXjv/n79BnAmjWrGDJkAKtWfU7v3v0vKXtW8199dVnWrl3NY4/1Ydiw\nx+jatRvXXHPNBcMuV7G7IKgwncnr9cK2bRY+/NDGunVGa9aNN3oYOjSdDh1ceeoiwPrjD4QPH0LQ\nD9/hKVGC5HFvkt6m/WUdDA3U7RsIAikrSN6CJBcEBSiTCWrWdDNrVhpff51Mt24ZHD5sYsgQO9Wq\nhTFrVtAlHxh13XYH8avXkzx6HCaHg8j+vYjs00P2zoUohqSQF7KbbvIyYUI6336bwqOPZvDPPyaG\nDw+hWrUwZswIurSLjKxW0voP4tSGbTjvf4DgFUsp8eD92FZ9XmD5hRBFjxRyP7nmGi+vv57Ozp0p\n9OuXwYkTJp5+OoQaNcJYutSK776uueKpcCPxS1eRPOo1TEmJRPXsarSdJ8QX3AsQQhQZUsj97Oqr\nvYwZYxT03r0zOHLERL9+dho1CuWrry6he1uLhbTHHuf0l1tw3nkXIQvmEvNgNYK2fFVw4YUQRYIU\n8iKidGkv48als21bCu3aOdm710LHjqF06GDnxx9z/9/kVpWJX/klKc+8gDnuOFEdWhH22ivkS8cw\nQogiSQp5EXPDDV4+/NDBl1+mUKeOi82brTRoEMqIEcGcPJnLM1KCgkh98hniV6zFc215Qt+ZQHSr\nxtIJlxBXKCnkRdTtt3uYPz+NefNSqVjRw8yZNqpVC+Pjj4NwuXK3DFfVezm9cSuO9p0I2r2LmHo1\nCV40v2CDCyEKnRTyIq5uXTcbN6YyZowDrxeefz6EevVC2bw5d+3n3ohIkj74mMT3poDXS+TAPkQ8\nPsC4lZIQ4ooghTwABAVBv35OvvkmhUceyUBrMx06hNK3bwj//pu75pb0Tg9xer3vQOi82cQ0rYfl\nt18LOLkQojDkqpArpf5PKfWNUmqnUqrdeeMaKKV2+MZn3VGvyBexsV7eeiuddetSqVrVzbJlQdSo\nEcYnnwTl6nRFT4Ubif/8C9J698N6YD/RDWtjW76k4IMLIQrURQu5UqoucKvW+gGgCTDxvEkmAe2B\nGkAjpVSVfE8pznHHHR5Wrkxl/HijueWpp0Jo1crOgQO5+F622Uge9yaJU6Zj8nqJ6tODsJHPylkt\nQgSw3OyRbwY6+h7HA2FKKQuAUqoCcEpr/ZfW2gOsAuoXSFJxDrMZevVysm1bCi1aONmxw0r9+qGM\nH28jPf3i86e37cDptRtxVVKETnmf6DbN4MiRgg8uhMh3Fy3kWmu31vrMkbHewCqt9Zm++8oAcZkm\nPw5cnb8RRU7KlPEyfbqDTz9NpVQpL2+9FUyjRqG5OvfcrSpzes1GHG3bE7TzW7j7brmASIgAlOve\nD5VSrYHngUZa6wTfsOrAU1rrtr7nfYAKWuvns1uOy+X2Wq2XcMWiyLWkJBgxAqZONW568eKL8Pzz\nxsHSHHm9MHkyDB8OHg9MnAiDBslt5YQoWrL9QOaqkCulGgOjgSZa61OZhl8PzPG1n6OUehk4qbV+\nL7tlFcdubAvbxo0WnngihKNHzdx2m5tJkxzccsvFj4bG6h/wtG2H+UQcaQ93J3n8BAgOLoTEeRNI\n74dAygqStyD5pRtbpVQU8AbQInMRB9BaHwQilVLXK6WsQAtg3SUnFPmqbl03mzen0LVrBj/+aKFR\no1Deftt28QuJatbk9LpNOG+7A/tnM4lu1wLT8eOFklkIkXe5OdjZGbgKmK+U2uT795JSqq1v/EBg\nDrAFmKe1/qWAsopLEBkJEyemM3t2KiVLehk3Lpi2be389VfOzSWea64lfsVaHG3aEbTzW2Ia1cb6\nw3eFlFoIkRdyh6ACVFTynj4NI0aEsGJFEJGRXt54w0Hbthfunp+T1+vFPuktwsa+CsHBJE2cTHq7\njhfM409FZfvmRiBlBclbkOQOQSJPYmLg448dTJyYhssF/fvbefzxEJKTc5jJZCJt6JMkfjoXrzWI\nyAG9CX39NePAqBCiSJFCXkyYTNC1q4sNG1K480438+YFUa9eGHv25PwWyGjUlPg1G3Bfdz1hE14n\n4rG+5OpEdSFEoZFCXsxUqODl889TefzxdA4dMtGiRShTpgTluKPtrqQ4vWYDznvuI2TRfKI7tMJ0\n8mThhRZC5EgKeTFks8HIkRksWJBGTIyXkSNDePTREOJzuDOc96qriF+0AkfrdgR9+w3Rzepj+V06\n3RKiKJBCXow9+KCbDRtSqV7dxapVQVStCnv35vCWsNtJmjKdlGEjsP75B9HNGhD0zbbCCyyEyJIU\n8mKudGkvCxem8cQT6fzxBzRrFsqMGTk0tZjNpD7/EkkTJ2NKSiKqQyu5WYUQfiaFXGC1wnPPZbB6\nNYSHe3n66RAGDgzJ8d4Tjq7dSJi7GK89lMiBfbB/kO3FvEKIAiaFXPynSRNYvz6Ve+91s3hxEM2a\nhXLwYPYXEDkfrEP88jW4y1xN+MvPE/byC+SqY3QhRL6SQi7OUa6clyVLUnn00Qz277fQqFEYGzdm\n38mZu8otxK/8AlfFSoR+8K5xemJGRiEmFkJIIRcXsNng9dfTmTgxjdRUeOghO5Mm2bJtN/dcex3x\nK9birHovIYsXEPVwR0zJgXGVnRBXAinkIltdu7pYvjyV0qW9jBkTTN++2V8N6i1RkvhFK0hv1ATb\nVxuJatNcOtwSopBIIRc5uvtuD+vWpXL//S6WLw+iefNQ/vwzm3bz0FASZ8wm7eHuBO39npjmDTD/\n8XvhBhaiGJJCLi6qdGkvixal0auX0W7epEkYX3+dTbu51UryW++SMvwpLIcOEtOiEZZ9PxZuYCGK\nGSnkIldsNhg/Pp2333aQlAQdO9qZPdua9cQmE6nPjiRp/ARMJ08Q3bY51p3fFm5gIYoRKeTikjz8\nsJMFC9IID4dhw+y88kpwtmccOnr1Jem9KZiSk4ju2IagrzYWblghigkp5OKS1ajhZvXqFG680cPk\nyTZ69sz+IGh6xy4kTp8FLidRD3fEtnpl4YYVohiQQi7ypEIFL6tXp1Crlos1a4Jo1SqUo0ezPgia\n0bQ5CZ8tAKuVyF6PELxwXiGnFeLKJoVc5Fl0NMydm0a3bhns22ehceNQvv8+67eUs3Zd4ucvwxsW\nTsSgfoTMmFbIaYW4ckkhF5clKAjefDOd0aMdHD9uonXrUD7/POuDoK777id+yUq8JUsS8fQT2Ce9\nXchphbgySSEXl81kgv79ncyalYbZDL17h/DRR0FZTuu+7Xbil6/FXbYc4WNeJuy1V+T2cUJcJink\nIt80bOhm2bJUYmO9vPBCCC+9lPUZLe6bKhK/Yi2uGyoQ+s4Ewl56Toq5EJdBCrnIV7ff7mHVqlQq\nVXLz4Yc2+vcPweG4cDrPtdf4QY4/AAAgAElEQVQRv3wtLlWZ0CnvE/78U1LMhcgjKeQi3113nZcV\nK1KpVs3FsmVBdOpk5/TpC6fzli5N/OKVuG6+Bfu0qYQ/9YR0gytEHkghFwUiJgbmz0+jVSsn27db\nadEilMOHLzw90RsbS/ziz3Heejv2mdMJH/44uN1+SCxE4JJCLgpMSAhMnepgwIAMfv3VQrNmoezb\nd+FbzluyJAmLluO84y7ssz8lYshAKeZCXAIp5KJAmc3w6qtnTk8006ZNKNu3X9jhljemBAkLl+Gs\neg8hC+YSMagvuFx+SCxE4JFCLgpF//5OPvjAuFFFp0521q7NophHRZMwfynOe+8nZPFCIgb0BqfT\nD2mFCCxSyEWhad/exaefpmEyQc+edubOvfDCIW9EJAnzFpNRrTohy5cQKcVciIuSQi4KVf36bhYu\nTCUiAoYMsfP++xdeOOQNjyBhziIyqtckeMVS4z6g0swiRLakkItCd++9HpYvT6VMGQ+jRoUwZkwW\n9wMNCyNh1nyc9z9AyLLFRAzuJwdAhciGFHLhF5Ure/j881QqVPAwaVIwTz4ZfOFOd3g4CXMWnm0z\nf3yAFHMhsiCFXPjNmQuHbr/dzaxZNvr0ufAqUG94BAlzF+Gsei8hC+cRMWyQXDQkxHmkkAu/io31\nsmRJKjVruli1KoiHH7aTknLuNGcOgDrvupuQebONi4akmAvxHynkwu8iImD27DSaNHGyZYuVzp3t\nJCaeO403Mso4NdF30VD4U8OkmAvhk6tCrpS6VSn1u1JqcBbjDiqltiilNvn+lcv/mOJKFxIC06Y5\naNfOyY4dVtq3D+XUqXOnMc4zX4LztjuwfzoDBg+WjraEALK5DfpZSqkw4F1gfQ6TNdVaZ3PXRiFy\nJygIJk92YLd7+ewzG23bhjJ/fhqlS58t1t6YEiQsWEp0+1ZYP/iA8Aw3yWPfMDpFF6KYys0eeTrQ\nDDhawFmEwGKBCRPS6ds3g/37LbRuHcrff59bpL0lShK/cDncdhv2aVOlP3NR7Jm8ufwAKKVGASe0\n1u+dN/wgsBW43vf3Oa11tgt1udxeq/XCy7OFyMzrhRdegHHjoHx5WL8ebrzxvIni4qBuXfjpJ3j+\neXjtNb9kFaKQZPuz86JNK7nwErAGOAUsBdoDC7Ob+PTp1DyvKDY2gri4pDzPX9gk7+V54gkwmWyM\nHRtMjRoeFi5MQ6mzBzhjY2M5MXcp0a2bYB07lhSspA4b4cfE2Stq2/ZiJG/ByWvW2NiIbMdd9lkr\nWuuZWuvjWmsXsAq47XKXKcQZw4ZlMHq0g3//NdOmjZ0ffzz3LestXZqERStwX3sdYWNfxT5lsp+S\nCuE/l1XIlVJRSqm1Simbb1BtYN/lxxLirP79nUyY4ODUKRPt2oWya9e5b1tPuWuIX7gcd5mrCR/5\nHCGfzvBPUCH8JDdnrVQFJmC0gTuVUh2A5cCfWuslSqlVwHalVBrwHTk0qwiRV926ObHbvTz+eAgd\nO4Yyd24aLVqcHe+5oQIJC5cT3boJ4SOG4g0JIb1jF/8FFqIQ5fpgZ36Ji0vK8woDqR0MJG9BWLHC\nSv/+IdhssHq1iZtvPjev5ce9RLdrgSk5icSPPiGjRSs/JT1XIGzbzCRvwbmMNvJsD3bKlZ0ioLRs\n6eLjjx04ndC0KWzbdu4ZUO7bbidh7iK8IXYi+z+K7cu1fkoqROGRQi4CTrNmLqZPT8PphK5d7Wze\nfG4xd1W9l8TP5oPVSmSvbgRt3eynpEIUDinkIiA1buxmyRKjV9tHHrGzceO5xdxZvSYJ//sMPB6i\nHumMdee3fkoqRMGTQi4CVvPmMHNmGl4vdO9uZ8OG84p5vQYkfvQJpDuIeqgD1r3f+ympEAVLCrkI\naPXquZk507gPaPfudr788txintG0OUmTp2JKSiSqUxssB/b7KakQBUcKuQh4deu6mTUrDYsFevSw\ns3btucU8vV1Hkt9+D/OpU0R1bI354J9+SipEwZBCLq4IDz7o5rPP0ggKgl697Kxade4lEo6u3Uge\nPQ7Lv/8Q3bE15n//8VNSIfKfFHJxxahZ083s2UYx79MnhJUrzy3maf0HkfLkM1gOHSSqUxtMp09l\nsyQhAosUcnFFqV7dzdy5adhs0LdvCKtXn1vMU59+ntS+A7Du/5morh0gWbrRF4FPCrm44lSrdraY\n9+kTcm6buclEyujxODp3JWj3LqJ6dOWCOz4LEWCkkIsrUrVqZ5tZevWys25dpmJuNpP09nukN22B\nbcsmIvv3ApfLf2GFuExSyMUVq3p14wCo1WoU83NOTbRaSZwynYxatQle/TkRwwbJzZxFwJJCLq5o\nNWoYpyaazdCz53kXDYWEkPjJbJxV7yFk/hzCRj4rt4wTAUkKubji1arl5tNPjWLeo8e5l/N7wyNI\nmL0Q181VCP3oQ0LfGOfHpELkjRRyUSzUrm1cAQpGMd+0KVMxjylBwvyluMtfT9ib4+UuQyLgSCEX\nxUadOm4++eRs3yyZe030lC5j3GWodBnCRz5H8JxZfkwqxKWRQi6KlXr13MyYkYbHA9262dm6NVMx\nL389CQuW4YmJIeKJwdg+X+7HpELknhRyUezUr28Uc7cbHn7Yztdfny3m7so3kzB3MV57KJEDehH0\n1UY/JhUid6SQi2KpQQM306en4XIZN6fYvv1sMXfdVZXET+eCyURUj65Yd+3wY1IhLk4KuSi2GjVy\n8/HHDjIy4KGH7Hz77dli7qz54Dl9mVt+2ufHpELkTAq5KNaaNnUxdaoDh8Mo5rt2nf1IZDRpRtKk\nDzAnxBPdqQ3mP373Y1IhsieFXBR7LVq4mDLFQVoadO4cyp49Zz8W6R27kDTuTcxxx41ifuyoH5MK\nkTUp5EIArVq5+OADBykp0KlTKD/8cPaj4ejdj5RnX8Ry+JDR/e2pk35MKsSFpJAL4dOmjYvJkx0k\nJ0PHjqH8+OPZj0fqE0+R2n8QVn2AqIfaY0pO8mNSIc4lhVyITNq3dzFpkoOEBOjQIZR9+3wfEZOJ\nlFfHkvbQIwR9t4fI7g9J97eiyJBCLsR5OnVy8c47DuLjoUMHOz//fLaYJ0+YRHrzVti2biayX0/p\n/lYUCVLIhchCly4uJkxI59QpMx062DlwwPdRsVpJ/HAaGQ/WJXjNKiKGPibd3wq/k0IuRDYeecTJ\nG284OHHCTLt2dn75xfdxCQ4mYcZnOKveS8iCuYS9+Ix0fyv8Sgq5EDno0cPJ+PFni/lvv5mMEeHh\nJMxeYHR/+/EU6f5W+JUUciEuolcvJ2PHOjh+3EzbtqH88YdRzC/o/nbq+35OKoorKeRC5EKfPk5e\nfdXBv/8axfzPP41ifk73ty8+S/Dcz/ycVBRHUsiFyKUBA5y8/LKDY8fMtGsXysGDvmKeufvbYYOw\nrVzh56SiuJFCLsQlGDTIyYsvpvP330YxP3zYKObuyjeTMGcRhNiJ7P8oQZs3+TeoKFZyVciVUrcq\npX5XSg3OYlwDpdQOpdQ3SqmR+R9RiKJlyJAMnnsunSNHjGJ+5IhRzF1330PCzDkARHV/COvunf6M\nKYqRixZypVQY8C6wPptJJgHtgRpAI6VUlfyLJ0TR9MQTGTz9dDqHDxtt5kePGsXc+WAdEqfO8HV/\n2x7L/p/9G1QUC7nZI08HmgEXdPumlKoAnNJa/6W19gCrgPr5G1GIomnEiAyGD0/n0CGjmB87ZhTz\njGYtSHr7Pczx8UR1aoP54J9+TiqudNaLTaC1dgEupVRWo8sAcZmeHwduzGl5MTGhWK2WnCbJUWxs\nRJ7n9QfJW7D8nffNNyEkBMaONdOxYzibNsHVVwOPDwBPOpZhwyjZuQ1s3Ups2bJ+zXqp/L1tL1Ug\n5c3vrBct5JfIdLEJTp9OzfPCY2MjiIsLnF7nJG/BKip5hw6FxEQb770XTO3abhYvTqN0aS907UXo\nkX8Ie3M8NG7MiUWf440p4e+4uVJUtm1uBVLevGbNqfhf7lkrRzH2ys8oRxZNMEJcyUwmGDkyg4ED\nM/j1VwsdOtiJizP2aVKfeo7UvgNg3z6iunaA5GQ/pxVXossq5Frrg0CkUup6pZQVaAGsy49gQgQS\nkwlGjUqnX78MtDaK+YkTJqP729HjoXt3gnbvIqpHV+n+VuS7izatKKWqAhOA6wGnUqoDsBz4U2u9\nBBgIzPFNPk9r/UsBZRWiSDOZYPTodNxumDbNRocOdhYvTqVECTNMm0b68ZMEr1lJZP9eJE6bCdb8\nbtkUxVVuDnbuBurkMH4z8EA+ZhIiYJlMMHasUcxnzLDRoUMoixalEhtrJXHq/4jq2oHg1Z8TMfxx\nkiZOBrNckycun7yLhMhnJhOMH59Ot24Z7NtnoWPHUE6fBkJCSJw5B+dddxMy9zPCXn5eur8V+UIK\nuRAFwGyGN95Ip2vXDPbutdC4MSQkgDc8goQ5i3CpyoROeZ/QCa/7O6q4AkghF6KAmM3w1lvpdO7s\nZOdO6NIllKQk8JYoaXR/e115wv5vrHR/Ky6bFHIhCpDZDBMnOnjkEdi920KXLqEkJ4Pn6rLEz1+K\nu1Rpwl98lpBPZ/g7qghgUsiFKGAWC8yYAe3aOdm508JDD9mNYl7hRhIWrcBTsiThI4YSvGCuv6OK\nACWFXIhCYLHAe+85aN3aybffWnn4YTspKeBWlYmfvwxvZBQRjw/AtnyJv6OKACSFXIhCYrXC++87\naNHCyTffWOnWzU5qKrhvu52EeYvxhoYROaA3tnWr/R1VBBgp5EIUoqAgmDLFQbNmTrZutdK9u520\nNKMv88TZC8BmI7JXN4I2bfB3VBFApJALUciCgmDqVAdNmjjZvNlKjx52HA5wVqtOwidzwGQiqsdD\nBH2zzd9RRYCQQi6EH9hs8NFHDho2dLFpk5VHH7WTng7O2nVJnP4puFxEdu2IddcOf0cVAUAKuRB+\nEhwM06alUa+ei/XrrfTqZRTzjIZNSPxwOiZHGlFd2mPd+72/o4oiTgq5EH4UEgIzZqRRu7aLL76w\n0rdvCBkZkNGyNUnvTcGUlEhUpzZYDuz3d1RRhEkhF8LPQkJg5sw0atVysWZNEP36heB0Qnr7TiS/\n/R7mU6eIbt8Sy++/+juqKKKkkAtRBNjt8OmnadSo4WLVqiB69TIOgDq6diNp3JuY444T1b6V3P9T\nZEkKuRBFRGgozJqVxoMPuli71sojjxgXDTl69yP55TFYjv5NdLsWmA8d9HdUUcRIIReiCAkLM4p5\n48YuNm+20rmzncRESBs0hOQXXsZy5C+i2zaXYi7OIYVciCImJASmT0+jTRsnO3ZYad8+lFOnIG3o\nk2eLebsWmA8f8ndUUURIIReiCAoKgg8+cNC1awY//GChbdtQ/v3XRNrQJ0l5/iUsfx029sylmAuk\nkAtRZFksRn/mffpksH+/hVatQjlyxETqsBFSzMU5pJALUYSZzfDaa+kMHZrOn3+aadUqlD/+8BXz\n50YaxVyaWYo9KeRCFHEmE7zwQgbPP5/OkSNGMT9wwEzqE0+R8uyLWA4fMor5X4f9HVX4iRRyIQLE\nsGEZjBnj4PhxM23a2Nm710zq8KfPFvO2zaWYF1NSyIUIIP36OXnrLQenT5to1y6UHTt8xfyZF4xi\n3rop5j9+93dMUcikkAsRYB55xMkHHzhISYFOnULZssVC6pPPnD01sXVTLPqAv2OKQiSFXIgA1K6d\ni+nTHbhc0LWrnS++sBjnmY8Zj+Xff4hu0xTLj3v9HVMUEinkQgSopk1dfPppGmYz9OhhZ/lyK2n9\nHiPpzXcwnTpFdLsWWHfv9HdMUQikkAsRwOrWdTNvXhohIdCvXwhz5lhxdH/0bBe4HVrLnYaKASnk\nQgS4atXcLFqUSlQUDB1q5+23bTg6dCHxoxmY0h1EdWmHbf06f8cUBUgKuRBXgLvu8rBiRSrXXONh\n3Lhgnn02mLRmbUicOQe8XiK7dSF46SJ/xxQFRAq5EFeISpU8rFqVSpUqbv73Pxu9e4eQUKMxCfOW\n4A2xE9G/FyGfzvB3TFEApJALcQUpU8bL8uWp1Kxp3KCiY0c7xyvXIGHJ53hLlCDiySHY353o75gi\nn0khF+IKExkJc+ak0bat0Q1uy5ahHCx5N/HL1+IuW47w0S8RNmYUeL3+jiryiRRyIa5AwcFGN7gD\nBmTwyy8WmjUL5XtHZeJXrMV1QwVCJ71F+NPDwePxd1SRD6y5mUgp9TZQDfACQ7XWOzONOwj8Bbh9\ngx7WWv+dvzGFEJfKbIZXX03n6qs9jBoVTMuWoUydegONV6wjunNb7J9Mw5RwmqR3pxiVXwSsi+6R\nK6VqAxW11g8AvYFJWUzWVGtdx/dPirgQRcjAgU6mTXPg9UL37namLitH/NKVOO9/gJCli4nq3BZT\nQry/Y4rLkJumlfrAUgCt9X4gRikVWaCphBD5qkULF0uXplKypJcXXgjh2fGlOTF7KenNW2H7eivR\nLRtj/vuIv2OKPDJ5L3LAQyk1FViptV7me74F6K21/sX3/CCwFbje9/c5rXW2C3W53F6r1ZIf2YUQ\nl+jQIWjRAvbtg2bNYO5nbiJeHg6TJkG5crB6Ndx2m79jiqyZshuRqzbyiyzsJWANcApjz709sDC7\nmU+fTs3DKg2xsRHExSXlef7CJnkLViDlLSpZQ0Nh6VLo08fOqlVWHqgJn80aw40xpQh/5UU8NWqS\nOOMzotu1KBJ5c6uobN/cyGvW2NiIbMflpmnlKFAm0/OywLEzT7TWM7XWx7XWLmAVIF/nQhRhkZEw\ne3Ya3btn8NNPFho3CWNrtWEkTpn+3yX9zJrl75jiEuSmkK8DOgAope4Gjmqtk3zPo5RSa5VSNt+0\ntYF9BZJUCJFvrFZ44410XnnFQVycidatQ5nh6GJcBWoPhW7dCB0/Rk5PDBAXLeRa66+B3UqprzHO\nWBmklOqplGqrtU7A2AvfrpTaBsSRQ7OKEKLoMJmMM1pmz07Dbjc63HpqZQNOLP8SKlQg7K3/I6J/\nL0hL83dUcREXPdiZ3+LikvK8wkBqBwPJW9ACKW9Rz/rHHyZ69rRz4ICFmjVdLP04noierbFt/xrn\n3VVJ/GQOntJlLr4gPynq2zezy2gjz/Zgp1zZKYSgQgUvq1al0qyZk61brVRtfBXbXl6Bo3NXgvbs\nJrpJPSz7fvR3TJENKeRCCADCw2H6dAdPP53OoUPQvF0MM+t+RPKLo7D8fYSYFo2wrV7p75giC1LI\nhRD/MZthxIgMli41HvcfEMrQo89xYuos8HqI6vEQoa+/JgdBixgp5EKIC7RuDevWpVK5spvp0200\nmNyFnz76Evd15Qmb8DqR3TrLZf1FiBRyIUSWKlb0sGZNKg895OSHHyzUeOwB5j+1lYw69Qj+Yi3R\njepg2f+zv2MKpJALIXIQGgrvvONg0qQ0nE7o+vg1DK34OUmDhmP98w9imtYneNlif8cs9qSQCyEu\nqksXF2vWpFKxopsPP7JT++v/45exs/CaTET27Un4s0+Cw+HvmMWWFHIhRK7cfLOHtWtT6djRyXff\nWbhrTFc+eWwLrspVsE//iOhmDbD8/qu/YxZLUsiFELkWHg6TJzuYOjWNoCDo9cadtCv3Dac79CRo\n316iG9QmeNF8f8csdqSQCyEuWZs2Lr76KoVatVysWB9JpU3T2PrY/wCIHNiHiCEDMSUm+Dll8SGF\nXAiRJ2XLelmwII3Rox0kJZmo9X5PnnjwWxxV7iRk7mfE1Lof25dr/R2zWJBCLoTIM7MZ+vd3sm5d\nKnfc4WbS6irc8O92drUeiflEHFFdOxIxuD+m06f8HfWKJoVcCHHZbr7Zw+rVqbz8soP4FBv3LnuV\nx+7bTmqVuwiZP8fYO5fL+wuMFHIhRL6wWmHQICebNqVQvbqLD7fdTdlD37K+wauY408T1eMhIvo/\niunkSX9HveJIIRdC5KsKFbwsXpzGm286MNusNPhyJM3L7uZkpfsIWbKIErXuxbZ8ib9jXlGkkAsh\n8p3ZDN27O/nmm2R69MhgzaFbKPXL10yv8jokJRPVpweRvbphOn7c31GvCFLIhRAFpkQJ45ZyX3yR\nyt33QO+fn+YO7/f8fnV1gj9fRoma92D/8D3IyPB31IAmhVwIUeBuv93D55+n8u67aZwsWZGKx7bw\ntP0d0tO8hL/0PCVq3ottxTIo5DuWXSmkkAshCoXZDJ07u9i+PYWRL2Uw1fY416T/zsehj8Phv4jq\n3Y3olo0J2vKVFPRLJIVcCFGo7HYYPNjJzp3JdB0czlDe4WbPT6wMak3Qju1Et29JVJtmBG3b4u+o\nAUMKuRDCL6Kj4aWXMti9O4VWT5bnkbAl3MsO1pibYftmG9FtmxPVqolxdajsoedICrkQwq9KlvTy\nzDMZ7NmTTJsxt9G37AruZzuraIpt+9dEde1IVJ0aBC+cBy6Xv+MWSVLIhRBFQng49Ovn5NtvU+j5\n/u2MrracO/mO2TyEZf/PRD7Wl4jbbsX+xjjMx476O26RIoVcCFGkBAVBhw4uli9P4/1tFdn22Azu\nL/EL7zKYjJNJhL8xjug7b8HTuitBy5dBaqq/I/udFHIhRJFVsaKHUaPSWbGvFFfNfp3HWh9ikG0K\nP3hvp/Q3nxPdpxsRN93I7/d24fiUFXiSUvwd2S+s/g4ghBAXY7VCgwZuGjSwkpLyEBs3PMLChT9R\natNimqctpOKuebBrHs6RVn6JrMpxVRNT7epENr2PslWisFj8/QoKlslbyEeD4+KS8rzC2NgI4uKS\n8jNOgZK8BSuQ8gZSVgicvB4P7P/ZxIkvf8WyZB7X/7GJ29J3YcVtjMfEXtMd7I2uxZGy95B6/c14\nK1UkqkwIV13lpUQJLyEhXoKCjC8L468302MICjr7PPMXgsl04ePMw7KT120bGxuR7dJlj1wIEbDM\nZrjlVi+xde8ibthNwAv8fjiFv+bvxLz1a0r/so2bT+7gztPfw2ngJ2Al/E1Z/uQGDnI9P3MdccQS\nRyynKEEGNlxYc/XPSVCWwz2YgbN112TyYrHAqFHpvPBC/m8HKeRCiCtKievCKDGiDoyoA0Ciw0HQ\n93tw7dqL8weN5bdfiTx2kOqnv6Gmd1uB5XBjxmsy48GMFxNuj5WffxsNPJHv65JCLoS4soWE4KxW\nHapV/6/gOQCH04n56N9Yjh3FdPIk5pMnMMXHY3K7wOkEtwuTy22cu+52YXK5wOXO9Ng33Gn8xeUb\n7naD04nJ4wGvF5PHg8nrwWwyUbHu1QXyEqWQCyGKp6AgPOWvx1P+en8nuWxy+qEQQgQ4KeRCCBHg\nctW0opR6G6gGeIGhWuudmcY1AMYCbmCV1np0QQQVQgiRtYvukSulagMVtdYPAL2BSedNMgloD9QA\nGimlquR7SiGEENnKTdNKfWApgNZ6PxCjlIoEUEpVAE5prf/SWnuAVb7phRBCFJLcFPIyQFym53G+\nYVmNOw4UzPk1QgghspSX0w9zugj1oheoxsSEYrXmveOD2NiIPM/rD5K3YAVS3kDKCpK3IOV31twU\n8qOc3QMHKAscy2ZcOd+wbJ0+nfcuJwOl/4czJG/BCqS8gZQVJG9Buoy+VrIdd9FOs5RS1YFXtNYN\nlVJ3A5O01jUzjf8JaA4cAb4BHtZa/3LJKYUQQuRJrno/VEqNBx4EPMAg4C4gQWu9RCn1IPC6b9JF\nWus3CyqsEEKICxV6N7ZCCCHyl1zZKYQQAU4KuRBCBDgp5EIIEeCkkAshRICTQi6EEAEuYG4skVMP\njP6klPo/oBbGthwHtAKqAid9k7yhtV6plHoYGIZxCudUrfU0P2StAyzAuHMhwI/A/wGfAhaMC726\naa3Ti0je3kC3TIPuAXYBYUCKb9iTWuvdSqmngI4Y749XtNarCjHnrcAy4G2t9XtKqWvJ5TZVSgUB\nM4DyGD2IPqq1/qOQs/4PCAKcwCNa63+UUk4g833Q6mPs+BVa1mzyziCXn6/C3rbZ5F0AxPpGlwC2\nY/QW+yOw2zc8TmvdUSkVBcwGooBkoKvW+lRu1hsQhTxzD4xKqZuB6cADfo6FUqoucKsvV0ngO2AD\n8JzW+vNM04UBLwH3ARnATqXUktz+J+Wzr7TWHTJl+x8wWWu9QCk1FuillJpZFPL6vjym+XLWBjoB\nt2B8IPdleg03AF0w3hNRwBal1FqttbugM/r+b98F1mca/Cq53KZASyBea/2wUqoRxs5A50LMOgaj\n8M1XSg0ChgNPY1wnUue8+R8prKw55IVcfr4oxG2bXV6tdcdM46cDH58dde72xfgi2qS1fkMp1Q94\nxvfvogKlaSXbHhj9bDPGXiBAPMaeYlYdydwP7NRaJ2it0zD2dGoUTsSLqgMs9z1eATSgaOZ9Cciu\nr/u6wGqtdYbWOg44BBRWd8rpQDPO7ZqiDrnfpvWBJb5pv6Rgt3NWWR8DFvkexwElc5i/MLNC1nmz\nUhS2LeSQVymlgGit9Y4c5s+c98z7JlcCYo8coz+X3Zmen+mBMdE/cQy+Pb4zP/F7Y3Tj6wYGK6WG\nY/QGOZii1UtkFaXUcoyfea8AYVrr9PNyFaW8KKXuBf7y/eQHeFUpdRWwH2MvJru8PxZ0Nq21C3D5\ncp1xKdv0v+Faa49SyquUsmmtMwojq9Y6BUApZcG4avtV36gQpdRsjGaJRVrrtwoza3Z5fXL7+Soq\neQGGYuytn1FGKbUQo++qyVrrzzj3dVzSZy5Q9sjPd9FeFguTUqo1RiEf/P/tnU9oVVcQxn8BCUoV\nstJiN27kcyMUNDvBP4u2oiLUUJAsqlhcCO4MBGqpUruQoiAudCFEVAoBpaAbEauLdtFCQQNV+BRX\nEimiYKhK0Y2LM9c805i8h+TdXJjf6t7z7iUf886Zc97MMKHERodtbwJuA4emeaUu/fcpzns78DUl\nbNG6mb9PV932/oYS6wQ4AQzZbm0ZMZW69bbSqU27rj2c+Hnghu0qLHAA2At8BgxKWjvNq3XY+UPW\nVy3zQlIvsM72zRh6CoV1RFoAAAI8SURBVHwH7KTk1H6QNNVpd6S1KSfymTow1oqkz4FvgS9sT/Bu\nPO8ycAq4yP+7RP7RNZGB7XFgNG4fSPoH6Je0KH6SVt0rp+tq2XW9LWwA9gPY/qVl/Aol5nkTaD0G\nzdqFc4553oFNq/GxSM71zNWJcQZGgPu2D1cDtk9X15J+BVbPB60tGw3Mvr5q1xusB96GVGz/S7E5\nwBNJfwGrmNQ7QYdzuCkn8mvAAEB0YHwUxqiVyDL/BGytEoGSLsV/ToLigP4G/qQ4zD5Jiymxut9q\n0Dso6UBcfwwso0yoHfHIDuDqfNEbOpcDz22/ktQj6bqkvvh4A8W+N4Atknrj+U+Au3XoDa7Tvk2v\nMZln2UbZlLpGVHu8sv19y5gk/Rz2XhBa79StNbR1sr5q1xv0A2PVjaSNko7H9UfAp8A93tVbzZu2\naEzTrKkdGG2PzfLKnBOZ5UOUL6FihBJieUkpIdpt+7GkAWCIUh53MmJiXUXSEkp5Ux/QSwmz3ALO\nAQspScLdtl/PB72heQ1wxPbmuP+Kksl/AYwDe2y/lLQfGAy9B6ec3OZa3zFgBaV8bzx0nKUNm0ZY\n4wywkpIs22X7YRe1LgX+YzLfdNf2PklHgU2U9XbZ9o/d1DqD3pPAMG2sr3mi90vKOvvd9mg8tyB0\niVIcccr2SGxCFygJ52eUUtCJdv52Yxx5kiRJMj1NCa0kSZIk7yEdeZIkScNJR54kSdJw0pEnSZI0\nnHTkSZIkDScdeZIkScNJR54kSdJw3gBHEGJhW3k1SwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff9b4157590>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3T8QnghuRWHA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#y_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XrKmUPC5RWHD",
        "colab_type": "code",
        "outputId": "4f10b885-a16b-46cc-92d9-80c1212dbb10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = model.predict([X_test_main, X_test_aux]) \n",
        "\n",
        "print(\"Number of predictions:\", len(y_pred))\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE: \", mse)\n",
        "rmse = math.sqrt(mse)\n",
        "print(\"RMSE: \", rmse) \n",
        "y_pred = np.array([x[0] for x in y_pred]) \n",
        "\n",
        "results = pd.DataFrame({\"y\":y_test[\"precio\"], 'pred': y_pred}, columns = [ \"y\", 'pred']) \n",
        "#result[\"y\"] = y_test[\"precio\"]\n",
        "results['error'] = abs(results['pred'].astype(float) - results['y'].astype(float))\n",
        "print(\"MAPE: \", 100 * np.mean(results['error'] / y_test[\"precio\"]))\n",
        "results.sort_values(by = 'error', ascending = True)\n",
        "\n",
        "results.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Number of predictions:', 177)\n",
            "('MSE: ', 909072641186630.9)\n",
            "('RMSE: ', 30150831.517333496)\n",
            "('MAPE: ', 18.150972997740162)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>pred</th>\n",
              "      <th>error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>812</th>\n",
              "      <td>185000000.0</td>\n",
              "      <td>156279584.0</td>\n",
              "      <td>28720416.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>643</th>\n",
              "      <td>170000000.0</td>\n",
              "      <td>173483824.0</td>\n",
              "      <td>3483824.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>145000000.0</td>\n",
              "      <td>141957424.0</td>\n",
              "      <td>3042576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>120000000.0</td>\n",
              "      <td>135846992.0</td>\n",
              "      <td>15846992.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>195000000.0</td>\n",
              "      <td>163726736.0</td>\n",
              "      <td>31273264.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               y         pred       error\n",
              "812  185000000.0  156279584.0  28720416.0\n",
              "643  170000000.0  173483824.0   3483824.0\n",
              "440  145000000.0  141957424.0   3042576.0\n",
              "254  120000000.0  135846992.0  15846992.0\n",
              "883  195000000.0  163726736.0  31273264.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "8untzcXqYgfM",
        "colab_type": "code",
        "outputId": "48eb115d-8f3b-41f8-f6c7-f43f02002bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict([X_train_main, X_train_aux]) \n",
        "\n",
        "print(\"Number of predictions:\", len(y_pred))\n",
        "\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "print(\"MSE: \", mse)\n",
        "rmse = math.sqrt(mse)\n",
        "print(\"RMSE: \", rmse) \n",
        "y_pred = np.array([x[0] for x in y_pred]) \n",
        "\n",
        "results = pd.DataFrame({\"y\":y_train[\"precio\"], 'pred': y_pred}, columns = [ \"y\", 'pred']) \n",
        "#result[\"y\"] = y_test[\"precio\"]\n",
        "results['error'] = abs(results['pred'].astype(float) - results['y'].astype(float))\n",
        "print(\"MAPE: \", 100 * np.mean(results['error'] / y_train[\"precio\"]))\n",
        "results.sort_values(by = 'error', ascending = True)\n",
        "\n",
        "results.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Number of predictions:', 707)\n",
            "('MSE: ', 1028935290068909.1)\n",
            "('RMSE: ', 32077021.215644527)\n",
            "('MAPE: ', 18.45192301745015)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>pred</th>\n",
              "      <th>error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>130000000.0</td>\n",
              "      <td>135528816.0</td>\n",
              "      <td>5528816.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>115000000.0</td>\n",
              "      <td>136885744.0</td>\n",
              "      <td>21885744.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>91000000.0</td>\n",
              "      <td>129564544.0</td>\n",
              "      <td>38564544.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>160000000.0</td>\n",
              "      <td>146446656.0</td>\n",
              "      <td>13553344.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>94161000.0</td>\n",
              "      <td>122769808.0</td>\n",
              "      <td>28608808.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               y         pred       error\n",
              "309  130000000.0  135528816.0   5528816.0\n",
              "228  115000000.0  136885744.0  21885744.0\n",
              "110   91000000.0  129564544.0  38564544.0\n",
              "564  160000000.0  146446656.0  13553344.0\n",
              "121   94161000.0  122769808.0  28608808.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "NbFrxe6dRWHJ",
        "colab_type": "code",
        "outputId": "fbbc9f26-2543-4b2c-ef21-c9222a3733c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "LSTM_SIZE = 40\n",
        "DENSE1_SIZE = 256\n",
        "DENSE2_SIZE = 256\n",
        "LEARNING_RATE = 0.01\n",
        "RHO = 0.05\n",
        "EPSILON = None\n",
        "DECAY = 0.0\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(DENSE1_SIZE, input_dim=X_train_aux.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "# Compile model\n",
        "optimizer = RMSprop(lr = LEARNING_RATE, rho = RHO, epsilon = EPSILON, decay = DECAY)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               2816      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 3,073\n",
            "Trainable params: 3,073\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c9MR9oUnxOI6",
        "colab_type": "code",
        "outputId": "a7a637b2-2564-47ea-fbf0-6c2111d4fb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34034
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "#df_results = pd.read_csv(DF_RESULTS_FILENAME, index_col = 0)\n",
        "    \n",
        "PATIENCE = 10\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = int(math.ceil(y_train.shape[0] / 4))\n",
        "\n",
        "\n",
        "history = model.fit(X_train_aux, \n",
        "                    y_train,\n",
        "                    epochs = EPOCHS, \n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    validation_data = (X_test_aux, y_test),\n",
        "                    verbose = 1,\n",
        "                    callbacks = [EarlyStopping(monitor = 'mean_squared_error', \n",
        "                                               restore_best_weights = True,\n",
        "                                               patience = PATIENCE)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 707 samples, validate on 177 samples\n",
            "Epoch 1/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 983761560485725.7500 - val_loss: 881085089113961.6250\n",
            "Epoch 2/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 983709796628257.0000 - val_loss: 881256976425284.0000\n",
            "Epoch 3/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 983587700260840.7500 - val_loss: 881239478236269.8750\n",
            "Epoch 4/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 983484705240308.8750 - val_loss: 881300979265466.6250\n",
            "Epoch 5/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 983531906454390.5000 - val_loss: 881113864512737.6250\n",
            "Epoch 6/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983490697933721.2500 - val_loss: 880978921928437.8750\n",
            "Epoch 7/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 983462421870160.3750 - val_loss: 880820424008466.6250\n",
            "Epoch 8/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983620131971987.2500 - val_loss: 880509719574169.3750\n",
            "Epoch 9/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 983547903945124.0000 - val_loss: 880608617776122.3750\n",
            "Epoch 10/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983547656914263.2500 - val_loss: 880318434868692.6250\n",
            "Epoch 11/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983579592866982.6250 - val_loss: 879956050963392.3750\n",
            "Epoch 12/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983794130543646.5000 - val_loss: 879456149467344.3750\n",
            "Epoch 13/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 983762189524540.0000 - val_loss: 878810053191992.3750\n",
            "Epoch 14/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 984006227965937.6250 - val_loss: 878597243306539.3750\n",
            "Epoch 15/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 984209791480520.6250 - val_loss: 878797555356828.3750\n",
            "Epoch 16/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 983903768772190.8750 - val_loss: 878817584621695.3750\n",
            "Epoch 17/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 983927009511508.0000 - val_loss: 878752404183745.6250\n",
            "Epoch 18/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 984065010667827.0000 - val_loss: 878556419357794.3750\n",
            "Epoch 19/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 984007945079586.3750 - val_loss: 878379768821864.1250\n",
            "Epoch 20/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 983937758320220.0000 - val_loss: 878404642063539.3750\n",
            "Epoch 21/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 983869384686689.0000 - val_loss: 878429333634979.3750\n",
            "Epoch 22/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 983844350659941.6250 - val_loss: 878324320786489.8750\n",
            "Epoch 23/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 983921820487083.2500 - val_loss: 878347742443531.6250\n",
            "Epoch 24/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 983706465484730.3750 - val_loss: 878718850859563.3750\n",
            "Epoch 25/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983593174426747.1250 - val_loss: 878683869884346.6250\n",
            "Epoch 26/1000\n",
            "707/707 [==============================] - 0s 13us/step - loss: 983695021809768.2500 - val_loss: 878201413840456.3750\n",
            "Epoch 27/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 983890534847257.7500 - val_loss: 878197765702372.5000\n",
            "Epoch 28/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 983816634983871.5000 - val_loss: 877706831666783.5000\n",
            "Epoch 29/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 984179245058595.5000 - val_loss: 878098347154946.8750\n",
            "Epoch 30/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 984096223420497.1250 - val_loss: 877727008407644.6250\n",
            "Epoch 31/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 984038511316186.6250 - val_loss: 877731546532013.5000\n",
            "Epoch 32/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 983959385219500.7500 - val_loss: 877910829953417.3750\n",
            "Epoch 33/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 983834496762643.8750 - val_loss: 877956099408242.3750\n",
            "Epoch 34/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983811254267295.7500 - val_loss: 877585616571467.3750\n",
            "Epoch 35/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 984168728140908.7500 - val_loss: 877851797784564.3750\n",
            "Epoch 36/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 983882197401461.0000 - val_loss: 878389886976000.0000\n",
            "Epoch 37/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 983625009656845.0000 - val_loss: 878235030114743.6250\n",
            "Epoch 38/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983608510012408.7500 - val_loss: 878845241755636.3750\n",
            "Epoch 39/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 983273442312470.1250 - val_loss: 878833671584403.5000\n",
            "Epoch 40/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 983279560562105.7500 - val_loss: 879315487377859.3750\n",
            "Epoch 41/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 983019104416479.7500 - val_loss: 879469179857671.3750\n",
            "Epoch 42/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 982884158220337.2500 - val_loss: 879973909803308.8750\n",
            "Epoch 43/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 982795365884379.0000 - val_loss: 879668547866693.3750\n",
            "Epoch 44/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 982832175286124.3750 - val_loss: 879594296736866.3750\n",
            "Epoch 45/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 982851822521377.3750 - val_loss: 880103686915621.6250\n",
            "Epoch 46/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 982664659174442.0000 - val_loss: 880571905794464.5000\n",
            "Epoch 47/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 982472699964206.0000 - val_loss: 880324197110755.1250\n",
            "Epoch 48/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 982496621402873.8750 - val_loss: 880083849412793.1250\n",
            "Epoch 49/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 982547417568558.7500 - val_loss: 880604606353847.6250\n",
            "Epoch 50/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 982406038176230.6250 - val_loss: 880221148529473.1250\n",
            "Epoch 51/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 982369264749392.7500 - val_loss: 880074332945784.0000\n",
            "Epoch 52/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 982390838445677.3750 - val_loss: 879908908597803.3750\n",
            "Epoch 53/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 982372129215790.7500 - val_loss: 879479697628860.0000\n",
            "Epoch 54/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 982416486086641.6250 - val_loss: 879151658355324.3750\n",
            "Epoch 55/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 982410219428648.2500 - val_loss: 879403954781189.6250\n",
            "Epoch 56/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 982557920248155.6250 - val_loss: 879248246215558.5000\n",
            "Epoch 57/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 982338536910471.3750 - val_loss: 878611618667387.0000\n",
            "Epoch 58/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 982592956865320.2500 - val_loss: 878664378713730.1250\n",
            "Epoch 59/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 982726987267516.6250 - val_loss: 878774110512631.3750\n",
            "Epoch 60/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 982519926379706.7500 - val_loss: 878831771964572.3750\n",
            "Epoch 61/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 982490070244113.0000 - val_loss: 878622265899204.6250\n",
            "Epoch 62/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 982405564261406.5000 - val_loss: 879252278194662.0000\n",
            "Epoch 63/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 982242333506309.3750 - val_loss: 879899606834135.5000\n",
            "Epoch 64/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 982128409565227.5000 - val_loss: 879662739155534.1250\n",
            "Epoch 65/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 982134838062843.3750 - val_loss: 879987589970116.6250\n",
            "Epoch 66/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 982073559079697.0000 - val_loss: 879731927742660.6250\n",
            "Epoch 67/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 982131652527516.8750 - val_loss: 880064793007260.3750\n",
            "Epoch 68/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 981925113631815.0000 - val_loss: 880481996219987.8750\n",
            "Epoch 69/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 981816672755445.5000 - val_loss: 881031282600618.6250\n",
            "Epoch 70/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 981656595049099.7500 - val_loss: 880787211373735.6250\n",
            "Epoch 71/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 981681906481299.6250 - val_loss: 880476751462029.6250\n",
            "Epoch 72/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 981678182461411.0000 - val_loss: 880917802305431.8750\n",
            "Epoch 73/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 981585073706101.3750 - val_loss: 880775327029583.5000\n",
            "Epoch 74/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 981553810230235.7500 - val_loss: 881126974082400.8750\n",
            "Epoch 75/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 981592825491797.8750 - val_loss: 881474057080808.8750\n",
            "Epoch 76/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 981541928781467.7500 - val_loss: 881414518017284.3750\n",
            "Epoch 77/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 981576292601421.5000 - val_loss: 881492096432787.5000\n",
            "Epoch 78/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 981384770239515.6250 - val_loss: 881773463742591.3750\n",
            "Epoch 79/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 981258456668504.7500 - val_loss: 881642338733223.6250\n",
            "Epoch 80/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 981400856647121.0000 - val_loss: 881319427707227.1250\n",
            "Epoch 81/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 981354559978894.2500 - val_loss: 881654474418673.5000\n",
            "Epoch 82/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 981125825538329.0000 - val_loss: 881414899112456.6250\n",
            "Epoch 83/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 981164720820253.0000 - val_loss: 881037743185410.8750\n",
            "Epoch 84/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 981138184864856.3750 - val_loss: 881331703013237.1250\n",
            "Epoch 85/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 981037819714460.1250 - val_loss: 881690741756714.0000\n",
            "Epoch 86/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 980994620495243.3750 - val_loss: 881651059182944.8750\n",
            "Epoch 87/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 980870628373609.6250 - val_loss: 881552040228875.6250\n",
            "Epoch 88/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 980945502074888.6250 - val_loss: 881685255740375.5000\n",
            "Epoch 89/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980997048279422.3750 - val_loss: 881202808751561.0000\n",
            "Epoch 90/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 981002576455150.0000 - val_loss: 881446375414928.6250\n",
            "Epoch 91/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 980839265088733.6250 - val_loss: 881284025841305.3750\n",
            "Epoch 92/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980767442467751.6250 - val_loss: 881835322978975.1250\n",
            "Epoch 93/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 980616873788434.7500 - val_loss: 881859986697985.5000\n",
            "Epoch 94/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 980628107645860.7500 - val_loss: 881432932116699.8750\n",
            "Epoch 95/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 980729846458996.6250 - val_loss: 881261727197311.3750\n",
            "Epoch 96/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 980610388694411.3750 - val_loss: 880870187873297.3750\n",
            "Epoch 97/1000\n",
            "707/707 [==============================] - 0s 13us/step - loss: 980638741904689.6250 - val_loss: 880787236030082.1250\n",
            "Epoch 98/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 980726120777997.3750 - val_loss: 880410785691011.6250\n",
            "Epoch 99/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 980707022278731.3750 - val_loss: 880689517923403.3750\n",
            "Epoch 100/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 980576399573524.8750 - val_loss: 880405304716131.6250\n",
            "Epoch 101/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 980626818548476.7500 - val_loss: 880691012022758.0000\n",
            "Epoch 102/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 980498014427040.5000 - val_loss: 881166189755490.3750\n",
            "Epoch 103/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 980378373180935.8750 - val_loss: 881018257085873.8750\n",
            "Epoch 104/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980311334558179.7500 - val_loss: 880984003173705.6250\n",
            "Epoch 105/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 980480027732758.8750 - val_loss: 880494866986834.5000\n",
            "Epoch 106/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 980428284520520.5000 - val_loss: 880381320249251.3750\n",
            "Epoch 107/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980447936406882.7500 - val_loss: 880845496041194.3750\n",
            "Epoch 108/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 980302729815974.2500 - val_loss: 880501372156841.3750\n",
            "Epoch 109/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980277234783604.2500 - val_loss: 880715428236363.3750\n",
            "Epoch 110/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980162200485076.8750 - val_loss: 880507173516120.3750\n",
            "Epoch 111/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 980163373347737.2500 - val_loss: 880368379521058.6250\n",
            "Epoch 112/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980170608267038.1250 - val_loss: 880013798200106.0000\n",
            "Epoch 113/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980103059778280.3750 - val_loss: 880516695978377.3750\n",
            "Epoch 114/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980138403938188.1250 - val_loss: 879890875649839.6250\n",
            "Epoch 115/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 980091392138163.2500 - val_loss: 879865130647552.0000\n",
            "Epoch 116/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 980018185574837.3750 - val_loss: 880406081106683.6250\n",
            "Epoch 117/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 979836313814034.7500 - val_loss: 880279796709202.5000\n",
            "Epoch 118/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 979813238616243.5000 - val_loss: 879780714556815.1250\n",
            "Epoch 119/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 979923115926048.6250 - val_loss: 879415757739256.6250\n",
            "Epoch 120/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 980068049833804.5000 - val_loss: 879217254091573.5000\n",
            "Epoch 121/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 980189744687083.7500 - val_loss: 878698452638124.1250\n",
            "Epoch 122/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980107118061061.0000 - val_loss: 878828118094865.3750\n",
            "Epoch 123/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 980212665733820.8750 - val_loss: 878406033488346.3750\n",
            "Epoch 124/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 980152435575841.3750 - val_loss: 878414170423296.0000\n",
            "Epoch 125/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 980098501691082.1250 - val_loss: 878340585619085.6250\n",
            "Epoch 126/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 980063331710474.8750 - val_loss: 878179666759286.6250\n",
            "Epoch 127/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 980268846958699.2500 - val_loss: 877940745435703.0000\n",
            "Epoch 128/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 980228197781393.8750 - val_loss: 878417380612061.3750\n",
            "Epoch 129/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 980061046117354.1250 - val_loss: 878479766002555.0000\n",
            "Epoch 130/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 979995055702780.7500 - val_loss: 878112082289056.5000\n",
            "Epoch 131/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 980416169614537.3750 - val_loss: 877937768409955.6250\n",
            "Epoch 132/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 980235935329000.3750 - val_loss: 877665723192568.6250\n",
            "Epoch 133/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 980261172628212.1250 - val_loss: 877451092567450.6250\n",
            "Epoch 134/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 980551061468636.5000 - val_loss: 876997417482159.0000\n",
            "Epoch 135/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 980363233724963.5000 - val_loss: 876981932076246.0000\n",
            "Epoch 136/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 980230252432695.3750 - val_loss: 876729631233920.6250\n",
            "Epoch 137/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 980184466911898.3750 - val_loss: 877411159428448.8750\n",
            "Epoch 138/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 979995660726683.3750 - val_loss: 877563435501278.6250\n",
            "Epoch 139/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 979878213665161.8750 - val_loss: 878177264273211.3750\n",
            "Epoch 140/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 979666831659431.0000 - val_loss: 878468821653469.3750\n",
            "Epoch 141/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 979492619279121.0000 - val_loss: 879064570247087.0000\n",
            "Epoch 142/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 979327328011375.6250 - val_loss: 879492825752986.6250\n",
            "Epoch 143/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 979326628731317.3750 - val_loss: 879754996565726.6250\n",
            "Epoch 144/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 979225867382338.0000 - val_loss: 879464860476919.3750\n",
            "Epoch 145/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 979404275110774.5000 - val_loss: 879074095825451.3750\n",
            "Epoch 146/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 979335640303213.3750 - val_loss: 879127570363779.6250\n",
            "Epoch 147/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 979356893974696.0000 - val_loss: 879161609578530.6250\n",
            "Epoch 148/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 979327764741054.7500 - val_loss: 879378958995340.3750\n",
            "Epoch 149/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 979170002717659.7500 - val_loss: 880121509994357.1250\n",
            "Epoch 150/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 978978873920290.3750 - val_loss: 880198767788494.8750\n",
            "Epoch 151/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 978964708919194.6250 - val_loss: 879920077629474.6250\n",
            "Epoch 152/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 978952017275647.6250 - val_loss: 880528427834697.6250\n",
            "Epoch 153/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 978886840093312.1250 - val_loss: 880775039533148.6250\n",
            "Epoch 154/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 978720423921539.3750 - val_loss: 880286402856485.6250\n",
            "Epoch 155/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 978818123321813.2500 - val_loss: 880053996919119.5000\n",
            "Epoch 156/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 978968599999338.8750 - val_loss: 879563188777816.3750\n",
            "Epoch 157/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 978926554065408.7500 - val_loss: 879296008870131.0000\n",
            "Epoch 158/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 978897015059538.6250 - val_loss: 878980408776669.3750\n",
            "Epoch 159/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 979170158672205.1250 - val_loss: 879161929397172.6250\n",
            "Epoch 160/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 978964469244680.2500 - val_loss: 879523968104737.3750\n",
            "Epoch 161/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 978806554731341.8750 - val_loss: 879934088382082.1250\n",
            "Epoch 162/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 978722475345540.5000 - val_loss: 880572801005857.3750\n",
            "Epoch 163/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 978691123735898.1250 - val_loss: 880047621120879.3750\n",
            "Epoch 164/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 978669153413887.6250 - val_loss: 880617605660116.6250\n",
            "Epoch 165/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 978582623225661.8750 - val_loss: 880726773538399.5000\n",
            "Epoch 166/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 978526877591415.8750 - val_loss: 881264086906521.3750\n",
            "Epoch 167/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 978389853440964.6250 - val_loss: 880999853021982.3750\n",
            "Epoch 168/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 978288918624589.1250 - val_loss: 881669282437085.3750\n",
            "Epoch 169/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 978241631602285.3750 - val_loss: 882111632152171.0000\n",
            "Epoch 170/1000\n",
            "707/707 [==============================] - 0s 31us/step - loss: 978187787797175.1250 - val_loss: 882098384619230.6250\n",
            "Epoch 171/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 978079647415694.2500 - val_loss: 882117499881275.3750\n",
            "Epoch 172/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 978077295900217.1250 - val_loss: 882525445567505.3750\n",
            "Epoch 173/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 978041705563734.1250 - val_loss: 882433647538754.5000\n",
            "Epoch 174/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 977997666392130.7500 - val_loss: 882289840686907.3750\n",
            "Epoch 175/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 977900459107706.0000 - val_loss: 882433611793865.0000\n",
            "Epoch 176/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 977947720786209.7500 - val_loss: 882556856094048.8750\n",
            "Epoch 177/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 978024755022719.1250 - val_loss: 883182830683222.6250\n",
            "Epoch 178/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 977791952380170.5000 - val_loss: 882686695255994.6250\n",
            "Epoch 179/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 977827636450745.7500 - val_loss: 883153604376032.1250\n",
            "Epoch 180/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 977810354743623.3750 - val_loss: 882871116881966.3750\n",
            "Epoch 181/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 977836161459647.5000 - val_loss: 882241495705357.0000\n",
            "Epoch 182/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 978066694835418.6250 - val_loss: 881665004401033.3750\n",
            "Epoch 183/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 977960479348779.5000 - val_loss: 881898101341097.3750\n",
            "Epoch 184/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 977901241111066.8750 - val_loss: 882115219413605.3750\n",
            "Epoch 185/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 977913986859355.6250 - val_loss: 882349346786934.6250\n",
            "Epoch 186/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 977796204888295.6250 - val_loss: 881743118850661.3750\n",
            "Epoch 187/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 977829484554822.2500 - val_loss: 881602324111869.1250\n",
            "Epoch 188/1000\n",
            "707/707 [==============================] - 0s 25us/step - loss: 977895878856547.5000 - val_loss: 881879609475257.1250\n",
            "Epoch 189/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 977838906515931.0000 - val_loss: 882095993061746.3750\n",
            "Epoch 190/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 977792190915637.5000 - val_loss: 882142086564013.5000\n",
            "Epoch 191/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 977785335038021.6250 - val_loss: 881511359881725.1250\n",
            "Epoch 192/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 977777083969970.5000 - val_loss: 881682072913815.8750\n",
            "Epoch 193/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 977717587880448.7500 - val_loss: 881953385169173.6250\n",
            "Epoch 194/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 977640744814027.1250 - val_loss: 881679571150697.6250\n",
            "Epoch 195/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 977782717626214.3750 - val_loss: 881357754846931.1250\n",
            "Epoch 196/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 977818022797485.7500 - val_loss: 881352878792287.5000\n",
            "Epoch 197/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 977707604558913.2500 - val_loss: 881142187975257.6250\n",
            "Epoch 198/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 977712383146664.7500 - val_loss: 881521692077177.5000\n",
            "Epoch 199/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 977583492932244.3750 - val_loss: 881773284957421.3750\n",
            "Epoch 200/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 977434100532729.5000 - val_loss: 881762036216201.3750\n",
            "Epoch 201/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 977487349422980.8750 - val_loss: 881270323740301.6250\n",
            "Epoch 202/1000\n",
            "707/707 [==============================] - 0s 27us/step - loss: 977517764261224.6250 - val_loss: 881541690850402.3750\n",
            "Epoch 203/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 977395597462723.6250 - val_loss: 881721450865889.6250\n",
            "Epoch 204/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 977361867356423.6250 - val_loss: 880916559836177.3750\n",
            "Epoch 205/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 977509247890097.3750 - val_loss: 880763045270684.3750\n",
            "Epoch 206/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 977550409828482.3750 - val_loss: 880881520978972.8750\n",
            "Epoch 207/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 977350508564207.7500 - val_loss: 880575773387064.3750\n",
            "Epoch 208/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 977371319336172.1250 - val_loss: 881144864871302.5000\n",
            "Epoch 209/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 977193323658059.0000 - val_loss: 881179263712076.6250\n",
            "Epoch 210/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 977241014235292.5000 - val_loss: 880945267302631.3750\n",
            "Epoch 211/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 977186897913140.5000 - val_loss: 881512311368177.5000\n",
            "Epoch 212/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 977013496676815.5000 - val_loss: 881314640460157.8750\n",
            "Epoch 213/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 977077127365280.1250 - val_loss: 881398792671116.3750\n",
            "Epoch 214/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 977028817526054.0000 - val_loss: 881710549730575.8750\n",
            "Epoch 215/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 976876788944183.3750 - val_loss: 880911035475725.0000\n",
            "Epoch 216/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 977033673260523.1250 - val_loss: 880565629910114.3750\n",
            "Epoch 217/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976952747303155.3750 - val_loss: 880613326902798.5000\n",
            "Epoch 218/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 977067860077288.3750 - val_loss: 880618593008460.6250\n",
            "Epoch 219/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976897943471099.6250 - val_loss: 880299381207526.0000\n",
            "Epoch 220/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976910666343523.8750 - val_loss: 880375026959724.5000\n",
            "Epoch 221/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976932184273956.2500 - val_loss: 880032722148866.8750\n",
            "Epoch 222/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 976882246261683.2500 - val_loss: 880216188203679.1250\n",
            "Epoch 223/1000\n",
            "707/707 [==============================] - 0s 26us/step - loss: 976839567681956.0000 - val_loss: 880308699892070.6250\n",
            "Epoch 224/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 976783431354521.6250 - val_loss: 879840465217941.0000\n",
            "Epoch 225/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 976915718112761.5000 - val_loss: 879881031729429.6250\n",
            "Epoch 226/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 976874217212915.0000 - val_loss: 880338149457086.8750\n",
            "Epoch 227/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 976757095539209.3750 - val_loss: 880524833175471.0000\n",
            "Epoch 228/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976593773767122.3750 - val_loss: 879931066397898.5000\n",
            "Epoch 229/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976696074502792.8750 - val_loss: 879746226952897.6250\n",
            "Epoch 230/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 976647498838427.3750 - val_loss: 879407263989297.1250\n",
            "Epoch 231/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976807017913313.5000 - val_loss: 879129329418147.3750\n",
            "Epoch 232/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 976851104046883.8750 - val_loss: 879240437834422.3750\n",
            "Epoch 233/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976608013245293.7500 - val_loss: 878820590637310.5000\n",
            "Epoch 234/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 976637845271055.1250 - val_loss: 878773640605441.5000\n",
            "Epoch 235/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976526217696582.0000 - val_loss: 879190241873341.5000\n",
            "Epoch 236/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 976417726227532.7500 - val_loss: 878395654819354.0000\n",
            "Epoch 237/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 976643468936906.1250 - val_loss: 878063641777493.3750\n",
            "Epoch 238/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 976761179877694.5000 - val_loss: 877757215154129.6250\n",
            "Epoch 239/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 976777705838867.1250 - val_loss: 878247862161292.3750\n",
            "Epoch 240/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976742321812307.6250 - val_loss: 877772042300167.3750\n",
            "Epoch 241/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 976764699438611.5000 - val_loss: 878075726364116.6250\n",
            "Epoch 242/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976627529015516.1250 - val_loss: 878393057943876.0000\n",
            "Epoch 243/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976552454794470.2500 - val_loss: 878183262258262.6250\n",
            "Epoch 244/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 976501774787869.3750 - val_loss: 878728723993478.5000\n",
            "Epoch 245/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 976312314742103.2500 - val_loss: 878738608347749.3750\n",
            "Epoch 246/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 976329736041832.6250 - val_loss: 878263075903083.0000\n",
            "Epoch 247/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 976403572593178.8750 - val_loss: 878265145940211.0000\n",
            "Epoch 248/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 976378969648335.1250 - val_loss: 878537157623900.6250\n",
            "Epoch 249/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976294012199526.0000 - val_loss: 878648880384266.1250\n",
            "Epoch 250/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976481952424109.7500 - val_loss: 878325663064480.5000\n",
            "Epoch 251/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976367173106043.5000 - val_loss: 878587819075711.3750\n",
            "Epoch 252/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976217533912969.2500 - val_loss: 878618563726874.0000\n",
            "Epoch 253/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976189981737632.1250 - val_loss: 878145322998373.3750\n",
            "Epoch 254/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976247123226757.2500 - val_loss: 877846484309333.3750\n",
            "Epoch 255/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 976408776615058.2500 - val_loss: 877546918800135.3750\n",
            "Epoch 256/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976614276106463.0000 - val_loss: 876971816188100.6250\n",
            "Epoch 257/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 976665796824815.7500 - val_loss: 877239955862730.5000\n",
            "Epoch 258/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976417915855160.7500 - val_loss: 876885761027934.0000\n",
            "Epoch 259/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976628051648338.2500 - val_loss: 876316444514547.0000\n",
            "Epoch 260/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 976583842663781.6250 - val_loss: 876260687824606.6250\n",
            "Epoch 261/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 976587018422286.3750 - val_loss: 876379261965745.8750\n",
            "Epoch 262/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976668795651322.6250 - val_loss: 875936931290904.6250\n",
            "Epoch 263/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976800919051210.5000 - val_loss: 876096499593702.0000\n",
            "Epoch 264/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 976631153938298.7500 - val_loss: 876697240037040.5000\n",
            "Epoch 265/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 976418494396216.1250 - val_loss: 876645355077724.6250\n",
            "Epoch 266/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976305795405473.5000 - val_loss: 876807323174067.3750\n",
            "Epoch 267/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 976645901419654.6250 - val_loss: 876200587642590.6250\n",
            "Epoch 268/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976483688474418.2500 - val_loss: 876095536609939.5000\n",
            "Epoch 269/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 976471754154876.2500 - val_loss: 875952547170477.5000\n",
            "Epoch 270/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976431497569171.2500 - val_loss: 875496770602626.1250\n",
            "Epoch 271/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976850194992300.3750 - val_loss: 875141977137568.5000\n",
            "Epoch 272/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 976742804103874.8750 - val_loss: 875063738915984.6250\n",
            "Epoch 273/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 976846901911934.3750 - val_loss: 875329766557129.0000\n",
            "Epoch 274/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 976691129614150.6250 - val_loss: 875293417166535.6250\n",
            "Epoch 275/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976617965456602.6250 - val_loss: 875602317537673.3750\n",
            "Epoch 276/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 976603561516627.2500 - val_loss: 876011185376886.6250\n",
            "Epoch 277/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 976531401713945.0000 - val_loss: 876119358144697.1250\n",
            "Epoch 278/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976365620584714.5000 - val_loss: 876129794387620.8750\n",
            "Epoch 279/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 976367774973836.1250 - val_loss: 876508914789219.6250\n",
            "Epoch 280/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 976256449840123.6250 - val_loss: 876325173951216.1250\n",
            "Epoch 281/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 976326276874147.3750 - val_loss: 876199656554496.0000\n",
            "Epoch 282/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976423115368311.8750 - val_loss: 876063859748054.0000\n",
            "Epoch 283/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976371849084626.7500 - val_loss: 876565806615997.5000\n",
            "Epoch 284/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 976287993948742.2500 - val_loss: 876912727588678.8750\n",
            "Epoch 285/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 976090107516577.5000 - val_loss: 876871566972985.8750\n",
            "Epoch 286/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 976076641036175.0000 - val_loss: 876836404626703.8750\n",
            "Epoch 287/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 975950763174447.0000 - val_loss: 877187662560076.6250\n",
            "Epoch 288/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 975787368847426.7500 - val_loss: 877363520807976.5000\n",
            "Epoch 289/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 975540505806283.1250 - val_loss: 878106040106822.8750\n",
            "Epoch 290/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 975389061935306.8750 - val_loss: 878009063823232.6250\n",
            "Epoch 291/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 975355790793836.7500 - val_loss: 877573960873162.5000\n",
            "Epoch 292/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 975506194952024.0000 - val_loss: 878135363531700.6250\n",
            "Epoch 293/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 975411262392311.3750 - val_loss: 877679986486115.6250\n",
            "Epoch 294/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 975489041850449.1250 - val_loss: 877298020963114.0000\n",
            "Epoch 295/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 975654410383562.8750 - val_loss: 877752026874133.6250\n",
            "Epoch 296/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 975402837334801.0000 - val_loss: 877693213555532.6250\n",
            "Epoch 297/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 975434997952714.8750 - val_loss: 877445346536847.1250\n",
            "Epoch 298/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 975445325385154.5000 - val_loss: 877770630727298.1250\n",
            "Epoch 299/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 975318204300871.7500 - val_loss: 878027116768192.3750\n",
            "Epoch 300/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 975156443676023.1250 - val_loss: 877713427150581.8750\n",
            "Epoch 301/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 975242096892828.1250 - val_loss: 878033168009945.0000\n",
            "Epoch 302/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 975105848243676.5000 - val_loss: 878638566464841.6250\n",
            "Epoch 303/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974913223450195.2500 - val_loss: 878896044652648.1250\n",
            "Epoch 304/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974881533875757.6250 - val_loss: 878844010143882.8750\n",
            "Epoch 305/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974840046739398.1250 - val_loss: 879121570228258.6250\n",
            "Epoch 306/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974783927604457.1250 - val_loss: 878918531827052.5000\n",
            "Epoch 307/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974703990779063.8750 - val_loss: 878987397267641.1250\n",
            "Epoch 308/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 974684365090787.0000 - val_loss: 879093538024014.1250\n",
            "Epoch 309/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974546759563527.6250 - val_loss: 878884214286463.3750\n",
            "Epoch 310/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974562909532339.5000 - val_loss: 878610403407791.0000\n",
            "Epoch 311/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974484747354343.6250 - val_loss: 878917489425506.3750\n",
            "Epoch 312/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 974473127174526.3750 - val_loss: 878678243314393.0000\n",
            "Epoch 313/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974489055325762.0000 - val_loss: 879273789840898.8750\n",
            "Epoch 314/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974448198367263.8750 - val_loss: 879605307082555.3750\n",
            "Epoch 315/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 974367771953282.3750 - val_loss: 879100305563138.8750\n",
            "Epoch 316/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974347050596522.8750 - val_loss: 879137665234492.6250\n",
            "Epoch 317/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 974425547559474.0000 - val_loss: 878680372579773.5000\n",
            "Epoch 318/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974400128012711.0000 - val_loss: 878452965100526.6250\n",
            "Epoch 319/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974495276421867.5000 - val_loss: 878525655755249.5000\n",
            "Epoch 320/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 974423563908781.1250 - val_loss: 878140651244249.0000\n",
            "Epoch 321/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 974529816699216.0000 - val_loss: 877666509815622.8750\n",
            "Epoch 322/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 974510997930957.3750 - val_loss: 877768835664415.8750\n",
            "Epoch 323/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 974549913858111.6250 - val_loss: 877492854057179.8750\n",
            "Epoch 324/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 974488902123913.8750 - val_loss: 878204061834014.3750\n",
            "Epoch 325/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974388859228953.7500 - val_loss: 878503632027740.6250\n",
            "Epoch 326/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974208490179895.3750 - val_loss: 878133749850250.8750\n",
            "Epoch 327/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974282111332670.5000 - val_loss: 877989619958500.5000\n",
            "Epoch 328/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974334491889275.8750 - val_loss: 878174347185134.6250\n",
            "Epoch 329/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974272519084005.8750 - val_loss: 878606381390159.5000\n",
            "Epoch 330/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974153919354182.0000 - val_loss: 878323919472165.6250\n",
            "Epoch 331/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974501151484926.5000 - val_loss: 878080160193680.6250\n",
            "Epoch 332/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974251147758439.8750 - val_loss: 877702475827281.0000\n",
            "Epoch 333/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974222093132390.0000 - val_loss: 876922307118351.8750\n",
            "Epoch 334/1000\n",
            "707/707 [==============================] - 0s 25us/step - loss: 974454403231946.8750 - val_loss: 876464886546246.8750\n",
            "Epoch 335/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974462155682087.5000 - val_loss: 876009704035206.5000\n",
            "Epoch 336/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974539633197797.5000 - val_loss: 875986996819794.5000\n",
            "Epoch 337/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974487797770196.5000 - val_loss: 876761848139104.8750\n",
            "Epoch 338/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974317938923192.6250 - val_loss: 876438192419764.6250\n",
            "Epoch 339/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 974400863077835.1250 - val_loss: 877068773723211.3750\n",
            "Epoch 340/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 974290465864175.3750 - val_loss: 877383608704237.3750\n",
            "Epoch 341/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 974117035297384.8750 - val_loss: 877712619831481.1250\n",
            "Epoch 342/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974117417596032.8750 - val_loss: 877489115587531.8750\n",
            "Epoch 343/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 974014178101563.6250 - val_loss: 877756463137045.6250\n",
            "Epoch 344/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974057941718666.2500 - val_loss: 878019784588664.0000\n",
            "Epoch 345/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 973926563730248.1250 - val_loss: 877857872035747.3750\n",
            "Epoch 346/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974008991829836.5000 - val_loss: 877625987224362.0000\n",
            "Epoch 347/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 974053642479943.3750 - val_loss: 877781307079315.5000\n",
            "Epoch 348/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 973974156064660.7500 - val_loss: 877991336116038.8750\n",
            "Epoch 349/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 973808103154023.1250 - val_loss: 878664392004579.1250\n",
            "Epoch 350/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 973800369296455.0000 - val_loss: 878815600908438.3750\n",
            "Epoch 351/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 973736551388972.5000 - val_loss: 878496509902570.3750\n",
            "Epoch 352/1000\n",
            "707/707 [==============================] - 0s 26us/step - loss: 973743146567161.5000 - val_loss: 878808592669285.3750\n",
            "Epoch 353/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 973698216945412.0000 - val_loss: 879027796306139.8750\n",
            "Epoch 354/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 973539929738203.7500 - val_loss: 879043669845125.0000\n",
            "Epoch 355/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 973637161547738.3750 - val_loss: 879277570977815.1250\n",
            "Epoch 356/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 973459193633900.7500 - val_loss: 878845449019716.0000\n",
            "Epoch 357/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 973470738209460.2500 - val_loss: 879477963260459.3750\n",
            "Epoch 358/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 973474220288605.3750 - val_loss: 879248249099142.5000\n",
            "Epoch 359/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 973489112274709.2500 - val_loss: 879433114982180.1250\n",
            "Epoch 360/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 973460487828817.5000 - val_loss: 879536709566464.0000\n",
            "Epoch 361/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 973379144861525.1250 - val_loss: 879717443874931.6250\n",
            "Epoch 362/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 973317456818054.2500 - val_loss: 879770687462914.8750\n",
            "Epoch 363/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 973259483326142.3750 - val_loss: 879518223140482.1250\n",
            "Epoch 364/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 973319939703641.3750 - val_loss: 879657745309372.0000\n",
            "Epoch 365/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 973226677319341.1250 - val_loss: 879247591970781.3750\n",
            "Epoch 366/1000\n",
            "707/707 [==============================] - 0s 26us/step - loss: 973278277011632.6250 - val_loss: 879472171713067.3750\n",
            "Epoch 367/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 973229917623853.6250 - val_loss: 879100834316473.1250\n",
            "Epoch 368/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 973192720518918.8750 - val_loss: 879587050103663.3750\n",
            "Epoch 369/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 973024651115060.8750 - val_loss: 878927707851943.6250\n",
            "Epoch 370/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 973030144360007.7500 - val_loss: 878391745812445.3750\n",
            "Epoch 371/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 973030801637700.5000 - val_loss: 877965463991683.6250\n",
            "Epoch 372/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 973080095234021.8750 - val_loss: 878333936681608.0000\n",
            "Epoch 373/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 972936410505088.5000 - val_loss: 877885151803773.8750\n",
            "Epoch 374/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 972944938741290.7500 - val_loss: 877710867092266.0000\n",
            "Epoch 375/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 972986290046272.1250 - val_loss: 877839497771852.6250\n",
            "Epoch 376/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 972996621892519.6250 - val_loss: 878210348722488.3750\n",
            "Epoch 377/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 972869461772140.3750 - val_loss: 878463013077084.6250\n",
            "Epoch 378/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 972928358960138.1250 - val_loss: 878388168765740.8750\n",
            "Epoch 379/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 973018703247901.7500 - val_loss: 878066753976239.0000\n",
            "Epoch 380/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 972917994484934.5000 - val_loss: 877791507108279.6250\n",
            "Epoch 381/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 973012384620880.0000 - val_loss: 877663997430489.0000\n",
            "Epoch 382/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 972987936159312.3750 - val_loss: 878115227723810.6250\n",
            "Epoch 383/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 972884203605720.5000 - val_loss: 878402135709198.5000\n",
            "Epoch 384/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 972763893169388.1250 - val_loss: 877953883395413.3750\n",
            "Epoch 385/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 972856958071380.7500 - val_loss: 877879662969017.1250\n",
            "Epoch 386/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 972762421900092.3750 - val_loss: 877764948060009.6250\n",
            "Epoch 387/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 972717041690256.0000 - val_loss: 877688677048458.8750\n",
            "Epoch 388/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 973021719729640.0000 - val_loss: 877269090346950.1250\n",
            "Epoch 389/1000\n",
            "707/707 [==============================] - 0s 28us/step - loss: 972847349899785.3750 - val_loss: 876824273714644.6250\n",
            "Epoch 390/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 972843253909796.6250 - val_loss: 877009329658006.3750\n",
            "Epoch 391/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 972785582241661.6250 - val_loss: 876574693533082.6250\n",
            "Epoch 392/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 972829716571166.5000 - val_loss: 876719736096201.0000\n",
            "Epoch 393/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 972808357490357.7500 - val_loss: 876571257777551.1250\n",
            "Epoch 394/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 972828659108225.2500 - val_loss: 876676662574270.8750\n",
            "Epoch 395/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 972694659697604.6250 - val_loss: 876881454708469.8750\n",
            "Epoch 396/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 972538294051152.0000 - val_loss: 877449936497600.3750\n",
            "Epoch 397/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 972394166232859.2500 - val_loss: 877919134420598.6250\n",
            "Epoch 398/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 972355405405964.6250 - val_loss: 878282506511030.3750\n",
            "Epoch 399/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 972241418485064.8750 - val_loss: 878014368449252.5000\n",
            "Epoch 400/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 972232208956258.1250 - val_loss: 877882551606324.1250\n",
            "Epoch 401/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 972163606374240.6250 - val_loss: 878324264980919.6250\n",
            "Epoch 402/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 972014069125890.5000 - val_loss: 878778061738053.3750\n",
            "Epoch 403/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 971961950597600.7500 - val_loss: 879335528031920.5000\n",
            "Epoch 404/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 971877566753975.8750 - val_loss: 879576296886434.0000\n",
            "Epoch 405/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 971788201762039.7500 - val_loss: 879223271527117.3750\n",
            "Epoch 406/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 971939088116873.5000 - val_loss: 878674737975180.3750\n",
            "Epoch 407/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 971852168279586.1250 - val_loss: 878873868096668.3750\n",
            "Epoch 408/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 971812802713550.7500 - val_loss: 878951288698521.3750\n",
            "Epoch 409/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 971679991376616.3750 - val_loss: 879555291255131.1250\n",
            "Epoch 410/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 971685266541485.3750 - val_loss: 879780311551143.6250\n",
            "Epoch 411/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 971538442408636.8750 - val_loss: 879816104778063.5000\n",
            "Epoch 412/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 971497122474913.8750 - val_loss: 880038406912567.0000\n",
            "Epoch 413/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 971482216855838.8750 - val_loss: 880354077884878.8750\n",
            "Epoch 414/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 971295784634822.8750 - val_loss: 880254891786610.3750\n",
            "Epoch 415/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 971270635327007.2500 - val_loss: 879920506043120.1250\n",
            "Epoch 416/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 971204181889748.2500 - val_loss: 880141008560544.5000\n",
            "Epoch 417/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 971220787584224.5000 - val_loss: 880257377027991.8750\n",
            "Epoch 418/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 971174247634500.8750 - val_loss: 880196688617796.0000\n",
            "Epoch 419/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 971084892609227.5000 - val_loss: 880692288835098.0000\n",
            "Epoch 420/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 971008253194952.6250 - val_loss: 880199816597018.0000\n",
            "Epoch 421/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 970967724566809.0000 - val_loss: 880326308890242.1250\n",
            "Epoch 422/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 970833954816579.3750 - val_loss: 880069750636821.6250\n",
            "Epoch 423/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 970898730342397.1250 - val_loss: 880308464640046.3750\n",
            "Epoch 424/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 970891812339248.5000 - val_loss: 879906061090445.6250\n",
            "Epoch 425/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 970775967347609.2500 - val_loss: 879319829471833.6250\n",
            "Epoch 426/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 970788193405614.5000 - val_loss: 879646975858468.1250\n",
            "Epoch 427/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 970749533384397.0000 - val_loss: 879204996672818.6250\n",
            "Epoch 428/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970754187056639.2500 - val_loss: 878736855073138.3750\n",
            "Epoch 429/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 970800263509074.6250 - val_loss: 879228343713606.8750\n",
            "Epoch 430/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970738296683802.5000 - val_loss: 878641044885295.6250\n",
            "Epoch 431/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970676704984740.3750 - val_loss: 878912394893092.1250\n",
            "Epoch 432/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 970726000764235.7500 - val_loss: 878229825441861.3750\n",
            "Epoch 433/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 970677807962109.1250 - val_loss: 877987478221286.0000\n",
            "Epoch 434/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 970855948536517.7500 - val_loss: 877712491096561.5000\n",
            "Epoch 435/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 970746743217098.5000 - val_loss: 877466464710864.3750\n",
            "Epoch 436/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970866678123808.2500 - val_loss: 877607486601135.0000\n",
            "Epoch 437/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970625950845151.0000 - val_loss: 877495469975864.3750\n",
            "Epoch 438/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970620060737176.8750 - val_loss: 877257907532724.6250\n",
            "Epoch 439/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 970703256602035.8750 - val_loss: 876707009970638.8750\n",
            "Epoch 440/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970757766417534.0000 - val_loss: 876696600784826.6250\n",
            "Epoch 441/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970785489117728.6250 - val_loss: 877170559355423.8750\n",
            "Epoch 442/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 970607888024548.3750 - val_loss: 877690129772011.6250\n",
            "Epoch 443/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 970417872645813.7500 - val_loss: 876958315398878.6250\n",
            "Epoch 444/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970531387241809.5000 - val_loss: 877423346664442.3750\n",
            "Epoch 445/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970451686021209.7500 - val_loss: 876767760919951.1250\n",
            "Epoch 446/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970537287791049.7500 - val_loss: 876754586000898.8750\n",
            "Epoch 447/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 970494918177874.6250 - val_loss: 876460436810318.1250\n",
            "Epoch 448/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 970504472790521.5000 - val_loss: 876230749363992.6250\n",
            "Epoch 449/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970464990187386.7500 - val_loss: 876050828150679.8750\n",
            "Epoch 450/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970459983790195.8750 - val_loss: 876301521940688.3750\n",
            "Epoch 451/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970299762658823.8750 - val_loss: 876759482954491.6250\n",
            "Epoch 452/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970389304962440.5000 - val_loss: 877049900975468.5000\n",
            "Epoch 453/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 970194601028143.0000 - val_loss: 876707381670096.3750\n",
            "Epoch 454/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970348518790979.7500 - val_loss: 876742745436275.6250\n",
            "Epoch 455/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970181003035249.7500 - val_loss: 876256613478886.0000\n",
            "Epoch 456/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970434225887090.1250 - val_loss: 876456138217437.3750\n",
            "Epoch 457/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970382218095545.0000 - val_loss: 875993538869167.0000\n",
            "Epoch 458/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 970441398146930.1250 - val_loss: 876266488015345.5000\n",
            "Epoch 459/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 970317065260343.3750 - val_loss: 876791177260060.8750\n",
            "Epoch 460/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970069814278708.8750 - val_loss: 877417618058268.8750\n",
            "Epoch 461/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970028636108901.3750 - val_loss: 877220888873689.0000\n",
            "Epoch 462/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 969984093858119.3750 - val_loss: 877512306646716.0000\n",
            "Epoch 463/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 969917724710353.0000 - val_loss: 877283550095950.1250\n",
            "Epoch 464/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970025173595264.8750 - val_loss: 876849246034903.5000\n",
            "Epoch 465/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969936163891133.2500 - val_loss: 876767913311515.5000\n",
            "Epoch 466/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970143143764211.3750 - val_loss: 876850768111095.3750\n",
            "Epoch 467/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969955563312430.7500 - val_loss: 876700207723352.3750\n",
            "Epoch 468/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 969965228673687.3750 - val_loss: 877222256301212.3750\n",
            "Epoch 469/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 969875506044661.5000 - val_loss: 877004916060923.6250\n",
            "Epoch 470/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 969927997966860.2500 - val_loss: 876740263074775.5000\n",
            "Epoch 471/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970053219879783.8750 - val_loss: 876188165886947.1250\n",
            "Epoch 472/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970114354512428.1250 - val_loss: 875877552883145.0000\n",
            "Epoch 473/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 970098506402988.3750 - val_loss: 875657685189626.3750\n",
            "Epoch 474/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 970186554941127.2500 - val_loss: 875341227451339.8750\n",
            "Epoch 475/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 970145678523906.1250 - val_loss: 874908863854233.3750\n",
            "Epoch 476/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 970113799452356.3750 - val_loss: 874394453718588.6250\n",
            "Epoch 477/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 970374263844209.3750 - val_loss: 874630804693593.6250\n",
            "Epoch 478/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 970177167246354.7500 - val_loss: 875120093086899.3750\n",
            "Epoch 479/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 969865521180666.2500 - val_loss: 875257085945335.3750\n",
            "Epoch 480/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 969846762354766.2500 - val_loss: 875419877490780.6250\n",
            "Epoch 481/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969849159099909.0000 - val_loss: 875494591794991.6250\n",
            "Epoch 482/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969924244141903.3750 - val_loss: 875035257685726.6250\n",
            "Epoch 483/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 969794338751669.0000 - val_loss: 875673555525122.8750\n",
            "Epoch 484/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 969691879652843.1250 - val_loss: 876222617744493.8750\n",
            "Epoch 485/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 969664552581708.1250 - val_loss: 876453465003256.6250\n",
            "Epoch 486/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 969469080312616.2500 - val_loss: 876106693772559.8750\n",
            "Epoch 487/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969576285726190.0000 - val_loss: 876495843562189.3750\n",
            "Epoch 488/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969567677377001.5000 - val_loss: 876480248580084.3750\n",
            "Epoch 489/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 969483142135020.1250 - val_loss: 876147945910572.8750\n",
            "Epoch 490/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 969433888310429.8750 - val_loss: 875720519423965.3750\n",
            "Epoch 491/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969442535197381.7500 - val_loss: 875261847096487.6250\n",
            "Epoch 492/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 969530849038596.6250 - val_loss: 875262609735587.3750\n",
            "Epoch 493/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 969526954992183.7500 - val_loss: 874964347523419.1250\n",
            "Epoch 494/1000\n",
            "707/707 [==============================] - 0s 25us/step - loss: 969590751530666.1250 - val_loss: 875197880577793.5000\n",
            "Epoch 495/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 969587674489585.2500 - val_loss: 874695195288709.0000\n",
            "Epoch 496/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 969528072326293.1250 - val_loss: 874619291733437.5000\n",
            "Epoch 497/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 969511050333796.6250 - val_loss: 874450985640907.8750\n",
            "Epoch 498/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 969528193824660.7500 - val_loss: 874390655697821.6250\n",
            "Epoch 499/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969438499078561.1250 - val_loss: 875131110782987.6250\n",
            "Epoch 500/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 969319343066562.5000 - val_loss: 875812009695660.1250\n",
            "Epoch 501/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 969398572057178.5000 - val_loss: 875513889820278.6250\n",
            "Epoch 502/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 969333949970510.2500 - val_loss: 875384825302739.1250\n",
            "Epoch 503/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969289833687784.3750 - val_loss: 875042632162263.5000\n",
            "Epoch 504/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 969351535482877.1250 - val_loss: 875418033916447.8750\n",
            "Epoch 505/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 969143087425423.0000 - val_loss: 875454395651505.8750\n",
            "Epoch 506/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 969178919334832.3750 - val_loss: 875348465039834.3750\n",
            "Epoch 507/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 969141680369913.1250 - val_loss: 875028009648498.3750\n",
            "Epoch 508/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 969062395281222.6250 - val_loss: 874772240795081.0000\n",
            "Epoch 509/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 969024386225478.0000 - val_loss: 875004273008408.6250\n",
            "Epoch 510/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 969039294075187.0000 - val_loss: 874710929085688.6250\n",
            "Epoch 511/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 968971377389423.2500 - val_loss: 874786509832099.3750\n",
            "Epoch 512/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 968931661518914.7500 - val_loss: 874556159453218.6250\n",
            "Epoch 513/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 968996278812092.6250 - val_loss: 874770367731769.8750\n",
            "Epoch 514/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 968847334675109.8750 - val_loss: 875341067038465.5000\n",
            "Epoch 515/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 968728201871197.7500 - val_loss: 874878865898779.5000\n",
            "Epoch 516/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 968830798984575.8750 - val_loss: 874560442341156.1250\n",
            "Epoch 517/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 968876347488635.5000 - val_loss: 874992844888121.8750\n",
            "Epoch 518/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 968767469763935.8750 - val_loss: 875685989746676.3750\n",
            "Epoch 519/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 968552019746062.7500 - val_loss: 875296759429964.6250\n",
            "Epoch 520/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 968800236544799.5000 - val_loss: 875468780839050.8750\n",
            "Epoch 521/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 968578047923711.2500 - val_loss: 875878020413266.5000\n",
            "Epoch 522/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 968405675057163.5000 - val_loss: 875031650588729.8750\n",
            "Epoch 523/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 968590762727884.6250 - val_loss: 875188003674632.6250\n",
            "Epoch 524/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 968566512128307.0000 - val_loss: 874923919341024.1250\n",
            "Epoch 525/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 968461747692875.7500 - val_loss: 874728025824123.0000\n",
            "Epoch 526/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 968440112677883.6250 - val_loss: 875149139788424.0000\n",
            "Epoch 527/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 968387363829351.5000 - val_loss: 874498142742840.3750\n",
            "Epoch 528/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 968704872001437.6250 - val_loss: 874004403544295.3750\n",
            "Epoch 529/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 968659087145084.6250 - val_loss: 874367674732983.6250\n",
            "Epoch 530/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 968464777653339.2500 - val_loss: 874323886947148.6250\n",
            "Epoch 531/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 968438175111141.8750 - val_loss: 874497839864328.6250\n",
            "Epoch 532/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 968326929592082.3750 - val_loss: 874253541762921.6250\n",
            "Epoch 533/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 968297177252164.5000 - val_loss: 874207030531968.6250\n",
            "Epoch 534/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 968325086708639.0000 - val_loss: 873961405816045.3750\n",
            "Epoch 535/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 968269128547170.1250 - val_loss: 874572142929769.6250\n",
            "Epoch 536/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 968264571219336.5000 - val_loss: 874622764553464.6250\n",
            "Epoch 537/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 968124299218185.0000 - val_loss: 874351492897844.1250\n",
            "Epoch 538/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 968257892178797.7500 - val_loss: 874715335739658.1250\n",
            "Epoch 539/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 968059476184607.2500 - val_loss: 874242618209112.3750\n",
            "Epoch 540/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 968085557897473.7500 - val_loss: 874264660569579.6250\n",
            "Epoch 541/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967956654014487.2500 - val_loss: 874985467271781.3750\n",
            "Epoch 542/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 967899350946771.1250 - val_loss: 875398588383324.6250\n",
            "Epoch 543/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 967767704309325.5000 - val_loss: 875623113624836.3750\n",
            "Epoch 544/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967742404789660.8750 - val_loss: 875349300420191.5000\n",
            "Epoch 545/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967734327426307.2500 - val_loss: 874995771645905.6250\n",
            "Epoch 546/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 967721194354511.3750 - val_loss: 874966633161398.3750\n",
            "Epoch 547/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967690634192829.2500 - val_loss: 875064797062462.3750\n",
            "Epoch 548/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 967594477157380.3750 - val_loss: 875395218766576.1250\n",
            "Epoch 549/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 967661996932656.5000 - val_loss: 875756436206609.3750\n",
            "Epoch 550/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967597023022785.5000 - val_loss: 875535140157081.3750\n",
            "Epoch 551/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967811830428198.3750 - val_loss: 874710167237464.3750\n",
            "Epoch 552/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 967691697398466.8750 - val_loss: 874327037137520.8750\n",
            "Epoch 553/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 967789989577885.8750 - val_loss: 873935638926955.0000\n",
            "Epoch 554/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967749840727061.8750 - val_loss: 874595386160052.6250\n",
            "Epoch 555/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 967777044781099.5000 - val_loss: 874412584290755.3750\n",
            "Epoch 556/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 967744869592873.6250 - val_loss: 874718533807595.6250\n",
            "Epoch 557/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967639769313529.1250 - val_loss: 874247048787794.5000\n",
            "Epoch 558/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 967598693933832.2500 - val_loss: 874126109719742.8750\n",
            "Epoch 559/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967612750986476.1250 - val_loss: 873736880325718.6250\n",
            "Epoch 560/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967671854436937.2500 - val_loss: 873713988291381.5000\n",
            "Epoch 561/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 967763559126735.8750 - val_loss: 873452159834262.3750\n",
            "Epoch 562/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967714505489690.5000 - val_loss: 873516808910767.0000\n",
            "Epoch 563/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 967793353326638.3750 - val_loss: 873189012030088.0000\n",
            "Epoch 564/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967798546005506.1250 - val_loss: 873444744095963.8750\n",
            "Epoch 565/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967763447025507.5000 - val_loss: 873919866186040.3750\n",
            "Epoch 566/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 967579511975629.0000 - val_loss: 873838349234662.0000\n",
            "Epoch 567/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967579026931364.3750 - val_loss: 873635519559321.3750\n",
            "Epoch 568/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 967628156125207.2500 - val_loss: 874066946495181.3750\n",
            "Epoch 569/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967500151804744.1250 - val_loss: 873696985730771.1250\n",
            "Epoch 570/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967417665423907.5000 - val_loss: 874529374115435.0000\n",
            "Epoch 571/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 967359892404302.2500 - val_loss: 875158511163172.1250\n",
            "Epoch 572/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967262400666570.5000 - val_loss: 875158075727918.3750\n",
            "Epoch 573/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 967252081753254.6250 - val_loss: 874873040350422.0000\n",
            "Epoch 574/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 967231731725881.1250 - val_loss: 874850618320282.6250\n",
            "Epoch 575/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967287544682562.7500 - val_loss: 874495416868817.6250\n",
            "Epoch 576/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967250010395927.6250 - val_loss: 874812527105735.6250\n",
            "Epoch 577/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967176708152637.1250 - val_loss: 875288215011420.6250\n",
            "Epoch 578/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967072103658416.3750 - val_loss: 875497033407910.3750\n",
            "Epoch 579/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 966916813462358.5000 - val_loss: 875148064372284.6250\n",
            "Epoch 580/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967070572731523.7500 - val_loss: 874971864723294.0000\n",
            "Epoch 581/1000\n",
            "707/707 [==============================] - 0s 26us/step - loss: 967012008857211.8750 - val_loss: 875564505105124.5000\n",
            "Epoch 582/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966962557597774.2500 - val_loss: 875487117933296.1250\n",
            "Epoch 583/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 967124169790535.0000 - val_loss: 874930624897162.8750\n",
            "Epoch 584/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966988650131589.2500 - val_loss: 874744379451704.3750\n",
            "Epoch 585/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966976154024477.7500 - val_loss: 874682279810759.6250\n",
            "Epoch 586/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967071619183675.3750 - val_loss: 874403494546784.8750\n",
            "Epoch 587/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 967072681250265.6250 - val_loss: 874145966624189.5000\n",
            "Epoch 588/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967092797203440.1250 - val_loss: 874464546749908.6250\n",
            "Epoch 589/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 967069742888180.8750 - val_loss: 873925659568440.3750\n",
            "Epoch 590/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 967137693508059.0000 - val_loss: 874000090933375.3750\n",
            "Epoch 591/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 967092247708088.2500 - val_loss: 874238450977034.1250\n",
            "Epoch 592/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 967036773356916.2500 - val_loss: 874658288404468.3750\n",
            "Epoch 593/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966888771828948.8750 - val_loss: 874825206360098.6250\n",
            "Epoch 594/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 967013508792528.6250 - val_loss: 874398577011185.5000\n",
            "Epoch 595/1000\n",
            "707/707 [==============================] - 0s 26us/step - loss: 966968966541746.5000 - val_loss: 874468631260003.6250\n",
            "Epoch 596/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 966898969433738.2500 - val_loss: 874279172577060.1250\n",
            "Epoch 597/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 967029791282362.7500 - val_loss: 874602143360040.5000\n",
            "Epoch 598/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 966889820321893.3750 - val_loss: 875015479026821.0000\n",
            "Epoch 599/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 966767197957906.3750 - val_loss: 875222376678671.8750\n",
            "Epoch 600/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966717374651245.7500 - val_loss: 875165134281687.5000\n",
            "Epoch 601/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966755290305120.2500 - val_loss: 875016314505667.3750\n",
            "Epoch 602/1000\n",
            "707/707 [==============================] - 0s 25us/step - loss: 966723918335009.3750 - val_loss: 875140124383012.1250\n",
            "Epoch 603/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966713067296811.5000 - val_loss: 874616281972273.1250\n",
            "Epoch 604/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966754737012944.6250 - val_loss: 874880566056647.6250\n",
            "Epoch 605/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966697662680143.6250 - val_loss: 874966752299181.5000\n",
            "Epoch 606/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 966631518684039.7500 - val_loss: 874547245019899.6250\n",
            "Epoch 607/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966644383557681.2500 - val_loss: 874480260301077.6250\n",
            "Epoch 608/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966633589756604.8750 - val_loss: 874661415521725.5000\n",
            "Epoch 609/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 966916685224628.2500 - val_loss: 873943004462456.0000\n",
            "Epoch 610/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 966805669757507.3750 - val_loss: 874393272628056.3750\n",
            "Epoch 611/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 966718255609331.7500 - val_loss: 873805907249973.5000\n",
            "Epoch 612/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966857029443062.6250 - val_loss: 873525902770546.3750\n",
            "Epoch 613/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966838279385453.1250 - val_loss: 873824962204087.6250\n",
            "Epoch 614/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 966661607042880.8750 - val_loss: 873459847312216.3750\n",
            "Epoch 615/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966816843846101.2500 - val_loss: 873766671308857.8750\n",
            "Epoch 616/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 966690389009507.8750 - val_loss: 873293642777779.3750\n",
            "Epoch 617/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 966730730359308.2500 - val_loss: 873469166621759.6250\n",
            "Epoch 618/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 966585651341685.7500 - val_loss: 873338829304756.6250\n",
            "Epoch 619/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966525410932281.1250 - val_loss: 873350780740000.5000\n",
            "Epoch 620/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966591732050224.1250 - val_loss: 873578126055574.3750\n",
            "Epoch 621/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 966558719258896.2500 - val_loss: 873435978576728.3750\n",
            "Epoch 622/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 966578459184115.0000 - val_loss: 873132087898875.6250\n",
            "Epoch 623/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 966658166977966.1250 - val_loss: 873389073445535.1250\n",
            "Epoch 624/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 966494569615783.0000 - val_loss: 873419989695864.0000\n",
            "Epoch 625/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966567259336443.3750 - val_loss: 873859774207502.5000\n",
            "Epoch 626/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 966574384883483.2500 - val_loss: 874052691819803.5000\n",
            "Epoch 627/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966385707078379.5000 - val_loss: 873730514697777.1250\n",
            "Epoch 628/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966416706532596.8750 - val_loss: 873987321810272.8750\n",
            "Epoch 629/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 966338224163687.8750 - val_loss: 873695375070641.8750\n",
            "Epoch 630/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 966304244890813.6250 - val_loss: 873308344128760.6250\n",
            "Epoch 631/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 966377960513315.8750 - val_loss: 873662007645062.5000\n",
            "Epoch 632/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 966279634257401.5000 - val_loss: 874285055156548.0000\n",
            "Epoch 633/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966162262230614.1250 - val_loss: 873923213724191.8750\n",
            "Epoch 634/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966168852994995.2500 - val_loss: 873517654735415.0000\n",
            "Epoch 635/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 966165643444756.8750 - val_loss: 873483677322963.1250\n",
            "Epoch 636/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 966249930469370.2500 - val_loss: 873828159272323.6250\n",
            "Epoch 637/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 966123004826602.1250 - val_loss: 873822716700799.3750\n",
            "Epoch 638/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966134729988603.0000 - val_loss: 874475047015210.0000\n",
            "Epoch 639/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 965910325398654.0000 - val_loss: 874251551779683.6250\n",
            "Epoch 640/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965981751393910.1250 - val_loss: 874162211578272.5000\n",
            "Epoch 641/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 965943475990962.5000 - val_loss: 874395288700546.1250\n",
            "Epoch 642/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 965852556199602.8750 - val_loss: 874073448960069.3750\n",
            "Epoch 643/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 965934537555388.6250 - val_loss: 873587892221408.1250\n",
            "Epoch 644/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966035876733519.0000 - val_loss: 873499198615945.3750\n",
            "Epoch 645/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 966189386788649.6250 - val_loss: 872853435041317.6250\n",
            "Epoch 646/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966228832372750.3750 - val_loss: 873076604745091.6250\n",
            "Epoch 647/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 966130149832365.1250 - val_loss: 872840262235708.6250\n",
            "Epoch 648/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966165432139637.0000 - val_loss: 872675630768174.3750\n",
            "Epoch 649/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 966202196513422.6250 - val_loss: 872499019170254.8750\n",
            "Epoch 650/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 966217044752995.1250 - val_loss: 873035430502353.6250\n",
            "Epoch 651/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 966001266464093.0000 - val_loss: 873367284365045.8750\n",
            "Epoch 652/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965983365756009.6250 - val_loss: 873689940567821.0000\n",
            "Epoch 653/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 965910681825505.8750 - val_loss: 873338489847530.3750\n",
            "Epoch 654/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 966005463319084.1250 - val_loss: 873620464836746.8750\n",
            "Epoch 655/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 965897035802789.1250 - val_loss: 874067262746739.6250\n",
            "Epoch 656/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965976345855597.3750 - val_loss: 873823608590960.8750\n",
            "Epoch 657/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 965955286439122.0000 - val_loss: 873836021370764.3750\n",
            "Epoch 658/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 965821513461592.0000 - val_loss: 873511958230773.8750\n",
            "Epoch 659/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965947651927826.3750 - val_loss: 873087144937738.1250\n",
            "Epoch 660/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 965937426321460.1250 - val_loss: 872923144264200.6250\n",
            "Epoch 661/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 965964655814218.6250 - val_loss: 873194729030835.3750\n",
            "Epoch 662/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 965907624243147.8750 - val_loss: 873549885552003.6250\n",
            "Epoch 663/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 965852738684455.8750 - val_loss: 873133632971168.5000\n",
            "Epoch 664/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965869893375950.7500 - val_loss: 873663215543151.3750\n",
            "Epoch 665/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965795292215443.6250 - val_loss: 873719267828591.3750\n",
            "Epoch 666/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965605066302818.7500 - val_loss: 873687911166715.6250\n",
            "Epoch 667/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 965692194206061.1250 - val_loss: 873944100890843.8750\n",
            "Epoch 668/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 965537401393945.7500 - val_loss: 873801137418448.3750\n",
            "Epoch 669/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 965570963823006.2500 - val_loss: 873278782882995.3750\n",
            "Epoch 670/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 965569653895000.0000 - val_loss: 872957848907058.6250\n",
            "Epoch 671/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 965819381544922.3750 - val_loss: 873235453914326.0000\n",
            "Epoch 672/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 965531850247433.0000 - val_loss: 873137713523926.0000\n",
            "Epoch 673/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 965560798752252.3750 - val_loss: 873522438453167.0000\n",
            "Epoch 674/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965440278731235.7500 - val_loss: 873067860635763.6250\n",
            "Epoch 675/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 965515859068919.3750 - val_loss: 872978045249478.1250\n",
            "Epoch 676/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965497346799277.1250 - val_loss: 872970315936600.3750\n",
            "Epoch 677/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 965557181518038.3750 - val_loss: 872920667505473.1250\n",
            "Epoch 678/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965503779283558.0000 - val_loss: 872969612161417.3750\n",
            "Epoch 679/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 965403972325613.6250 - val_loss: 873447085774257.8750\n",
            "Epoch 680/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 965298030720499.7500 - val_loss: 873504288915548.6250\n",
            "Epoch 681/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 965201334725885.5000 - val_loss: 874266039121931.6250\n",
            "Epoch 682/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 965069295140887.2500 - val_loss: 873788735844190.0000\n",
            "Epoch 683/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965104973516226.5000 - val_loss: 874294769721482.8750\n",
            "Epoch 684/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 965152065144475.7500 - val_loss: 874416736457699.1250\n",
            "Epoch 685/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 964951794164465.2500 - val_loss: 874106946416478.0000\n",
            "Epoch 686/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 965056590920360.7500 - val_loss: 874000800611241.3750\n",
            "Epoch 687/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 965005821332443.7500 - val_loss: 874286214420260.1250\n",
            "Epoch 688/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 965021603044924.0000 - val_loss: 874104503488396.3750\n",
            "Epoch 689/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964974697745812.0000 - val_loss: 873665368956141.3750\n",
            "Epoch 690/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 965055685282918.7500 - val_loss: 874026660584407.5000\n",
            "Epoch 691/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 965104536501785.3750 - val_loss: 874217253620718.6250\n",
            "Epoch 692/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 964951384392236.1250 - val_loss: 874025589345540.3750\n",
            "Epoch 693/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 964888603527900.8750 - val_loss: 874386902704359.3750\n",
            "Epoch 694/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964811825956693.1250 - val_loss: 874847628005908.3750\n",
            "Epoch 695/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 964689238897236.7500 - val_loss: 874511455642201.6250\n",
            "Epoch 696/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 964858313795007.5000 - val_loss: 874279447071449.0000\n",
            "Epoch 697/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 964773750076846.1250 - val_loss: 873822991966069.1250\n",
            "Epoch 698/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 964775130696969.0000 - val_loss: 873344650729900.1250\n",
            "Epoch 699/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964741922820078.7500 - val_loss: 873149352791578.0000\n",
            "Epoch 700/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 964856227535146.3750 - val_loss: 873553084430069.8750\n",
            "Epoch 701/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964684721293674.0000 - val_loss: 873409520757511.3750\n",
            "Epoch 702/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 964755026181726.8750 - val_loss: 873351635233173.0000\n",
            "Epoch 703/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 964754125242854.6250 - val_loss: 873015586623650.0000\n",
            "Epoch 704/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 964824657892886.5000 - val_loss: 872581843393773.3750\n",
            "Epoch 705/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 964921979699014.6250 - val_loss: 872254234494038.6250\n",
            "Epoch 706/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964857863396762.0000 - val_loss: 872533239676094.8750\n",
            "Epoch 707/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964801583027317.3750 - val_loss: 872723844925798.6250\n",
            "Epoch 708/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 964797546481354.1250 - val_loss: 872465538117840.3750\n",
            "Epoch 709/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964817305627994.1250 - val_loss: 872798089457594.6250\n",
            "Epoch 710/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 964732802089492.8750 - val_loss: 873256378536468.3750\n",
            "Epoch 711/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 964680754182129.6250 - val_loss: 873547531212302.5000\n",
            "Epoch 712/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 964498870698632.8750 - val_loss: 873419914950019.6250\n",
            "Epoch 713/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 964531566051745.1250 - val_loss: 873620677435530.8750\n",
            "Epoch 714/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964380140927587.1250 - val_loss: 874059682018413.8750\n",
            "Epoch 715/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 964221910945635.5000 - val_loss: 873470968039782.6250\n",
            "Epoch 716/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 964301333479354.3750 - val_loss: 873930159707107.1250\n",
            "Epoch 717/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 964277812392046.1250 - val_loss: 874177337739530.1250\n",
            "Epoch 718/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 964255193145355.5000 - val_loss: 874111477196371.8750\n",
            "Epoch 719/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964312281171936.1250 - val_loss: 873419805471576.3750\n",
            "Epoch 720/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 964276030874770.2500 - val_loss: 873037715333906.6250\n",
            "Epoch 721/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 964203534697212.7500 - val_loss: 873449905580992.3750\n",
            "Epoch 722/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 964158124017863.8750 - val_loss: 872828037981872.5000\n",
            "Epoch 723/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 964208583618832.2500 - val_loss: 872963300515145.6250\n",
            "Epoch 724/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 964133913095097.0000 - val_loss: 873108427219082.8750\n",
            "Epoch 725/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 964223347284150.5000 - val_loss: 872767313938605.5000\n",
            "Epoch 726/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 964269668133400.0000 - val_loss: 872957915675283.5000\n",
            "Epoch 727/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 964181501917447.6250 - val_loss: 873267397504769.5000\n",
            "Epoch 728/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 964156598121763.1250 - val_loss: 873304417105746.5000\n",
            "Epoch 729/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 964076509103053.3750 - val_loss: 873283618431947.8750\n",
            "Epoch 730/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 964049758484720.5000 - val_loss: 873211904808647.6250\n",
            "Epoch 731/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 964030444325851.7500 - val_loss: 873107266952706.8750\n",
            "Epoch 732/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963950771771273.2500 - val_loss: 873646593367554.8750\n",
            "Epoch 733/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 963882691792415.2500 - val_loss: 873410220196210.3750\n",
            "Epoch 734/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963872349131151.6250 - val_loss: 873517575696777.3750\n",
            "Epoch 735/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 963816050109809.3750 - val_loss: 873476988957991.0000\n",
            "Epoch 736/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963890502506006.5000 - val_loss: 873982871243481.0000\n",
            "Epoch 737/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 963732647080741.3750 - val_loss: 873846836060761.6250\n",
            "Epoch 738/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963694997228276.1250 - val_loss: 874217183648264.6250\n",
            "Epoch 739/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963630872928962.8750 - val_loss: 874141981171202.8750\n",
            "Epoch 740/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963607380460215.1250 - val_loss: 874091557552128.0000\n",
            "Epoch 741/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 963520916716409.3750 - val_loss: 874096926958817.6250\n",
            "Epoch 742/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 963564227740227.3750 - val_loss: 874315645914372.3750\n",
            "Epoch 743/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963490611422470.1250 - val_loss: 873823988838238.0000\n",
            "Epoch 744/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963603876845959.0000 - val_loss: 873211833549170.3750\n",
            "Epoch 745/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 963605334826370.7500 - val_loss: 873562250077317.0000\n",
            "Epoch 746/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 963547040763863.3750 - val_loss: 873771938815583.5000\n",
            "Epoch 747/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963480680064030.5000 - val_loss: 873737016545812.3750\n",
            "Epoch 748/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963464514041772.0000 - val_loss: 873475890544269.6250\n",
            "Epoch 749/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 963618595329071.7500 - val_loss: 873154279661429.1250\n",
            "Epoch 750/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 963652331723861.5000 - val_loss: 873180507189363.6250\n",
            "Epoch 751/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963501899611557.5000 - val_loss: 872526352027624.8750\n",
            "Epoch 752/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963556637568717.0000 - val_loss: 872544344162581.6250\n",
            "Epoch 753/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963620756718587.6250 - val_loss: 872530874497405.8750\n",
            "Epoch 754/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963464791779446.7500 - val_loss: 872938055750256.8750\n",
            "Epoch 755/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 963356883194070.3750 - val_loss: 873339722608570.6250\n",
            "Epoch 756/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963366404394666.1250 - val_loss: 873548016754826.8750\n",
            "Epoch 757/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963335047659311.3750 - val_loss: 873204990524421.6250\n",
            "Epoch 758/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 963428532300196.0000 - val_loss: 873251944457349.0000\n",
            "Epoch 759/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963366403160698.3750 - val_loss: 873026280374920.0000\n",
            "Epoch 760/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963322483494130.0000 - val_loss: 873296302396016.8750\n",
            "Epoch 761/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963189779061521.0000 - val_loss: 873155681232097.6250\n",
            "Epoch 762/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 963220137659309.3750 - val_loss: 873311744462269.5000\n",
            "Epoch 763/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 963134467570367.8750 - val_loss: 873520449153429.0000\n",
            "Epoch 764/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963176019110354.3750 - val_loss: 873995516038496.8750\n",
            "Epoch 765/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963068784983299.2500 - val_loss: 873787561727132.3750\n",
            "Epoch 766/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 963199623300647.8750 - val_loss: 873817159155434.3750\n",
            "Epoch 767/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963204073178362.6250 - val_loss: 873262826281388.1250\n",
            "Epoch 768/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963306173435793.8750 - val_loss: 872648991533461.0000\n",
            "Epoch 769/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963311294158914.7500 - val_loss: 872487324148128.5000\n",
            "Epoch 770/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963208367955817.2500 - val_loss: 872304761925099.6250\n",
            "Epoch 771/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963290373658737.0000 - val_loss: 872648641693406.6250\n",
            "Epoch 772/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963154577021481.2500 - val_loss: 872627146693313.6250\n",
            "Epoch 773/1000\n",
            "707/707 [==============================] - 0s 13us/step - loss: 963195191647688.2500 - val_loss: 872978341257447.3750\n",
            "Epoch 774/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 963154049832470.5000 - val_loss: 873337868113052.3750\n",
            "Epoch 775/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 962990474444406.1250 - val_loss: 872504042094406.8750\n",
            "Epoch 776/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963211318372818.3750 - val_loss: 872125963959579.5000\n",
            "Epoch 777/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963235938498290.6250 - val_loss: 872268659702974.8750\n",
            "Epoch 778/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963169517239115.0000 - val_loss: 872139540520265.6250\n",
            "Epoch 779/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 963172007386127.8750 - val_loss: 871924521060062.6250\n",
            "Epoch 780/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 963213066762804.8750 - val_loss: 871725666088323.6250\n",
            "Epoch 781/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963211020322135.2500 - val_loss: 871343657943225.1250\n",
            "Epoch 782/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963280724695013.8750 - val_loss: 871633818583225.1250\n",
            "Epoch 783/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963123931336338.8750 - val_loss: 871356827637910.3750\n",
            "Epoch 784/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 963190803515823.6250 - val_loss: 871188694324935.6250\n",
            "Epoch 785/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 963172593805593.0000 - val_loss: 870789349346379.3750\n",
            "Epoch 786/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963308581897698.2500 - val_loss: 870681394184747.3750\n",
            "Epoch 787/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963314714231177.8750 - val_loss: 870877368556040.6250\n",
            "Epoch 788/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963446744146140.1250 - val_loss: 870574643716975.3750\n",
            "Epoch 789/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963352599047722.7500 - val_loss: 871160066154820.0000\n",
            "Epoch 790/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963285124739413.8750 - val_loss: 871397112106892.3750\n",
            "Epoch 791/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963204377327694.2500 - val_loss: 871076328965206.6250\n",
            "Epoch 792/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 963430732749538.6250 - val_loss: 870977205239437.6250\n",
            "Epoch 793/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 963219404801089.2500 - val_loss: 871107927718547.5000\n",
            "Epoch 794/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 963278264637831.0000 - val_loss: 871117293629000.3750\n",
            "Epoch 795/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963303172562561.6250 - val_loss: 870909306294156.3750\n",
            "Epoch 796/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963331574704409.0000 - val_loss: 871404400300286.5000\n",
            "Epoch 797/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 963116330094713.7500 - val_loss: 870823284039726.3750\n",
            "Epoch 798/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963262846281406.3750 - val_loss: 871217457530313.0000\n",
            "Epoch 799/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 963220335972172.5000 - val_loss: 871381622321545.3750\n",
            "Epoch 800/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 963113362781846.0000 - val_loss: 871939326885795.3750\n",
            "Epoch 801/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 962985789211064.2500 - val_loss: 872349442524524.5000\n",
            "Epoch 802/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 962843581921265.6250 - val_loss: 871844801917414.0000\n",
            "Epoch 803/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 962892063092174.0000 - val_loss: 872516439418081.6250\n",
            "Epoch 804/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 962754847107190.7500 - val_loss: 872704594821027.3750\n",
            "Epoch 805/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 962654138154382.2500 - val_loss: 873205504314073.0000\n",
            "Epoch 806/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 962612684999597.3750 - val_loss: 873601655685582.8750\n",
            "Epoch 807/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 962457860175510.0000 - val_loss: 874086734465382.6250\n",
            "Epoch 808/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 962350169693941.5000 - val_loss: 873760617759275.3750\n",
            "Epoch 809/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 962323878396390.6250 - val_loss: 873248649732327.3750\n",
            "Epoch 810/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 962435075648325.2500 - val_loss: 872840707913091.6250\n",
            "Epoch 811/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 962566831895333.3750 - val_loss: 872999707944717.0000\n",
            "Epoch 812/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 962390194436081.6250 - val_loss: 873268107296675.3750\n",
            "Epoch 813/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 962318433940632.1250 - val_loss: 873395515797249.5000\n",
            "Epoch 814/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 962363630146886.0000 - val_loss: 873035026816885.1250\n",
            "Epoch 815/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 962323605612384.6250 - val_loss: 873382120834227.3750\n",
            "Epoch 816/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 962217696321802.5000 - val_loss: 873174163236435.8750\n",
            "Epoch 817/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 962249690401524.1250 - val_loss: 873426026091676.3750\n",
            "Epoch 818/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 962288648093742.3750 - val_loss: 873646557107633.8750\n",
            "Epoch 819/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 962188605898821.6250 - val_loss: 873608237754865.5000\n",
            "Epoch 820/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 962263282461879.8750 - val_loss: 873706748611659.3750\n",
            "Epoch 821/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 962062227722477.6250 - val_loss: 874047428423332.8750\n",
            "Epoch 822/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 962209680846680.0000 - val_loss: 873408873424745.6250\n",
            "Epoch 823/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 962068897887940.3750 - val_loss: 873268171490853.6250\n",
            "Epoch 824/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 962090262497873.8750 - val_loss: 873745356409838.6250\n",
            "Epoch 825/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 961955985800451.2500 - val_loss: 873281531993788.0000\n",
            "Epoch 826/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 962092233690238.0000 - val_loss: 873067723425595.3750\n",
            "Epoch 827/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 962091182492056.5000 - val_loss: 872739375187956.3750\n",
            "Epoch 828/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 962202276897521.2500 - val_loss: 873000828516271.0000\n",
            "Epoch 829/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 961985379886295.7500 - val_loss: 873395402954624.6250\n",
            "Epoch 830/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 962094548756204.8750 - val_loss: 873212898111140.8750\n",
            "Epoch 831/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 962011790332929.5000 - val_loss: 873142317904820.6250\n",
            "Epoch 832/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 961972219026494.2500 - val_loss: 872763375366444.8750\n",
            "Epoch 833/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 962182906498197.1250 - val_loss: 872382769830223.5000\n",
            "Epoch 834/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 962070759091059.5000 - val_loss: 872425581027466.8750\n",
            "Epoch 835/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 962057277992884.6250 - val_loss: 872014969062764.5000\n",
            "Epoch 836/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 962060058691853.3750 - val_loss: 871290910598277.0000\n",
            "Epoch 837/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 962190624729461.7500 - val_loss: 871436509490945.5000\n",
            "Epoch 838/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 962175069236500.5000 - val_loss: 871082016009707.6250\n",
            "Epoch 839/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 962105496435438.3750 - val_loss: 871580129981364.6250\n",
            "Epoch 840/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961995779458391.2500 - val_loss: 871299847063540.3750\n",
            "Epoch 841/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 961931615980600.3750 - val_loss: 871371497656759.6250\n",
            "Epoch 842/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 961849302972239.3750 - val_loss: 871281286501902.5000\n",
            "Epoch 843/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 961942484436728.3750 - val_loss: 871021445385106.1250\n",
            "Epoch 844/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 962202309360366.3750 - val_loss: 870538705817669.3750\n",
            "Epoch 845/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 962088194961101.0000 - val_loss: 870383699656518.8750\n",
            "Epoch 846/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 962135594416855.1250 - val_loss: 870283151966022.8750\n",
            "Epoch 847/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 962093158169419.0000 - val_loss: 870379588863155.3750\n",
            "Epoch 848/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 962165709116162.5000 - val_loss: 870329418678827.3750\n",
            "Epoch 849/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 962033356388105.7500 - val_loss: 871154066729457.5000\n",
            "Epoch 850/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961923961820740.8750 - val_loss: 871545742606798.8750\n",
            "Epoch 851/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 961735038693347.0000 - val_loss: 872106233509251.6250\n",
            "Epoch 852/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 961618683028881.1250 - val_loss: 872741141326136.3750\n",
            "Epoch 853/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 961534352435712.7500 - val_loss: 872415791336800.8750\n",
            "Epoch 854/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 961676460865706.8750 - val_loss: 872029375205538.0000\n",
            "Epoch 855/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961626717393203.0000 - val_loss: 872521720972467.3750\n",
            "Epoch 856/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961482429254017.2500 - val_loss: 871950546723047.3750\n",
            "Epoch 857/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961667439374608.2500 - val_loss: 872168976922543.0000\n",
            "Epoch 858/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961583690858287.3750 - val_loss: 872252131071317.3750\n",
            "Epoch 859/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961756396113045.1250 - val_loss: 871563076861026.3750\n",
            "Epoch 860/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961700604110086.1250 - val_loss: 872060393954171.0000\n",
            "Epoch 861/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 961575466723956.6250 - val_loss: 872634931216384.0000\n",
            "Epoch 862/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961527440032879.6250 - val_loss: 872448219377502.0000\n",
            "Epoch 863/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 961488911500422.6250 - val_loss: 872136783520715.8750\n",
            "Epoch 864/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961525188231492.5000 - val_loss: 872509161078784.0000\n",
            "Epoch 865/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 961550260179019.3750 - val_loss: 872314352596633.3750\n",
            "Epoch 866/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961588099279430.2500 - val_loss: 871924545836373.3750\n",
            "Epoch 867/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961492062650761.8750 - val_loss: 872034821064229.6250\n",
            "Epoch 868/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 961698216524781.2500 - val_loss: 871947583374700.5000\n",
            "Epoch 869/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 961489674068790.6250 - val_loss: 871639078696601.3750\n",
            "Epoch 870/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961613515266670.8750 - val_loss: 871228356678071.6250\n",
            "Epoch 871/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961541901281167.0000 - val_loss: 871084117779589.0000\n",
            "Epoch 872/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961529089468140.8750 - val_loss: 871631630788642.6250\n",
            "Epoch 873/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961425156157480.6250 - val_loss: 871191590991698.5000\n",
            "Epoch 874/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961498331112246.6250 - val_loss: 871448306158297.0000\n",
            "Epoch 875/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961464297046456.2500 - val_loss: 871170037976324.3750\n",
            "Epoch 876/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 961594631193039.5000 - val_loss: 871409529223920.1250\n",
            "Epoch 877/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961423928217142.2500 - val_loss: 871110376149431.6250\n",
            "Epoch 878/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 961595203232033.7500 - val_loss: 871300544503576.6250\n",
            "Epoch 879/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 961452246876322.2500 - val_loss: 871589064946335.1250\n",
            "Epoch 880/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961313429296059.8750 - val_loss: 871863642340728.0000\n",
            "Epoch 881/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 961239749197592.3750 - val_loss: 872419717008736.8750\n",
            "Epoch 882/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 961159943967745.5000 - val_loss: 872083078246793.3750\n",
            "Epoch 883/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 961083834281607.3750 - val_loss: 872116559198497.3750\n",
            "Epoch 884/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 961076252522435.2500 - val_loss: 872554334360824.6250\n",
            "Epoch 885/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 960925920195411.6250 - val_loss: 873022193469243.3750\n",
            "Epoch 886/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 960852058323995.6250 - val_loss: 872752296795928.6250\n",
            "Epoch 887/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 960825555733189.7500 - val_loss: 872268292695450.6250\n",
            "Epoch 888/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 960895764798897.0000 - val_loss: 871843206455712.5000\n",
            "Epoch 889/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 960990043901471.2500 - val_loss: 872048473289004.8750\n",
            "Epoch 890/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960858246862335.2500 - val_loss: 872603685702453.5000\n",
            "Epoch 891/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 960723326342215.0000 - val_loss: 873050093745817.3750\n",
            "Epoch 892/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 960781582721244.1250 - val_loss: 872949342633798.8750\n",
            "Epoch 893/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960670288507917.0000 - val_loss: 873025426172031.3750\n",
            "Epoch 894/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 960659748524524.5000 - val_loss: 873273543614510.3750\n",
            "Epoch 895/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 960613859350432.5000 - val_loss: 872848806857895.6250\n",
            "Epoch 896/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 960477988200505.8750 - val_loss: 872641923462161.3750\n",
            "Epoch 897/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960616295487624.1250 - val_loss: 872061368717751.6250\n",
            "Epoch 898/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 960571210765614.7500 - val_loss: 871989536650164.6250\n",
            "Epoch 899/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 960508832269701.6250 - val_loss: 871898580776045.8750\n",
            "Epoch 900/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 960604602029103.7500 - val_loss: 872149561080560.1250\n",
            "Epoch 901/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 960386891548338.8750 - val_loss: 871802888366392.3750\n",
            "Epoch 902/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960457953356983.8750 - val_loss: 872245091190494.6250\n",
            "Epoch 903/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 960324708232909.0000 - val_loss: 872464707096182.6250\n",
            "Epoch 904/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 960190904667215.6250 - val_loss: 873041615911849.3750\n",
            "Epoch 905/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 960090554680385.2500 - val_loss: 872811810615134.0000\n",
            "Epoch 906/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 960048789901271.3750 - val_loss: 872834438352410.0000\n",
            "Epoch 907/1000\n",
            "707/707 [==============================] - 0s 27us/step - loss: 960068618529790.5000 - val_loss: 872429869108299.3750\n",
            "Epoch 908/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 960009410951431.6250 - val_loss: 872274536680060.3750\n",
            "Epoch 909/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960070518316650.3750 - val_loss: 872515647678406.1250\n",
            "Epoch 910/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960082195355390.2500 - val_loss: 872062268395219.1250\n",
            "Epoch 911/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 960102837975512.1250 - val_loss: 872546328578221.5000\n",
            "Epoch 912/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 960045097015339.5000 - val_loss: 872457848890778.6250\n",
            "Epoch 913/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960070199218807.5000 - val_loss: 872013587338766.5000\n",
            "Epoch 914/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960039302041536.3750 - val_loss: 872190009331781.3750\n",
            "Epoch 915/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959959104457390.5000 - val_loss: 871684788430049.6250\n",
            "Epoch 916/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 960017664772180.0000 - val_loss: 871404759374992.6250\n",
            "Epoch 917/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 960081528965319.8750 - val_loss: 871981194380930.1250\n",
            "Epoch 918/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959872402083196.8750 - val_loss: 871384108467471.8750\n",
            "Epoch 919/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959936995314002.8750 - val_loss: 870724308238706.3750\n",
            "Epoch 920/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 960114294559675.8750 - val_loss: 870522665595828.6250\n",
            "Epoch 921/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 960151895955175.0000 - val_loss: 870793265496341.6250\n",
            "Epoch 922/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 959989045262159.3750 - val_loss: 870642640872210.6250\n",
            "Epoch 923/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960036848320303.3750 - val_loss: 870477248541343.1250\n",
            "Epoch 924/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959989153685214.2500 - val_loss: 871143522457657.8750\n",
            "Epoch 925/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960092860491593.5000 - val_loss: 871105153231623.3750\n",
            "Epoch 926/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959764658541013.2500 - val_loss: 870578156296990.3750\n",
            "Epoch 927/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 959953686365828.5000 - val_loss: 870238145479396.5000\n",
            "Epoch 928/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 959983226819237.8750 - val_loss: 869762380820318.0000\n",
            "Epoch 929/1000\n",
            "707/707 [==============================] - 0s 25us/step - loss: 960210526914620.7500 - val_loss: 870032399457239.5000\n",
            "Epoch 930/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 959998060939371.2500 - val_loss: 870163394763828.1250\n",
            "Epoch 931/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 960228015465595.1250 - val_loss: 869543585019545.3750\n",
            "Epoch 932/1000\n",
            "707/707 [==============================] - 0s 22us/step - loss: 960197863747253.7500 - val_loss: 869888387984968.3750\n",
            "Epoch 933/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 959993545827474.2500 - val_loss: 870108524682662.3750\n",
            "Epoch 934/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959982232241194.0000 - val_loss: 870421453482111.3750\n",
            "Epoch 935/1000\n",
            "707/707 [==============================] - 0s 23us/step - loss: 959890401662949.8750 - val_loss: 870441540643776.3750\n",
            "Epoch 936/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 959841739312346.6250 - val_loss: 870296012695864.3750\n",
            "Epoch 937/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 959793913188496.7500 - val_loss: 870801927461292.1250\n",
            "Epoch 938/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 959718131619142.0000 - val_loss: 870770380616906.5000\n",
            "Epoch 939/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959721473773457.8750 - val_loss: 870310669216519.3750\n",
            "Epoch 940/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 959891728771584.7500 - val_loss: 870633231007466.3750\n",
            "Epoch 941/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959767797778817.2500 - val_loss: 870257179964063.1250\n",
            "Epoch 942/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959762224705646.1250 - val_loss: 870490255924461.3750\n",
            "Epoch 943/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959760374228554.6250 - val_loss: 870170265441806.5000\n",
            "Epoch 944/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959792892032685.1250 - val_loss: 870585723947737.0000\n",
            "Epoch 945/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959741000649390.5000 - val_loss: 870487316188327.6250\n",
            "Epoch 946/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959628370998146.0000 - val_loss: 870158816156724.1250\n",
            "Epoch 947/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959639085564251.6250 - val_loss: 869780078685305.5000\n",
            "Epoch 948/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959735504366984.5000 - val_loss: 869868288958695.3750\n",
            "Epoch 949/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 959890210896274.6250 - val_loss: 869976899532163.6250\n",
            "Epoch 950/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 959730162330553.0000 - val_loss: 869765112556295.3750\n",
            "Epoch 951/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 959727935113601.2500 - val_loss: 869108254270782.3750\n",
            "Epoch 952/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 960059224238036.5000 - val_loss: 869400298669420.5000\n",
            "Epoch 953/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 959958572925762.3750 - val_loss: 869258167541250.8750\n",
            "Epoch 954/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959882762192053.0000 - val_loss: 869421724795990.6250\n",
            "Epoch 955/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 959925303801354.8750 - val_loss: 869839367810076.8750\n",
            "Epoch 956/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959706309353367.7500 - val_loss: 870358948356234.8750\n",
            "Epoch 957/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959597271687457.7500 - val_loss: 870448603301506.1250\n",
            "Epoch 958/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 959724851333157.6250 - val_loss: 870603965946654.3750\n",
            "Epoch 959/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959548745227558.0000 - val_loss: 870656886039314.6250\n",
            "Epoch 960/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 959600503544036.7500 - val_loss: 870426165589379.6250\n",
            "Epoch 961/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 959565705177577.5000 - val_loss: 870159342272326.8750\n",
            "Epoch 962/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 959915351186633.3750 - val_loss: 870122938862297.0000\n",
            "Epoch 963/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 959589087633350.1250 - val_loss: 870196921522604.1250\n",
            "Epoch 964/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959526553182891.6250 - val_loss: 869608297213512.3750\n",
            "Epoch 965/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959651385731515.1250 - val_loss: 870024863486669.3750\n",
            "Epoch 966/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 959735431562884.5000 - val_loss: 869750213610201.0000\n",
            "Epoch 967/1000\n",
            "707/707 [==============================] - 0s 14us/step - loss: 959628310486263.7500 - val_loss: 869873994488791.5000\n",
            "Epoch 968/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 959600510568161.1250 - val_loss: 870169977273720.0000\n",
            "Epoch 969/1000\n",
            "707/707 [==============================] - 0s 24us/step - loss: 959523423935480.7500 - val_loss: 870482505075046.6250\n",
            "Epoch 970/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959479007407033.0000 - val_loss: 870295148110888.5000\n",
            "Epoch 971/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959401986127185.5000 - val_loss: 870215971594471.3750\n",
            "Epoch 972/1000\n",
            "707/707 [==============================] - 0s 15us/step - loss: 959620073988524.7500 - val_loss: 869846943422151.6250\n",
            "Epoch 973/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 959535966648606.8750 - val_loss: 870142754356877.6250\n",
            "Epoch 974/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959417398290041.0000 - val_loss: 870145173463178.8750\n",
            "Epoch 975/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959365799546955.3750 - val_loss: 869867685538752.3750\n",
            "Epoch 976/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959328300024989.8750 - val_loss: 870207994022339.3750\n",
            "Epoch 977/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 959223850203886.3750 - val_loss: 870371178439442.6250\n",
            "Epoch 978/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959065708521822.5000 - val_loss: 870494402932944.3750\n",
            "Epoch 979/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959074132748777.5000 - val_loss: 870607339157885.8750\n",
            "Epoch 980/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959059415119950.2500 - val_loss: 870347675163792.6250\n",
            "Epoch 981/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959167677664243.0000 - val_loss: 870502050609071.0000\n",
            "Epoch 982/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 959079146834536.8750 - val_loss: 870761880491071.6250\n",
            "Epoch 983/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 958929371786354.5000 - val_loss: 870276626661283.3750\n",
            "Epoch 984/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 958989135290935.7500 - val_loss: 870244278815171.3750\n",
            "Epoch 985/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 959038064249773.3750 - val_loss: 870451134902387.6250\n",
            "Epoch 986/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 958948130588524.3750 - val_loss: 870374895688385.6250\n",
            "Epoch 987/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 959045773985644.3750 - val_loss: 870658868979098.6250\n",
            "Epoch 988/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 958876797283233.8750 - val_loss: 870361548992812.8750\n",
            "Epoch 989/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 958932283831703.1250 - val_loss: 870083345179781.0000\n",
            "Epoch 990/1000\n",
            "707/707 [==============================] - 0s 16us/step - loss: 958978920315676.6250 - val_loss: 869782363783445.6250\n",
            "Epoch 991/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959106176234671.2500 - val_loss: 869913291693108.1250\n",
            "Epoch 992/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 958932127355094.3750 - val_loss: 869991399146305.1250\n",
            "Epoch 993/1000\n",
            "707/707 [==============================] - 0s 18us/step - loss: 959009511516394.7500 - val_loss: 869508366796603.3750\n",
            "Epoch 994/1000\n",
            "707/707 [==============================] - 0s 19us/step - loss: 959110790325023.5000 - val_loss: 869755645384947.0000\n",
            "Epoch 995/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 958949038741362.1250 - val_loss: 869508027928830.5000\n",
            "Epoch 996/1000\n",
            "707/707 [==============================] - 0s 27us/step - loss: 958995915801595.6250 - val_loss: 869746244239082.3750\n",
            "Epoch 997/1000\n",
            "707/707 [==============================] - 0s 25us/step - loss: 958919884211381.0000 - val_loss: 870234988807254.6250\n",
            "Epoch 998/1000\n",
            "707/707 [==============================] - 0s 17us/step - loss: 958804373530094.0000 - val_loss: 870181197650070.3750\n",
            "Epoch 999/1000\n",
            "707/707 [==============================] - 0s 20us/step - loss: 958754892821540.2500 - val_loss: 870082226120368.5000\n",
            "Epoch 1000/1000\n",
            "707/707 [==============================] - 0s 21us/step - loss: 958829149325349.6250 - val_loss: 869678437102701.8750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j2P_0gOvxtsw",
        "colab_type": "code",
        "outputId": "89dbfba5-52cd-4a3b-870f-e7f86869c1b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train_loss = history.history['loss']\n",
        "validation_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(train_loss))\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, train_loss, 'b-', label = 'Training loss')\n",
        "plt.plot(epochs, validation_loss, 'r-', label = 'Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEHCAYAAABV4gY/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCFsSIK7EELYkAR3IYSwof8HeQtNvf8hpucAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff9b4014910>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3mj-3gRgyU2z",
        "colab_type": "code",
        "outputId": "d661a07e-4e38-4910-ca4f-b983e7f2e37a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_aux) \n",
        "\n",
        "print(\"Number of predictions:\", len(y_pred))\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE: \", mse)\n",
        "rmse = math.sqrt(mse)\n",
        "print(\"RMSE: \", rmse) \n",
        "y_pred = np.array([x[0] for x in y_pred]) \n",
        "\n",
        "results = pd.DataFrame({\"y\":y_test[\"precio\"], 'pred': y_pred}, columns = [ \"y\", 'pred']) \n",
        "#result[\"y\"] = y_test[\"precio\"]\n",
        "results['error'] = abs(results['pred'].astype(float) - results['y'].astype(float))\n",
        "print(\"MAPE: \", 100 * np.mean(results['error'] / y_test[\"precio\"]))\n",
        "results.sort_values(by = 'error', ascending = True)\n",
        "\n",
        "results.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Number of predictions:', 177)\n",
            "('MSE: ', 869678476230696.1)\n",
            "('RMSE: ', 29490311.56550734)\n",
            "('MAPE: ', 17.96730892057084)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>pred</th>\n",
              "      <th>error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>812</th>\n",
              "      <td>185000000.0</td>\n",
              "      <td>155342560.0</td>\n",
              "      <td>29657440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>643</th>\n",
              "      <td>170000000.0</td>\n",
              "      <td>169581968.0</td>\n",
              "      <td>418032.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>145000000.0</td>\n",
              "      <td>143326720.0</td>\n",
              "      <td>1673280.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>120000000.0</td>\n",
              "      <td>136333120.0</td>\n",
              "      <td>16333120.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>195000000.0</td>\n",
              "      <td>161378208.0</td>\n",
              "      <td>33621792.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               y         pred       error\n",
              "812  185000000.0  155342560.0  29657440.0\n",
              "643  170000000.0  169581968.0    418032.0\n",
              "440  145000000.0  143326720.0   1673280.0\n",
              "254  120000000.0  136333120.0  16333120.0\n",
              "883  195000000.0  161378208.0  33621792.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "metadata": {
        "id": "gTP41PPPyg3o",
        "colab_type": "code",
        "outputId": "195c9d3f-e230-4f26-f42e-7dc384cc4029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_train_aux) \n",
        "\n",
        "print(\"Number of predictions:\", len(y_pred))\n",
        "\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "print(\"MSE: \", mse)\n",
        "rmse = math.sqrt(mse)\n",
        "print(\"RMSE: \", rmse) \n",
        "y_pred = np.array([x[0] for x in y_pred]) \n",
        "\n",
        "results = pd.DataFrame({\"y\":y_train[\"precio\"], 'pred': y_pred}, columns = [ \"y\", 'pred']) \n",
        "#result[\"y\"] = y_test[\"precio\"]\n",
        "results['error'] = abs(results['pred'].astype(float) - results['y'].astype(float))\n",
        "print(\"MAPE: \", 100 * np.mean(results['error'] / y_train[\"precio\"]))\n",
        "results.sort_values(by = 'error', ascending = True)\n",
        "\n",
        "results.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Number of predictions:', 707)\n",
            "('MSE: ', 958737339194117.4)\n",
            "('RMSE: ', 30963483.96408449)\n",
            "('MAPE: ', 18.278026723127123)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>pred</th>\n",
              "      <th>error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>130000000.0</td>\n",
              "      <td>137377600.0</td>\n",
              "      <td>7377600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>115000000.0</td>\n",
              "      <td>137780128.0</td>\n",
              "      <td>22780128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>91000000.0</td>\n",
              "      <td>129905208.0</td>\n",
              "      <td>38905208.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>160000000.0</td>\n",
              "      <td>148531456.0</td>\n",
              "      <td>11468544.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>94161000.0</td>\n",
              "      <td>124001856.0</td>\n",
              "      <td>29840856.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               y         pred       error\n",
              "309  130000000.0  137377600.0   7377600.0\n",
              "228  115000000.0  137780128.0  22780128.0\n",
              "110   91000000.0  129905208.0  38905208.0\n",
              "564  160000000.0  148531456.0  11468544.0\n",
              "121   94161000.0  124001856.0  29840856.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "cj_I74OiRWHQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%notebook ./filename.ipynb\n",
        "!./dropbox_uploader.sh upload filename.ipynb /"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}